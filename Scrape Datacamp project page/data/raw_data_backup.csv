repo_url,title,language,author,bio,datacamp_url,description,notebook_url,notebook_dl_url,imports,attributes,methods,functions,texts
https://github.com/mrbarkis/DataCamp_projects/tree/master/A%20Network%20Analysis%20of%20Game%20of%20Thrones,A Network Analysis of Game of Thrones,python,Mridul Seth,Data science enthusiast,https://www.datacamp.com/projects/76,"Analyze the network of characters in Game of Thrones and how it changes over the course of the books.
",https://www.github.com/mrbarkis/DataCamp_projects/blob/master/A%20Network%20Analysis%20of%20Game%20of%20Thrones/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/A%20Network%20Analysis%20of%20Game%20of%20Thrones/notebook.ipynb,"['import pandas as pd', 'import networkx as nx', 'import numpy as np']","['.csv', '.csv', '.csv', '.csv', '.csv', '.csv', '.DataFrame', '.DataFrame', '.T', '.index', '.DataFrame', '.T', '.index', '.DataFrame', '.T', '.T']","['.read_csv()', '.head()', '.set_xticks()', '.arange()', '.set_xticklabels()', '.arange()', '.set_xlabel()', '.Graph()', '.iterrows()', '.add_edge()', '.read_csv()', '.Graph()', '.iterrows()', '.add_edge()', '.append()', '.degree_centrality()', '.degree_centrality()', '.items()', '.items()', '.degree_centrality()', '.from_records()', '.head()', '.plot()', '.set_ylabel()', '.set_title()', '.betweenness_centrality()', '.from_records()', '.fillna()', '.sort_values()', '.plot()', '.set_ylabel()', '.set_title()', '.pagerank()', '.from_records()', '.sort_values()', '.plot()', '.set_ylabel()', '.set_title()', '.pagerank()', '.betweenness_centrality()', '.degree_centrality()', '.from_records()', '.corr()', '.idxmax()']","['setBookAxes()', 'sorted()', 'sorted()', 'print()', 'print()', 'setBookAxes()', 'set()', 'range()', 'set()', 'list()', 'list()', 'setBookAxes()', 'set()', 'range()', 'set()', 'list()', 'list()', 'setBookAxes()', 'print()']","## 1. Winter is Coming. Let's load the dataset ASAP!
If you haven't heard of Game of Thrones, then you must be really good at hiding. Game of Thrones is the hugely popular television series by HBO based on the (also) hugely popular book series A Song of Ice and Fire by George R.R. Martin. In this notebook, we will analyze the co-occurrence network of the characters in the  Game of Thrones books. Here, two characters are considered to co-occur if their names appear in the vicinity of 15 words from one another in the books. 

This dataset constitutes a network and is given as a text file describing the edges between characters, with some attributes attached to each edge. Let's start by loading in the data for the first book A Game of Thrones and inspect it.

## 2. Time for some Network of Thrones
The resulting DataFrame book1 has 5 columns: Source, Target, Type, weight, and book. Source and target are the two nodes that are linked by an edge. A network can have directed or undirected edges and in this network all the edges are undirected. The weight attribute of every edge tells us the number of interactions that the characters have had over the book, and the book column tells us the book number.
Once we have the data loaded as a pandas DataFrame, it's time to create a network. We will use networkx, a network analysis library, and create a graph object for the first book.

## 3. Populate the network with the DataFrame
Currently, the graph object G_book1 is empty. Let's now populate it with the edges from book1. And while we're at it, let's load in the rest of the books too!

## 4. The most important character in Game of Thrones
Is it Jon Snow, Tyrion, Daenerys, or someone else? Let's see! Network science offers us many different metrics to measure the importance of a node in a network. Note that there is no ""correct"" way of calculating the most important node in a network, every metric has a different meaning.
First, let's measure the importance of a node in a network by looking at the number of neighbors it has, that is, the number of nodes it is connected to. For example, an influential account on Twitter, where the follower-followee relationship forms the network, is an account which has a high number of followers. This measure of importance is called degree centrality.
Using this measure, let's extract the top ten important characters from the first book (book[0]) and the fifth book (book[4]).

## 5. The evolution of character importance
According to degree centrality, the most important character in the first book is Eddard Stark but he is not even in the top 10 of the fifth book. The importance of characters changes over the course of five books because, you know, stuff happens... ;)
Let's look at the evolution of degree centrality of a couple of characters like Eddard Stark, Jon Snow, and Tyrion, which showed up in the top 10 of degree centrality in the first book.

## 6. What's up with Stannis Baratheon?
We can see that the importance of Eddard Stark dies off as the book series progresses. With Jon Snow, there is a drop in the fourth book but a sudden rise in the fifth book.
Now let's look at various other measures like betweenness centrality and PageRank to find important characters in our Game of Thrones character co-occurrence network and see if we can uncover some more interesting facts about this network. Let's plot the evolution of betweenness centrality of this network over the five books. We will take the evolution of the top four characters of every book and plot it.

## 7. What does Google PageRank tell us about GoT?
We see a peculiar rise in the importance of Stannis Baratheon over the books. In the fifth book, he is significantly more important than other characters in the network, even though he is the third most important character according to degree centrality.
PageRank was the initial way Google ranked web pages. It evaluates the inlinks and outlinks of webpages in the world wide web, which is, essentially, a directed network. Let's look at the importance of characters in the Game of Thrones network according to PageRank. 

## 8. Correlation between different measures
Stannis, Jon Snow, and Daenerys are the most important characters in the fifth book according to PageRank. Eddard Stark follows a similar curve but for degree centrality and betweenness centrality: He is important in the first book but dies into oblivion over the book series.
We have seen three different measures to calculate the importance of a node in a network, and all of them tells us something about the characters and their importance in the co-occurrence network. We see some names pop up in all three measures so maybe there is a strong correlation between them?
Let's look at the correlation between PageRank, betweenness centrality and degree centrality for the fifth book using Pearson correlation.

## 9. Conclusion
We see a high correlation between these three measures for our character co-occurrence network.
So we've been looking at different ways to find the important characters in the Game of Thrones co-occurrence network. According to degree centrality, Eddard Stark is the most important character initially in the books. But who is/are the most important character(s) in the fifth book according to these three measures? 

"
https://github.com/mrbarkis/DataCamp_projects/tree/master/A%20New%20Era%20of%20Data%20Analysis%20in%20Baseball,A New Era of Data Analysis in Baseball,python,David Venturi,Curriculum Manager at DataCamp,https://www.datacamp.com/projects/250,"Use MLB's Statcast data to compare New York Yankees sluggers Aaron Judge and Giancarlo Stanton.
",https://www.github.com/mrbarkis/DataCamp_projects/blob/master/A%20New%20Era%20of%20Data%20Analysis%20in%20Baseball/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/A%20New%20Era%20of%20Data%20Analysis%20in%20Baseball/notebook.ipynb,"['import pandas as pd', 'import matplotlib.pyplot as plt', 'import seaborn as sns']","['.pyplot', '.csv', '.csv', '.max_columns', '.str', '.str', '.zone', '.zone', '.zone', '.zone', '.zone', '.zone', '.loc', '.zone', '.loc', '.zone']","['.read_csv()', '.read_csv()', '.set_option()', '.tail()', '.startswith()', '.value_counts()', '.startswith()', '.value_counts()', '.subplots()', '.regplot()', '.set_title()', '.regplot()', '.set_title()', '.subplots()', '.kdeplot()', '.set_title()', '.kdeplot()', '.set_title()', '.concat()', '.boxplot()', '.set_title()', '.copy()', '.apply()', '.apply()', '.hist2d()', '.title()', '.gca()', '.get_xaxis()', '.set_visible()', '.gca()', '.get_yaxis()', '.set_visible()', '.colorbar()', '.set_label()', '.copy()', '.apply()', '.apply()', '.hist2d()', '.title()', '.gca()', '.get_xaxis()', '.set_visible()', '.gca()', '.get_yaxis()', '.set_visible()', '.colorbar()', '.set_label()']","['print()', 'print()', 'print()', 'print()', 'assign_x_coord()', 'assign_y_coord()']","## 1. The Statcast revolution

This is Aaron Judge. Judge is one of the physically largest players in Major League Baseball standing 6 feet 7 inches (2.01 m) tall and weighing 282 pounds (128 kg). He also hit the hardest home run ever recorded. How do we know this? Statcast.
Statcast is a state-of-the-art tracking system that uses high-resolution cameras and radar equipment to measure the precise location and movement of baseballs and baseball players. Introduced in 2015 to all 30 major league ballparks, Statcast data is revolutionizing the game. Teams are engaging in an ""arms race"" of data analysis, hiring analysts left and right in an attempt to gain an edge over their competition. This video describing the system is incredible.
In this notebook, we're going to wrangle, analyze, and visualize Statcast data to compare Mr. Judge and another (extremely large) teammate of his. Let's start by loading the data into our Notebook. There are two CSV files, judge.csv and stanton.csv, both of which contain Statcast data for 2015-2017. We'll use pandas DataFrames to store this data. Let's also load our data visualization libraries, matplotlib and seaborn.

## 2. What can Statcast measure?
The better question might be, what can't Statcast measure?

Starting with the pitcher, Statcast can measure simple data points such as velocity. At the same time, Statcast digs a whole lot deeper, also measuring the release point and spin rate of every pitch.
Moving on to hitters, Statcast is capable of measuring the exit velocity, launch angle and vector of the ball as it comes off the bat. From there, Statcast can also track the hang time and projected distance that a ball travels.

Let's inspect the last five rows of the judge DataFrame. You'll see that each row represents one pitch thrown to a batter. You'll also see that some columns have esoteric names. If these don't make sense now, don't worry. The relevant ones will be explained as necessary.

## 3. Aaron Judge and Giancarlo Stanton, prolific sluggers

This is Giancarlo Stanton. He is also a very large human being, standing 6 feet 6 inches tall and weighing 245 pounds. Despite not wearing the same jersey as Judge in the pictures provided, in 2018 they will be teammates on the New York Yankees. They are similar in a lot of ways, one being that they hit a lot of home runs. Stanton and Judge led baseball in home runs in 2017, with 59 and 52, respectively. These are exceptional totals - the player in third ""only"" had 45 home runs.
Stanton and Judge are also different in many ways. One is batted ball events, which is any batted ball that produces a result. This includes outs, hits, and errors. Next, you'll find the counts of batted ball events for each player in 2017. The frequencies of other events are quite different.

## 4. Analyzing home runs with Statcast data
So Judge walks and strikes out more than Stanton. Stanton flies out more than Judge. But let's get into their hitting profiles in more detail. Two of the most groundbreaking Statcast metrics are launch angle and exit velocity:

Launch angle: the vertical angle at which the ball leaves a player's bat
Exit velocity: the speed of the baseball as it comes off the bat

This new data has changed the way teams value both hitters and pitchers. Why? As per the Washington Post:

Balls hit with a high launch angle are more likely to result in a hit. Hit fast enough and at the right angle, they become home runs.

Let's look at exit velocity vs. launch angle and let's focus on home runs only (2015-2017). The first two plots show data points. The second two show smoothed contours to represent density.

## 5. Home runs by pitch velocity
It appears that Stanton hits his home runs slightly lower and slightly harder than Judge, though this needs to be taken with a grain of salt given the small sample size of home runs.
Not only does Statcast measure the velocity of the ball coming off of the bat, it measures the velocity of the ball coming out of the pitcher's hand and begins its journey towards the plate. We can use this data to compare Stanton and Judge's home runs in terms of pitch velocity. Next you'll find box plots displaying the five-number summaries for each player: minimum, first quartile, median, third quartile, and maximum.

## 6. Home runs by pitch location (I)
So Judge appears to hit his home runs off of faster pitches than Stanton. We might call Judge a fastball hitter. Stanton appears agnostic to pitch speed and likely pitch movement since slower pitches (e.g. curveballs, sliders, and changeups) tend to have more break. Statcast does track pitch movement and type but let's move on to something else: pitch location. Statcast tracks the zone the pitch is in when it crosses the plate. The zone numbering looks like this (from the catcher's point of view):

We can plot this using a 2D histogram. For simplicity, let's only look at strikes, which gives us a 9x9 grid. We can view each zone as coordinates on a 2D plot, the bottom left corner being (1,1) and the top right corner being (3,3). Let's set up a function to assign x-coordinates to each pitch.

## 7. Home runs by pitch location (II)
And let's do the same but for y-coordinates.

## 8. Aaron Judge's home run zone
Now we can apply the functions we've created then construct our 2D histograms. First, for Aaron Judge (again, for pitches in the strike zone that resulted in home runs).

## 9. Giancarlo Stanton's home run zone
And now for Giancarlo Stanton.

## 10. Should opposing pitchers be scared?
A few takeaways:

Stanton does not hit many home runs on pitches in the upper third of the strike zone.
Like pretty much every hitter ever, both players love pitches in the horizontal and vertical middle of the plate.
Judge's least favorite home run pitch appears to be high-away while Stanton's appears to be low-away.
If we were to describe Stanton's home run zone, it'd be middle-inside. Judge's home run zone is much more spread out.

The grand takeaway from this whole exercise: Aaron Judge and Giancarlo Stanton are not identical despite their superficial similarities. In terms of home runs, their launch profiles, as well as their pitch speed and location preferences, are different.
Should opposing pitchers still be scared?

"
https://github.com/mrbarkis/DataCamp_projects/tree/master/A%20Visual%20History%20of%20Nobel%20Prize%20Winners,A Visual History of Nobel Prize Winners,python,Rasmus Bååth,Senior Data Scientist at King (Activision Blizzard),https://www.datacamp.com/projects/441,"Explore a dataset from Kaggle containing a century's worth of Nobel Laureates. Who won? Who got snubbed?
",https://www.github.com/mrbarkis/DataCamp_projects/blob/master/A%20Visual%20History%20of%20Nobel%20Prize%20Winners/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/A%20Visual%20History%20of%20Nobel%20Prize%20Winners/notebook.ipynb,"['import pandas as pd', 'import seaborn as sns', 'import numpy as np', 'import matplotlib.pyplot as plt', 'from matplotlib.ticker import PercentFormatter', 'from matplotlib.ticker import PercentFormatter']","['.csv', '.pyplot', '.rcParams', '.figsize', '.ticker', '.yaxis', '.ticker', '.yaxis', '.dt', '.year', '.values']","['.read_csv()', '.head()', '.value_counts()', '.value_counts()', '.head()', '.floor()', '.astype()', '.astype()', '.groupby()', '.mean()', '.set()', '.lineplot()', '.set_major_formatter()', '.groupby()', '.mean()', '.lineplot()', '.set_major_formatter()', '.nsmallest()', '.groupby()', '.filter()', '.to_datetime()', '.lmplot()', '.lmplot()', '.nsmallest()', '.nlargest()', '.nsmallest()']","['display()', 'len()', 'display()', 'PercentFormatter()', 'PercentFormatter()', 'len()', 'display()']","## 1. The most Nobel of Prizes

The Nobel Prize is perhaps the world's most well known scientific award. Except for the honor, prestige and substantial prize money the recipient also gets a gold medal showing Alfred Nobel (1833 - 1896) who established the prize. Every year it's given to scientists and scholars in the categories chemistry, literature, physics, physiology or medicine, economics, and peace. The first Nobel Prize was handed out in 1901, and at that time the Prize was very Eurocentric and male-focused, but nowadays it's not biased in any way whatsoever. Surely. Right?
Well, we're going to find out! The Nobel Foundation has made a dataset available of all prize winners from the start of the prize, in 1901, to 2016. Let's load it in and take a look.

## 2. So, who gets the Nobel Prize?
Just looking at the first couple of prize winners, or Nobel laureates as they are also called, we already see a celebrity: Wilhelm Conrad Röntgen, the guy who discovered X-rays. And actually, we see that all of the winners in 1901 were guys that came from Europe. But that was back in 1901, looking at all winners in the dataset, from 1901 to 2016, which sex and which country is the most commonly represented? 
(For country, we will use the birth_country of the winner, as the organization_country is NaN for all shared Nobel Prizes.)

## 3. USA dominance
Not so surprising perhaps: the most common Nobel laureate between 1901 and 2016 was a man born in the United States of America. But in 1901 all the winners were European. When did the USA start to dominate the Nobel Prize charts?

## 4. USA dominance, visualized
A table is OK, but to see when the USA started to dominate the Nobel charts we need a plot!

## 5. What is the gender of a typical Nobel Prize winner?
So the USA became the dominating winner of the Nobel Prize first in the 1930s and had kept the leading position ever since. But one group that was in the lead from the start, and never seems to let go, are men. Maybe it shouldn't come as a shock that there is some imbalance between how many male and female prize winners there are, but how significant is this imbalance? And is it better or worse within specific prize categories like physics, medicine, literature, etc.?

## 6. The first woman to win the Nobel Prize
The plot above is a bit messy as the lines are overplotting. But it does show some interesting trends and patterns. Overall the imbalance is pretty large with physics, economics, and chemistry having the largest imbalance. Medicine has a somewhat positive trend, and since the 1990s the literature prize is also now more balanced. The big outlier is the peace prize during the 2010s, but keep in mind that this just covers the years 2010 to 2016.
Given this imbalance, who was the first woman to receive a Nobel Prize? And in what category?

## 7. Repeat laureates
For most scientists/writers/activists a Nobel Prize would be the crowning achievement of a long career. But for some people, one is just not enough, and few have gotten it more than once. Who are these lucky few? (Having won no Nobel Prize myself, I'll assume it's just about luck.)

## 8. How old are you when you get the prize?
The list of repeat winners contains some illustrious names! We again meet Marie Curie, who got the prize in physics for discovering radiation and in chemistry for isolating radium and polonium. John Bardeen got it twice in physics for transistors and superconductivity, Frederick Sanger got it twice in chemistry, and Linus Carl Pauling got it first in chemistry and later in peace for his work in promoting nuclear disarmament. We also learn that organizations also get the prize as both the Red Cross and the UNHCR have gotten it twice.
But how old are you generally when you get the prize?

## 9. Age differences between prize categories
The plot above shows us a lot! We see that people use to be around 55 when they received the price, but nowadays the average is closer to 65. But there is a large spread in the laureates' ages, and while most are 50+, some are very young.
We also see that the density of points is much high nowadays than in the early 1900s -- nowadays many more of the prizes are shared, and so there are many more winners. We also see that there was a disruption in awarded prizes around the Second World War (1939 - 1945). 
Let's look at age trends within different prize categories.

## 10. Oldest and youngest winners
More plots with lots of exciting stuff going on! We see that both winners of the chemistry, medicine, and physics prize have gotten older over time. The trend is strongest for physics: the average age used to be below 50, and now it's almost 70. Literature and economics are more stable. We also see that economics is a newer category. But peace shows an opposite trend where winners are getting younger! 
In the peace category we also a winner around 2010 that seems exceptionally young. This begs the questions, who are the oldest and youngest people ever to have won a Nobel Prize?

## 11. You get a prize!

Hey! You get a prize for making it to the very end of this notebook! It might not be a Nobel Prize, but I made it myself in paint so it should count for something. But don't despair, Leonid Hurwicz was 90 years old when he got his prize, so it might not be too late for you. Who knows.
Before you leave, what was again the name of the youngest winner ever who in 2014 got the prize for ""[her] struggle against the suppression of children and young people and for the right of all children to education""?

"
https://github.com/mrbarkis/DataCamp_projects/tree/master/Analyze%20Your%20Runkeeper%20Fitness%20Data,Analyze Your Runkeeper Fitness Data,python,Andrii Pavlenko,Project Instructor,https://www.datacamp.com/projects/727,"Import, clean, and analyze seven years worth of training data tracked on the Runkeeper app.
",https://www.github.com/mrbarkis/DataCamp_projects/blob/master/Analyze%20Your%20Runkeeper%20Fitness%20Data/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/Analyze%20Your%20Runkeeper%20Fitness%20Data/notebook.ipynb,"['import pandas as pd', 'import matplotlib.pyplot as plt', 'import warnings', 'import statsmodels.api as sm']","['.csv', '.str', '.pyplot', '.style', '.figure', '.4', '.3', '.2', '.api', '.tsa', '.trend', '.observed', '.5', '.5', '.xaxis']","['.read_csv()', '.sample()', '.info()', '.drop()', '.value_counts()', '.replace()', '.isnull()', '.sum()', '.mean()', '.mean()', '.copy()', '.copy()', '.copy()', '.fillna()', '.fillna()', '.fillna()', '.isnull()', '.sum()', '.use()', '.filterwarnings()', '.plot()', '.show()', '.resample()', '.mean()', '.resample()', '.mean()', '.mean()', '.resample()', '.count()', '.mean()', '.subplots()', '.plot()', '.set()', '.axhline()', '.mean()', '.plot()', '.set()', '.axhline()', '.mean()', '.show()', '.resample()', '.sum()', '.figure()', '.plot()', '.set()', '.axhspan()', '.axhspan()', '.axhspan()', '.show()', '.resample()', '.bfill()', '.seasonal_decompose()', '.figure()', '.plot()', '.plot()', '.legend()', '.set_title()', '.show()', '.subplots()', '.hist()', '.set_facecolor()', '.set()', '.set()', '.set_xticklabels()', '.show()', '.append()', '.append()', '.sort_index()', '.head()', '.groupby()', '.sum()', '.groupby()', '.describe()', '.stack()', '.format()']","['display()', 'display()', 'int()', 'int()', 'print()', 'display()', 'print()', 'display()', 'print()', 'range()', 'len()', 'print()', 'display()', 'print()', 'print()']","## 1. Obtain and review raw data
One day, my old running friend and I were chatting about our running styles, training habits, and achievements, when I suddenly realized that I could take an in-depth analytical look at my training. I have been using a popular GPS fitness tracker called Runkeeper for years and decided it was time to analyze my running data to see how I was doing.
Since 2012, I've been using the Runkeeper app, and it's great. One key feature: its excellent data export. Anyone who has a smartphone can download the app and analyze their data like we will in this notebook.

After logging your run, the first step is to export the data from Runkeeper (which I've done already). Then import the data and start exploring to find potential problems. After that, create data cleaning strategies to fix the issues. Finally, analyze and visualize the clean time-series data.
I exported seven years worth of my training data, from 2012 through 2018. The data is a CSV file where each row is a single training activity. Let's load and inspect it.

## 2. Data preprocessing
Lucky for us, the column names Runkeeper provides are informative, and we don't need to rename any columns.
But, we do notice missing values using the info() method. What are the reasons for these missing values? It depends. Some heart rate information is missing because I didn't always use a cardio sensor. In the case of the Notes column, it is an optional field that I sometimes left blank. Also, I only used the Route Name column once, and never used the Friend's Tagged column.
We'll fill in missing values in the heart rate column to avoid misleading results later, but right now, our first data preprocessing steps will be to:

Remove columns not useful for our analysis.
Replace the ""Other"" activity type to ""Unicycling"" because that was always the ""Other"" activity.
Count missing values.


## 3. Dealing with missing values
As we can see from the last output, there are 214 missing entries for my average heart rate.
We can't go back in time to get those data, but we can fill in the missing values with an average value. This process is called mean imputation. When imputing the mean to fill in missing data, we need to consider that the average heart rate varies for different activities (e.g., walking vs. running). We'll filter the DataFrames by activity type (Type) and calculate each activity's mean heart rate, then fill in the missing values with those means.

## 4. Plot running data
Now we can create our first plot! As we found earlier, most of the activities in my data were running (459 of them to be exact). There are only 29, 18, and two instances for cycling, walking, and unicycling, respectively. So for now, let's focus on plotting the different running metrics.
An excellent first visualization is a figure with four subplots, one for each running metric (each numerical column). Each subplot will have a different y-axis, which is explained in each legend. The x-axis, Date, is shared among all subplots.

## 5. Running statistics
No doubt, running helps people stay mentally and physically healthy and productive at any age. And it is great fun! When runners talk to each other about their hobby, we not only discuss our results, but we also discuss different training strategies. 
You'll know you're with a group of runners if you commonly hear questions like:

What is your average distance?
How fast do you run?
Do you measure your heart rate?
How often do you train?

Let's find the answers to these questions in my data. If you look back at plots in Task 4, you can see the answer to, Do you measure your heart rate? Before 2015: no. To look at the averages, let's only use the data from 2015 through 2018.
In pandas, the resample() method is similar to the groupby() method - with resample() you group by a specific time span. We'll use resample() to group the time series data by a sampling period and apply several methods to each sampling period. In our case, we'll resample annually and weekly.

## 6. Visualization with averages
Let's plot the long term averages of my distance run and my heart rate with their raw data to visually compare the averages to each training session. Again, we'll use the data from 2015 through 2018.
In this task, we will use matplotlib functionality for plot creation and customization.

## 7. Did I reach my goals?
To motivate myself to run regularly, I set a target goal of running 1000 km per year. Let's visualize my annual running distance (km) from 2013 through 2018 to see if I reached my goal each year. Only stars in the green region indicate success.

## 8. Am I progressing?
Let's dive a little deeper into the data to answer a tricky question: am I progressing in terms of my running skills? 
To answer this question, we'll decompose my weekly distance run and visually compare it to the raw data. A red trend line will represent the weekly distance run.
We are going to use statsmodels library to decompose the weekly trend.

## 9. Training intensity
Heart rate is a popular metric used to measure training intensity. Depending on age and fitness level, heart rates are grouped into different zones that people can target depending on training goals. A target heart rate during moderate-intensity activities is about 50-70% of maximum heart rate, while during vigorous physical activity it’s about 70-85% of maximum.
We'll create a distribution plot of my heart rate data by training intensity. It will be a visual presentation for the number of activities from predefined training zones. 

## 10. Detailed summary report
With all this data cleaning, analysis, and visualization, let's create detailed summary tables of my training. 
To do this, we'll create two tables. The first table will be a summary of the distance (km) and climb (m) variables for each training activity. The second table will list the summary statistics for the average speed (km/hr), climb (m), and distance (km) variables for each training activity.

## 11. Fun facts
To wrap up, let’s pick some fun facts out of the summary tables and solve the last exercise.
These data (my running history) represent 6 years, 2 months and 21 days. And I remember how many running shoes I went through–7.
FUN FACTS
- Average distance: 11.38 km
- Longest distance: 38.32 km
- Highest climb: 982 m
- Total climb: 57,278 m
- Total number of km run: 5,224 km
- Total runs: 459
- Number of running shoes gone through: 7 pairs

The story of Forrest Gump is well known–the man, who for no particular reason decided to go for a ""little run."" His epic run duration was 3 years, 2 months and 14 days (1169 days). In the picture you can see Forrest’s route of 24,700 km.  
FORREST RUN FACTS
- Average distance: 21.13 km
- Total number of km run: 24,700 km
- Total runs: 1169
- Number of running shoes gone through: ...

Assuming Forest and I go through running shoes at the same rate, figure out how many pairs of shoes Forrest needed for his run.


"
https://github.com/mrbarkis/DataCamp_projects/tree/master/Bad%20passwords%20and%20the%20NIST%20guidelines,Bad passwords and the NIST guidelines,python,Rasmus Bååth,Senior Data Scientist at King (Activision Blizzard),https://www.datacamp.com/projects/141,"Check what passwords fail to conform to the National Institute of Standards and Technology password guidelines.
",https://www.github.com/mrbarkis/DataCamp_projects/blob/master/Bad%20passwords%20and%20the%20NIST%20guidelines/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/Bad%20passwords%20and%20the%20NIST%20guidelines/notebook.ipynb,['import pandas as pd'],"['.csv', '.csv', '.str', '.txt', '.txt', '.str', '.str', '.str', '.str']","['.read_csv()', '.head()', '.len()', '.sum()', '.head()', '.read_csv()', '.head()', '.isin()', '.sum()', '.head()', '.read_csv()', '.lower()', '.isin()', '.sum()', '.head()', '.extract()', '.extract()', '.sum()', '.head()', '.contains()', '.any()', '.sum()', '.head()', '.head()']","['print()', 'len()', 'print()', 'print()', 'print()', 'print()', 'print()']","## 1. The NIST Special Publication 800-63B
If you – 50 years ago – needed to come up with a secret password you were probably part of a secret espionage organization or (more likely) you were pretending to be a spy when playing as a kid. Today, many of us are forced to come up with new passwords all the time when signing into sites and apps. As a password inventeur it is your responsibility to come up with good, hard-to-crack passwords. But it is also in the interest of sites and apps to make sure that you use good passwords. The problem is that it's really hard to define what makes a good password. However, the National Institute of Standards and Technology (NIST) knows what the second best thing is: To make sure you're at least not using a bad password. 
In this notebook, we will go through the rules in NIST Special Publication 800-63B which details what checks a verifier (what the NIST calls a second party responsible for storing and verifying passwords) should perform to make sure users don't pick bad passwords. We will go through the passwords of users from a fictional company and use python to flag the users with bad passwords. But us being able to do this already means the fictional company is breaking one of the rules of 800-63B:

Verifiers SHALL store memorized secrets in a form that is resistant to offline attacks. Memorized secrets SHALL be salted and hashed using a suitable one-way key derivation function.

That is, never save users' passwords in plaintext, always encrypt the passwords! Keeping this in mind for the next time we're building a password management system, let's load in the data.
Warning: The list of passwords and the fictional user database both contain real passwords leaked from real websites. These passwords have not been filtered in any way and include words that are explicit, derogatory and offensive.

## 2. Passwords should not be too short
If we take a look at the first 12 users above we already see some bad passwords. But let's not get ahead of ourselves and start flagging passwords manually. What is the first thing we should check according to the NIST Special Publication 800-63B?

Verifiers SHALL require subscriber-chosen memorized secrets to be at least 8 characters in length.

Ok, so the passwords of our users shouldn't be too short. Let's start by checking that!

## 3.  Common passwords people use
Already this simple rule flagged a couple of offenders among the first 12 users. Next up in Special Publication 800-63B is the rule that

verifiers SHALL compare the prospective secrets against a list that contains values known to be commonly-used, expected, or compromised.

Passwords obtained from previous breach corpuses.
Dictionary words.
Repetitive or sequential characters (e.g. ‘aaaaaa’, ‘1234abcd’).
Context-specific words, such as the name of the service, the username, and derivatives thereof.


We're going to check these in order and start with Passwords obtained from previous breach corpuses, that is, websites where hackers have leaked all the users' passwords. As many websites don't follow the NIST guidelines and encrypt passwords there now exist large lists of the most popular passwords. Let's start by loading in the 10,000 most common passwords which I've taken from here.

## 4.  Passwords should not be common passwords
The list of passwords was ordered, with the most common passwords first, and so we shouldn't be surprised to see passwords like 123456 and qwerty above. As hackers also have access to this list of common passwords, it's important that none of our users use these passwords!
Let's flag all the passwords in our user database that are among the top 10,000 used passwords.

## 5. Passwords should not be common words
Ay ay ay! It turns out many of our users use common passwords, and of the first 12 users there are already two. However, as most common passwords also tend to be short, they were already flagged as being too short. What is the next thing we should check?

Verifiers SHALL compare the prospective secrets against a list that contains [...] dictionary words.

This follows the same logic as before: It is easy for hackers to check users' passwords against common English words and therefore common English words make bad passwords. Let's check our users' passwords against the top 10,000 English words from Google's Trillion Word Corpus.

## 6. Passwords should not be your name
It turns out many of our passwords were common English words too! Next up on the NIST list:

Verifiers SHALL compare the prospective secrets against a list that contains [...] context-specific words, such as the name of the service, the username, and derivatives thereof.

Ok, so there are many things we could check here. One thing to notice is that our users' usernames consist of their first names and last names separated by a dot. For now, let's just flag passwords that are the same as either a user's first or last name.

[Here is a handy regexp cheat sheet and tester](http://www.pyregex.com/?id=eyJyZWdleCI6IiheXFx3KykiLCJmbGFncyI6MCwibWF0Y2hfdHlwZSI6Im1hdGNoIiwidGVzdF9zdHJpbmciOiJqYW1lcy5ib25kIn0%3D)

## 7. Passwords should not be repetitive
Milford Hubbard (user number 12 above), what where you thinking!? Ok, so the last thing we are going to check is a bit tricky:

verifiers SHALL compare the prospective secrets [so that they don't contain] repetitive or sequential characters (e.g. ‘aaaaaa’, ‘1234abcd’).

This is tricky to check because what is repetitive is hard to define. Is 11111 repetitive? Yes! Is 12345 repetitive? Well, kind of. Is 13579 repetitive? Maybe not..? To check for repetitiveness can be arbitrarily complex, but here we're only going to do something simple. We're going to flag all passwords that contain 4 or more repeated characters.

## 8. All together now!
Now we have implemented all the basic tests for bad passwords suggested by NIST Special Publication 800-63B! What's left is just to flag all bad passwords and maybe to send these users an e-mail that strongly suggests they change their password.

## 9. Otherwise, the password should be up to the user
In this notebook, we've implemented the password checks recommended by the NIST Special Publication 800-63B. It's certainly possible to better implement these checks, for example, by using a longer list of common passwords. Also note that the NIST checks in no way guarantee that a chosen password is good, just that it's not obviously bad.
Apart from the checks we've implemented above the NIST is also clear with what password rules should not be imposed:

Verifiers SHOULD NOT impose other composition rules (e.g., requiring mixtures of different character types or prohibiting consecutively repeated characters) for memorized secrets. Verifiers SHOULD NOT require memorized secrets to be changed arbitrarily (e.g., periodically).

So the next time a website or app tells you to ""include both a number, symbol and an upper and lower case character in your password"" you should send them a copy of NIST Special Publication 800-63B.

"
https://github.com/mrbarkis/DataCamp_projects/tree/master/Book%20Recommendations%20from%20Charles%20Darwin,Book Recommendations from Charles Darwin,python,Philippe Julien,Senior Data Scientist	at King,https://www.datacamp.com/projects/607,"Build a book recommendation system using NLP and the text of books like ""On the Origin of Species.""
",https://www.github.com/mrbarkis/DataCamp_projects/blob/master/Book%20Recommendations%20from%20Charles%20Darwin/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/Book%20Recommendations%20from%20Charles%20Darwin/notebook.ipynb,"['import glob', 'import re, os', 'import pickle', 'import pandas as pd', 'import matplotlib.pyplot as plt', 'from gensim import corpora', 'from gensim.models import TfidfModel', 'from gensim import similarities', 'from scipy.cluster import hierarchy']","['.txt', '.txt', '.path', '.txt', '.stem', '.p', '.p', '.columns', '.models', '.columns', '.columns', '.index', '.pyplot', '.plot', '.cluster', '.index']","['.glob()', '.sub()', '.read()', '.append()', '.append()', '.basename()', '.replace()', '.split()', '.lower()', '.split()', '.stem()', '.dump()', '.load()', '.Dictionary()', '.doc2bow()', '.DataFrame()', '.apply()', '.sort_values()', '.head()', '.DataFrame()', '.sort_values()', '.head()', '.MatrixSimilarity()', '.DataFrame()', '.sort_values()', '.barh()', '.xlabel()', '.title()', '.linkage()', '.dendrogram()']","['sorted()', 'open()', 'len()', 'range()', 'len()', 'print()', 'set()', 'PorterStemmer()', 'open()', 'open()', 'TfidfModel()', 'list()']","## 1. Darwin's bibliography

Charles Darwin is one of the few universal figures of science. His most renowned work is without a doubt his ""On the Origin of Species"" published in 1859 which introduced the concept of natural selection. But Darwin wrote many other books on a wide range of topics, including geology, plants or his personal life. In this notebook, we will automatically detect how closely related his books are to each other.
To this purpose, we will develop the bases of a content-based book recommendation system, which will determine which books are close to each other based on how similar the discussed topics are. The methods we will use are commonly used in text- or documents-heavy industries such as legal, tech or customer support to perform some common task such as text classification or handling search engine queries.
Let's take a look at the books we'll use in our recommendation system.

[How to list files from a folder using glob](https://stackoverflow.com/questions/3964681/find-all-files-in-a-directory-with-extension-txt-in-python)

[gensim tutorials](https://radimrehurek.com/gensim/auto_examples/index.html#core-tutorials)

[This DataCamp article on stemming](https://www.datacamp.com/community/tutorials/stemming-lemmatization-python)


## 2. Load the contents of each book into Python
As a first step, we need to load the content of these books into Python and do some basic pre-processing to facilitate the downstream analyses. We call such a collection of texts a corpus. We will also store the titles for these books for future reference and print their respective length to get a gauge for their contents.

## 3. Find ""On the Origin of Species""
For the next parts of this analysis, we will often check the results returned by our method for a given book. For consistency, we will refer to Darwin's most famous book: ""On the Origin of Species."" Let's find to which index this book is associated.

## 4. Tokenize the corpus
As a next step, we need to transform the corpus into a format that is easier to deal with for the downstream analyses. We will tokenize our corpus, i.e., transform each text into a list of the individual words (called tokens) it is made of. To check the output of our process, we will print the first 20 tokens of ""On the Origin of Species"".

## 5. Stemming of the tokenized corpus
If you have read On the Origin of Species, you will have noticed that Charles Darwin can use different words to refer to a similar concept. For example, the concept of selection can be described by words such as selection, selective, select or selects. This will dilute the weight given to this concept in the book and potentially bias the results of the analysis.
To solve this issue, it is a common practice to use a stemming process, which will group together the inflected forms of a word so they can be analysed as a single item: the stem. In our On the Origin of Species example, the words related to the concept of selection would be gathered under the select stem.
As we are analysing 20 full books, the stemming algorithm can take several minutes to run and, in order to make the process faster, we will directly load the final results from a pickle file and review the method used to generate it.

## 6. Building a bag-of-words model
Now that we have transformed the texts into stemmed tokens, we need to build models that will be useable by downstream algorithms.
First, we need to will create a universe of all words contained in our corpus of Charles Darwin's books, which we call a dictionary. Then, using the stemmed tokens and the dictionary, we will create bag-of-words models (BoW) of each of our texts. The BoW models will represent our books as a list of all uniques tokens they contain associated with their respective number of occurrences. 
To better understand the structure of such a model, we will print the five first elements of one of the ""On the Origin of Species"" BoW model.

## 7. The most common words of a given book
The results returned by the bag-of-words model is certainly easy to use for a computer but hard to interpret for a human. It is not straightforward to understand which stemmed tokens are present in a given book from Charles Darwin, and how many occurrences we can find.
In order to better understand how the model has been generated and visualize its content, we will transform it into a DataFrame and display the 10 most common stems for the book ""On the Origin of Species"".

## 8. Build a tf-idf model
If it wasn't for the presence of the stem ""speci"", we would have a hard time to guess this BoW model comes from the On the Origin of Species book. The most recurring words are, apart from few exceptions, very common and unlikely to carry any information peculiar to the given book. We need to use an additional step in order to determine which tokens are the most specific to a book.
To do so, we will use a tf-idf model (term frequency–inverse document frequency). This model defines the importance of each word depending on how frequent it is in this text and how infrequent it is in all the other documents. As a result, a high tf-idf score for a word will indicate that this word is specific to this text.
After computing those scores, we will print the 10 words most specific to the ""On the Origin of Species"" book (i.e., the 10 words with the highest tf-idf score).

## 9. The results of the tf-idf model
Once again, the format of those results is hard to interpret for a human. Therefore, we will transform it into a more readable version and display the 10 most specific words for the ""On the Origin of Species"" book.

## 10. Compute distance between texts
The results of the tf-idf algorithm now return stemmed tokens which are specific to each book. We can, for example, see that topics such as selection, breeding or domestication are defining ""On the Origin of Species"" (and yes, in this book, Charles Darwin talks quite a lot about pigeons too). Now that we have a model associating tokens to how specific they are to each book, we can measure how related to books are between each other.
To this purpose, we will use a measure of similarity called cosine similarity and we will visualize the results as a distance matrix, i.e., a matrix showing all pairwise distances between Darwin's books.

## 11. The book most similar to ""On the Origin of Species""
We now have a matrix containing all the similarity measures between any pair of books from Charles Darwin! We can now use this matrix to quickly extract the information we need, i.e., the distance between one book and one or several others. 
As a first step, we will display which books are the most similar to ""On the Origin of Species,"" more specifically we will produce a bar chart showing all books ranked by how similar they are to Darwin's landmark work.

## 12. Which books have similar content?
This turns out to be extremely useful if we want to determine a given book's most similar work. For example, we have just seen that if you enjoyed ""On the Origin of Species,"" you can read books discussing similar concepts such as ""The Variation of Animals and Plants under Domestication"" or ""The Descent of Man, and Selection in Relation to Sex."" If you are familiar with Darwin's work, these suggestions will likely seem natural to you. Indeed, On the Origin of Species has a whole chapter about domestication and The Descent of Man, and Selection in Relation to Sex applies the theory of natural selection to human evolution. Hence, the results make sense.
However, we now want to have a better understanding of the big picture and see how Darwin's books are generally related to each other (in terms of topics discussed). To this purpose, we will represent the whole similarity matrix as a dendrogram, which is a standard tool to display such data. This last approach will display all the information about book similarities at once. For example, we can find a book's closest relative but, also, we can visualize which groups of books have similar topics (e.g., the cluster about Charles Darwin personal life with his autobiography and letters). If you are familiar with Darwin's bibliography, the results should not surprise you too much, which indicates the method gives good results. Otherwise, next time you read one of the author's book, you will know which other books to read next in order to learn more about the topics it addressed.

"
https://github.com/mrbarkis/DataCamp_projects/tree/master/Comparing%20Cosmetics%20by%20Ingredients,Comparing Cosmetics by Ingredients,python,Jiwon Jeong,Graduate Research Assistant at Yonsei University,https://www.datacamp.com/projects/695,"Process ingredient lists for cosmetics on Sephora then visualize similarity using t-SNE and Bokeh.
",https://www.github.com/mrbarkis/DataCamp_projects/blob/master/Comparing%20Cosmetics%20by%20Ingredients/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/Comparing%20Cosmetics%20by%20Ingredients/notebook.ipynb,"['import pandas as pd', 'import numpy as np', 'from sklearn.manifold import TSNE', 'from bokeh.io import show, output_notebook, push_notebook', 'from bokeh.plotting import figure', 'from bokeh.models import ColumnDataSource, HoverTool']","['.manifold', '.csv', '.Label', '.io', '.plotting', '.models', '.8', '.Ingredients', '.values', '.Ingredients', '.values']","['.read_csv()', '.head()', '.value_counts()', '.query()', '.query()', '.reset_index()', '.lower()', '.split()', '.append()', '.zeros()', '.zeros()', '.fit_transform()', '.circle()', '.add_tools()']","['display()', 'display()', 'range()', 'len()', 'print()', 'len()', 'len()', 'oh_encoder()', 'oh_encoder()', 'TSNE()', 'output_notebook()', 'ColumnDataSource()', 'figure()', 'HoverTool()', 'show()', 'display()', 'print()', 'display()', 'print()']","## 1. Cosmetics, chemicals... it's complicated
Whenever I want to try a new cosmetic item, it's so difficult to choose. It's actually more than difficult. It's sometimes scary because new items that I've never tried end up giving me skin trouble. We know the information we need is on the back of each product, but it's really hard to interpret those ingredient lists unless you're a chemist. You may be able to relate to this situation.

So instead of buying and hoping for the best, why don't we use data science to help us predict which products may be good fits for us? In this notebook, we are going to create a content-based recommendation system where the 'content' will be the chemical components of cosmetics. Specifically, we will process ingredient lists for 1472 cosmetics on Sephora via word embedding, then visualize ingredient similarity using a machine learning method called t-SNE and an interactive visualization library called Bokeh. Let's inspect our data first.

## 2. Focus on one product category and one skin type
There are six categories of product in our data (moisturizers, cleansers, face masks, eye creams, and sun protection) and there are five different skin types (combination, dry, normal, oily and sensitive). Because individuals have different product needs as well as different skin types, let's set up our workflow so its outputs (a t-SNE model and a visualization of that model) can be customized. For the example in this notebook, let's focus in on moisturizers for those with dry skin by filtering the data accordingly.

## 3. Tokenizing the ingredients
To get to our end goal of comparing ingredients in each product, we first need to do some preprocessing tasks and bookkeeping of the actual words in each product's ingredients list. The first step will be tokenizing the list of ingredients in Ingredients column. After splitting them into tokens, we'll make a binary bag of words. Then we will create a dictionary with the tokens, ingredient_idx, which will have the following format:
{ ""ingredient"": index value, ... }

## 4. Initializing a document-term matrix (DTM)
The next step is making a document-term matrix (DTM). Here each cosmetic product will correspond to a document, and each chemical composition will correspond to a term. This means we can think of the matrix as a “cosmetic-ingredient” matrix. The size of the matrix should be as the picture shown below.

To create this matrix, we'll first make an empty matrix filled with zeros. The length of the matrix is the total number of cosmetic products in the data. The width of the matrix is the total number of ingredients. After initializing this empty matrix, we'll fill it in the following tasks. 

## 5. Creating a counter function
Before we can fill the matrix, let's create a function to count the tokens (i.e., an ingredients list) for each row. Our end goal is to fill the matrix with 1 or 0: if an ingredient is in a cosmetic, the value is 1. If not, it remains 0. The name of this function, oh_encoder, will become clear next.

## 6. The Cosmetic-Ingredient matrix!
Now we'll apply the oh_encoder() functon to the tokens in corpus and set the values at each row of this matrix. So the result will tell us what ingredients each item is composed of. For example, if a cosmetic item contains water, niacin, decyl aleate and sh-polypeptide-1, the outcome of this item will be as follows. 

This is what we called one-hot encoding. By encoding each ingredient in the items, the Cosmetic-Ingredient matrix will be filled with binary values. 

## 7. Dimension reduction with t-SNE
The dimensions of the existing matrix is (190, 2233), which means there are 2233 features in our data. For visualization, we should downsize this into two dimensions. We'll use t-SNE for reducing the dimension of the data here.
T-distributed Stochastic Neighbor Embedding (t-SNE) is a nonlinear dimensionality reduction technique that is well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, this technique can reduce the dimension of data while keeping the similarities between the instances. This enables us to make a plot on the coordinate plane, which can be said as vectorizing. All of these cosmetic items in our data will be vectorized into two-dimensional coordinates, and the distances between the points will indicate the similarities between the items. 

## 8. Let's map the items with Bokeh
We are now ready to start creating our plot. With the t-SNE values, we can plot all our items on the coordinate plane. And the coolest part here is that it will also show us the name, the brand, the price and the rank of each item. Let's make a scatter plot using Bokeh and add a hover tool to show that information. Note that we won't display the plot yet as we will make some more additions to it.

## 9. Adding a hover tool
Why don't we add a hover tool? Adding a hover tool allows us to check the information of each item whenever the cursor is directly over a glyph. We'll add tooltips with each product's name, brand, price, and rank (i.e., rating).

## 10. Mapping the cosmetic items
Finally, it's show time! Let's see how the map we've made looks like. Each point on the plot corresponds to the cosmetic items. Then what do the axes mean here? The axes of a t-SNE plot aren't easily interpretable in terms of the original data. Like mentioned above, t-SNE is a visualizing technique to plot high-dimensional data in a low-dimensional space. Therefore, it's not desirable to interpret a t-SNE plot quantitatively.
Instead, what we can get from this map is the distance between the points (which items are close and which are far apart). The closer the distance between the two items is, the more similar the composition they have. Therefore this enables us to compare the items without having any chemistry background.

## 11. Comparing two products
Since there are so many cosmetics and so many ingredients, the plot doesn't have many super obvious patterns that simpler t-SNE plots can have (example). Our plot requires some digging to find insights, but that's okay!
Say we enjoyed a specific product, there's an increased chance we'd enjoy another product that is similar in chemical composition.  Say we enjoyed AmorePacific's Color Control Cushion Compact Broad Spectrum SPF 50+. We could find this product on the plot and see if a similar product(s) exist. And it turns out it does! If we look at the points furthest left on the plot, we see  LANEIGE's BB Cushion Hydra Radiance SPF 50 essentially overlaps with the AmorePacific product. By looking at the ingredients, we can visually confirm the compositions of the products are similar (though it is difficult to do, which is why we did this analysis in the first place!), plus LANEIGE's version is $22 cheaper and actually has higher ratings.
It's not perfect, but it's useful. In real life, we can actually use our little ingredient-based recommendation engine help us make educated cosmetic purchase choices.

"
https://github.com/mrbarkis/DataCamp_projects/tree/master/Disney%20Movies%20and%20Box%20Office%20Success,Disney Movies and Box Office Success,python,Sirinda Palahan,Assistant Professor at University of the Thai Chamber of Commerce,https://www.datacamp.com/projects/740,"Explore Disney movie data, then build a linear regression model to predict box office success.
",https://www.github.com/mrbarkis/DataCamp_projects/blob/master/Disney%20Movies%20and%20Box%20Office%20Success/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/Disney%20Movies%20and%20Box%20Office%20Success/notebook.ipynb,"['import pandas as pd', 'import seaborn as sns', 'import numpy as np', 'from sklearn.linear_model import LinearRegression']","['.csv', '.dt', '.year', '.linear_model', '.intercept_', '.coef_', '.random', '.intercept_', '.coef_', '.5', '.5', '.5', '.5']","['.read_csv()', '.head()', '.sort_values()', '.head()', '.groupby()', '.mean()', '.reset_index()', '.head()', '.relplot()', '.get_dummies()', '.head()', '.fit()', '.arange()', '.empty()', '.empty()', '.choice()', '.get_dummies()', '.fit()', '.percentile()', '.percentile()']","['LinearRegression()', 'print()', 'len()', 'range()', 'len()', 'LinearRegression()', 'print()', 'print()']","## 1. The dataset
Walt Disney Studios is the foundation on which The Walt Disney Company was built. The Studios has produced more than 600 films since their debut film,  Snow White and the Seven Dwarfs in 1937. While many of its films were big hits, some of them were not. In this notebook, we will explore a dataset of Disney movies and analyze what contributes to the success of Disney movies.

First, we will take a look at the Disney data compiled by Kelly Garrett. The data contains 579 Disney movies with six features: movie title, release date, genre, MPAA rating, total gross, and inflation-adjusted gross. 
Let's load the file and see what the data looks like.

## 2. Top ten movies at the box office
Let's started by exploring the data. We will check which are the 10 Disney movies that have earned the most at the box office. We can do this by sorting movies by their inflation-adjusted gross (we will call it adjusted gross from this point onward). 

## 3. Movie genre trend
From the top 10 movies above, it seems that some genres are more popular than others. So, we will check which genres are growing stronger in popularity. To do this, we will group movies by genre and then by year to see the adjusted gross of each genre in each year.

## 4. Visualize the genre popularity trend
We will make a plot out of these means of groups to better see how box office revenues have changed over time.

## 5. Data transformation
The line plot supports our belief that some genres are growing faster in popularity than others. For Disney movies, Action and Adventure genres are growing the fastest. Next, we will build a linear regression model to understand the relationship between genre and box office gross. 
Since linear regression requires numerical variables and the genre variable is a categorical variable, we'll use a technique called one-hot encoding to convert the categorical variables to numerical. This technique transforms each category value into a new column and assigns a 1 or 0 to the column. 
For this dataset, there will be 11 dummy variables, one for each genre except the action genre which we will use as a baseline. For example, if a movie is an adventure movie, like The Lion King, the adventure variable will be 1 and other dummy variables will be 0. Since the action genre is our baseline, if a movie is an action movie, such as The Avengers, all dummy variables will be 0.

## 6. The genre effect
Now that we have dummy variables, we can build a linear regression model to predict the adjusted gross using these dummy variables.
From the regression model, we can check the effect of each genre by looking at its coefficient given in units of box office gross dollars. We will focus on the impact of action and adventure genres here. (Note that the intercept and the first coefficient values represent the effect of action and adventure genres respectively). We expect that movies like the Lion King or Star Wars would perform better for box office.

## 7. Confidence intervals for regression parameters  (i)
Next, we will compute 95% confidence intervals for the intercept and coefficients. The 95% confidence intervals for the intercept  a and coefficient bi means that the intervals have a probability of 95% to contain the true value a and coefficient bi respectively. If there is a significant relationship between a given genre and the adjusted gross, the confidence interval of its coefficient should exclude 0.      
We will calculate the confidence intervals using the pairs bootstrap method. 

## 8. Confidence intervals for regression parameters  (ii)
After the initialization, we will perform pair bootstrap estimates for the regression parameters. Note that we will draw a sample from a set of (genre, adjusted gross) data where the genre is the original genre variable. We will perform one-hot encoding after that. 

## 9. Confidence intervals for regression parameters (iii)
Finally, we compute 95% confidence intervals for the intercept and coefficient and examine if they exclude 0. If one of them (or both) does, then it is unlikely that the value is 0 and we can conclude that there is a significant relationship between that genre and the adjusted gross. 

## 10. Should Disney make more action and adventure movies?
The confidence intervals from the bootstrap method for the intercept and coefficient do not contain the value zero, as we have already seen that lower and upper bounds of both confidence intervals are positive. These tell us that it is likely that the adjusted gross is significantly correlated with the action and adventure genres. 
From the results of the bootstrap analysis and the trend plot we have done earlier, we could say that Disney movies with plots that fit into the action and adventure genre, according to our data, tend to do better in terms of adjusted gross than other genres. So we could expect more Marvel, Star Wars, and live-action movies in the upcoming years!

"
https://github.com/mrbarkis/DataCamp_projects/tree/master/Do%20Left-handed%20People%20Really%20Die%20Young%3F,Do Left-handed People Really Die Young?,python,Madeleine Bonsma-Fisher,PhD Candidate at University of Toronto,https://www.datacamp.com/projects/479,"Use pandas and Bayesian statistics to see if left-handed people actually die earlier than righties.
",https://www.github.com/mrbarkis/DataCamp_projects/blob/master/Do%20Left-handed%20People%20Really%20Die%20Young%3F/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/Do%20Left-handed%20People%20Really%20Die%20Young%3F/notebook.ipynb,"['import pandas as pd', 'import matplotlib.pyplot as plt', 'import numpy as np']","['.pyplot', '.githubusercontent', '.com', '.csv', '.loc', '.shape', '.githubusercontent', '.com', '.tsv', '.values', '.values', '.Series', '.Series', '.array']","['.read_csv()', '.head()', '.subplots()', '.plot()', '.plot()', '.legend()', '.set_xlabel()', '.set_ylabel()', '.subplots()', '.plot()', '.set_xlabel()', '.set_ylabel()', '.tail()', '.mean()', '.mean()', '.head()', '.mean()', '.isin()', '.zeros()', '.logical_and()', '.read_csv()', '.dropna()', '.info()', '.subplots()', '.plot()', '.set_xlabel()', '.set_ylabel()', '.multiply()', '.sum()', '.sum()', '.sum()', '.sum()', '.arange()', '.subplots()', '.plot()', '.plot()', '.legend()', '.set_xlabel()', '.set_ylabel()', '.nansum()', '.array()', '.nansum()', '.array()', '.nansum()', '.array()', '.nansum()', '.array()']","['P()', 'P_lh_given_A()', 'P()', 'print()', 'P_lh()', 'P()', 'P_lh_given_A()', 'print()', 'P_lh()', 'P_A_given_lh()', 'P()', 'P_lh()', 'P()', 'P_lh_given_A()', 'P()', 'P_A_given_rh()', 'P()', 'P_lh()', 'P_lh_given_A()', 'P_A_given_lh()', 'P_A_given_rh()', 'print()', 'str()', 'round()', 'print()', 'str()', 'round()', 'print()', 'str()', 'round()', 'P_A_given_lh()', 'P_A_given_rh()', 'print()', 'str()', 'round()']","## 1. Where are the old left-handed people?

Barack Obama is left-handed. So are Bill Gates and Oprah Winfrey; so were Babe Ruth and Marie Curie. A 1991 study reported that left-handed people die on average nine years earlier than right-handed people. Nine years! Could this really be true? 
In this notebook, we will explore this phenomenon using age distribution data to see if we can reproduce a difference in average age at death purely from the changing rates of left-handedness over time, refuting the claim of early death for left-handers. This notebook uses pandas and Bayesian statistics to analyze the probability of being a certain age at death given that you are reported as left-handed or right-handed.
A National Geographic survey in 1986 resulted in over a million responses that included age, sex, and hand preference for throwing and writing. Researchers Avery Gilbert and Charles Wysocki analyzed this data and noticed that rates of left-handedness were around 13% for people younger than 40 but decreased with age to about 5% by the age of 80. They concluded based on analysis of a subgroup of people who throw left-handed but write right-handed that this age-dependence was primarily due to changing social acceptability of left-handedness. This means that the rates aren't a factor of age specifically but rather of the year you were born, and if the same study was done today, we should expect a shifted version of the same distribution as a function of age. Ultimately, we'll see what effect this changing rate has on the apparent mean age of death of left-handed people, but let's start by plotting the rates of left-handedness as a function of age.
This notebook uses two datasets: death distribution data for the United States from the year 1999 (source website here) and rates of left-handedness digitized from a figure in this 1992 paper by Gilbert and Wysocki. 

## 2. Rates of left-handedness over time
Let's convert this data into a plot of the rates of left-handedness as a function of the year of birth, and average over male and female to get a single rate for both sexes. 
Since the study was done in 1986, the data after this conversion will be the percentage of people alive in 1986 who are left-handed as a function of the year they were born. 

## 3. Applying Bayes' rule
The probability of dying at a certain age given that you're left-handed is not equal to the probability of being left-handed given that you died at a certain age. This inequality is why we need Bayes' theorem, a statement about conditional probability which allows us to update our beliefs after seeing evidence. 
We want to calculate the probability of dying at age A given that you're left-handed. Let's write this in shorthand as P(A | LH). We also want the same quantity for right-handers: P(A | RH). 
Here's Bayes' theorem for the two events we care about: left-handedness (LH) and dying at age A.
$$P(A | LH) = \frac{P(LH|A) P(A)}{P(LH)}$$
P(LH | A) is the probability that you are left-handed given that you died at age A. P(A) is the overall probability of dying at age A, and P(LH) is the overall probability of being left-handed. We will now calculate each of these three quantities, beginning with P(LH | A).
To calculate P(LH | A) for ages that might fall outside the original data, we will need to extrapolate the data to earlier and later years. Since the rates flatten out in the early 1900s and late 1900s, we'll use a few points at each end and take the mean to extrapolate the rates on each end. The number of points used for this is arbitrary, but we'll pick 10 since the data looks flat-ish until about 1910. 

## 4. When do people normally die?
To estimate the probability of living to an age A, we can use data that gives the number of people who died in a given year and how old they were to create a distribution of ages of death. If we normalize the numbers to the total number of people who died, we can think of this data as a probability distribution that gives the probability of dying at age A. The data we'll use for this is from the entire US for the year 1999 - the closest I could find for the time range we're interested in. 
In this block, we'll load in the death distribution data and plot it. The first column is the age, and the other columns are the number of people who died at that age. 

## 5. The overall probability of left-handedness
In the previous code block we loaded data to give us P(A), and now we need P(LH). P(LH) is the probability that a person who died in our particular study year is left-handed, assuming we know nothing else about them. This is the average left-handedness in the population of deceased people, and we can calculate it by summing up all of the left-handedness probabilities for each age, weighted with the number of deceased people at each age, then divided by the total number of deceased people to get a probability. In equation form, this is what we're calculating, where N(A) is the number of people who died at age A (given by the dataframe death_distribution_data):



## 6. Putting it all together: dying while left-handed (i)
Now we have the means of calculating all three quantities we need: P(A), P(LH), and P(LH | A). We can combine all three using Bayes' rule to get P(A | LH), the probability of being age A at death (in the study year) given that you're left-handed. To make this answer meaningful, though, we also want to compare it to P(A | RH), the probability of being age A at death given that you're right-handed. 
We're calculating the following quantity twice, once for left-handers and once for right-handers.
$$P(A | LH) = \frac{P(LH|A) P(A)}{P(LH)}$$
First, for left-handers.


## 7. Putting it all together: dying while left-handed (ii)
And now for right-handers.

## 8. Plotting the distributions of conditional probabilities
Now that we have functions to calculate the probability of being age A at death given that you're left-handed or right-handed, let's plot these probabilities for a range of ages of death from 6 to 120. 
Notice that the left-handed distribution has a bump below age 70: of the pool of deceased people, left-handed people are more likely to be younger. 

## 9. Moment of truth: age of left and right-handers at death
Finally, let's compare our results with the original study that found that left-handed people were nine years younger at death on average. We can do this by calculating the mean of these probability distributions in the same way we calculated P(LH) earlier, weighting the probability distribution by age and summing over the result.
$$\text{Average age of left-handed people at death} = \sum_A A P(A | LH)$$
$$\text{Average age of right-handed people at death} = \sum_A A P(A | RH)$$

## 10. Final comments
We got a pretty big age gap between left-handed and right-handed people purely as a result of the changing rates of left-handedness in the population, which is good news for left-handers: you probably won't die young because of your sinisterness. The reported rates of left-handedness have increased from just 3% in the early 1900s to about 11% today, which means that older people are much more likely to be reported as right-handed than left-handed, and so looking at a sample of recently deceased people will have more old right-handers.
Our number is still less than the 9-year gap measured in the study. It's possible that some of the approximations we made are the cause: 

We used death distribution data from almost ten years after the study (1999 instead of 1991), and we used death data from the entire United States instead of California alone (which was the original study). 
We extrapolated the left-handedness survey results to older and younger age groups, but it's possible our extrapolation wasn't close enough to the true rates for those ages. 

One thing we could do next is figure out how much variability we would expect to encounter in the age difference purely because of random sampling: if you take a smaller sample of recently deceased people and assign handedness with the probabilities of the survey, what does that distribution look like? How often would we encounter an age gap of nine years using the same data and assumptions? We won't do that here, but it's possible with this data and the tools of random sampling. 

To finish off, let's calculate the age gap we'd expect if we did the study in 2018 instead of in 1990. The gap turns out to be much smaller since rates of left-handedness haven't increased for people born after about 1960. Both the National Geographic study and the 1990 study happened at a unique time - the rates of left-handedness had been changing across the lifetimes of most people alive, and the difference in handedness between old and young was at its most striking. 

"
https://github.com/mrbarkis/DataCamp_projects/tree/master/Dr.%20Semmelweis%20and%20the%20Discovery%20of%20Handwashing,Dr. Semmelweis and the Discovery of Handwashing,python,Rasmus Bååth,Senior Data Scientist at King (Activision Blizzard),https://www.datacamp.com/projects/20,"Reanalyse the data behind one of the most important discoveries of modern medicine: handwashing.
",https://www.github.com/mrbarkis/DataCamp_projects/blob/master/Dr.%20Semmelweis%20and%20the%20Discovery%20of%20Handwashing/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/Dr.%20Semmelweis%20and%20the%20Discovery%20of%20Handwashing/notebook.ipynb,"['import pandas as pd', 'import pandas as pd']","['.csv', '.csv', '.deaths', '.births', '.clinic', '.clinic', '.csv', '.csv', '.deaths', '.births', '.date', '.date', '.025', '.975']","['.read_csv()', '.plot()', '.plot()', '.set_ylabel()', '.read_csv()', '.head()', '.plot()', '.set_ylabel()', '.to_datetime()', '.plot()', '.plot()', '.set_ylabel()', '.mean()', '.mean()', '.sample()', '.sample()', '.append()', '.mean()', '.mean()', '.Series()', '.quantile()']",['range()'],"## 1. Meet Dr. Ignaz Semmelweis


This is Dr. Ignaz Semmelweis, a Hungarian physician born in 1818 and active at the Vienna General Hospital. If Dr. Semmelweis looks troubled it's probably because he's thinking about childbed fever: A deadly disease affecting women that just have given birth. He is thinking about it because in the early 1840s at the Vienna General Hospital as many as 10% of the women giving birth die from it. He is thinking about it because he knows the cause of childbed fever: It's the contaminated hands of the doctors delivering the babies. And they won't listen to him and wash their hands!
In this notebook, we're going to reanalyze the data that made Semmelweis discover the importance of handwashing. Let's start by looking at the data that made Semmelweis realize that something was wrong with the procedures at Vienna General Hospital.

## 2. The alarming number of deaths
The table above shows the number of women giving birth at the two clinics at the Vienna General Hospital for the years 1841 to 1846. You'll notice that giving birth was very dangerous; an alarming number of women died as the result of childbirth, most of them from childbed fever.
We see this more clearly if we look at the proportion of deaths out of the number of women giving birth. Let's zoom in on the proportion of deaths at Clinic 1.

## 3. Death at the clinics
If we now plot the proportion of deaths at both clinic 1 and clinic 2  we'll see a curious pattern...

## 4. The handwashing begins
Why is the proportion of deaths constantly so much higher in Clinic 1? Semmelweis saw the same pattern and was puzzled and distressed. The only difference between the clinics was that many medical students served at Clinic 1, while mostly midwife students served at Clinic 2. While the midwives only tended to the women giving birth, the medical students also spent time in the autopsy rooms examining corpses. 
Semmelweis started to suspect that something on the corpses, spread from the hands of the medical students, caused childbed fever. So in a desperate attempt to stop the high mortality rates, he decreed: Wash your hands! This was an unorthodox and controversial request, nobody in Vienna knew about bacteria at this point in time. 
Let's load in monthly data from Clinic 1 to see if the handwashing had any effect.

## 5. The effect of handwashing
With the data loaded we can now look at the proportion of deaths over time. In the plot below we haven't marked where obligatory handwashing started, but it reduced the proportion of deaths to such a degree that you should be able to spot it!

## 6. The effect of handwashing highlighted
Starting from the summer of 1847 the proportion of deaths is drastically reduced and, yes, this was when Semmelweis made handwashing obligatory. 
The effect of handwashing is made even more clear if we highlight this in the graph.

## 7. More handwashing, fewer deaths?
Again, the graph shows that handwashing had a huge effect. How much did it reduce the monthly proportion of deaths on average?

## 8. A Bootstrap analysis of Semmelweis handwashing data
It reduced the proportion of deaths by around 8 percentage points! From 10% on average to just 2% (which is still a high number by modern standards). 
To get a feeling for the uncertainty around how much handwashing reduces mortalities we could look at a confidence interval (here calculated using the bootstrap method).

## 9. The fate of Dr. Semmelweis
So handwashing reduced the proportion of deaths by between 6.7 and 10 percentage points, according to a 95% confidence interval. All in all, it would seem that Semmelweis had solid evidence that handwashing was a simple but highly effective procedure that could save many lives.
The tragedy is that, despite the evidence, Semmelweis' theory — that childbed fever was caused by some ""substance"" (what we today know as bacteria) from autopsy room corpses — was ridiculed by contemporary scientists. The medical community largely rejected his discovery and in 1849 he was forced to leave the Vienna General Hospital for good.
One reason for this was that statistics and statistical arguments were uncommon in medical science in the 1800s. Semmelweis only published his data as long tables of raw data, but he didn't show any graphs nor confidence intervals. If he would have had access to the analysis we've just put together he might have been more successful in getting the Viennese doctors to wash their hands.

"
https://github.com/mrbarkis/DataCamp_projects/tree/master/Exploring%20the%20Bitcoin%20Cryptocurrency%20Market,Exploring the Bitcoin Cryptocurrency Market,python,Juan González-Vallinas,Director Data Science at multilayer.io,https://www.datacamp.com/projects/82,"You will explore the market capitalization of Bitcoin and other cryptocurrencies.
",https://www.github.com/mrbarkis/DataCamp_projects/blob/master/Exploring%20the%20Bitcoin%20Cryptocurrency%20Market/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/Exploring%20the%20Bitcoin%20Cryptocurrency%20Market/notebook.ipynb,"['import pandas as pd', 'import matplotlib.pyplot as plt']","['.pyplot', '.figure_format', '.style', '.csv', '.csv', '.market_cap_usd', '.market_cap_usd', '.market_cap_perc', '.plot', '.market_cap_usd', '.plot', '.plot', '.plot', '.percent_change_24h', '.percent_change_7d', '.id']","['.use()', '.read_csv()', '.count()', '.count()', '.info()', '.query()', '.query()', '.notnull()', '.count()', '.count()', '.sort_values()', '.set_index()', '.sum()', '.assign()', '.bar()', '.set_ylabel()', '.bar()', '.set_ylabel()', '.set_xlabel()', '.dropna()', '.set_index()', '.info()', '.head()', '.sort_values()', '.head()', '.subplots()', '.bar()', '.suptitle()', '.set_ylabel()', '.bar()', '.sort_values()', '.query()', '.query()', '.count()', '.bar()']","['top10_subplot()', 'top10_subplot()', 'top10_subplot()', 'capcount()', 'capcount()', 'capcount()', 'capcount()']","## 1. Bitcoin and Cryptocurrencies: Full dataset, filtering, and reproducibility
Since the launch of Bitcoin in 2008, hundreds of similar projects based on the blockchain technology have emerged. We call these cryptocurrencies (also coins or cryptos in the Internet slang). Some are extremely valuable nowadays, and others may have the potential to become extremely valuable in the future1. In fact, on the 6th of December of 2017, Bitcoin has a market capitalization above $200 billion. 

 
The astonishing increase of Bitcoin market capitalization in 2017.
*1 WARNING: The cryptocurrency market is exceptionally volatile2 and any money you put in might disappear into thin air.  Cryptocurrencies mentioned here might be scams similar to Ponzi Schemes or have many other issues (overvaluation, technical, etc.). Please do not mistake this for investment advice. *
2 Update on March 2020: Well, it turned out to be volatile indeed :D
That said, let's get to business. We will start with a CSV we conveniently downloaded on the 6th of December of 2017 using the coinmarketcap API (NOTE: The public API went private in 2020 and is no longer available) named datasets/coinmarketcap_06122017.csv. 

## 2. Discard the cryptocurrencies without a market capitalization
Why do the count() for id and market_cap_usd differ above? It is because some cryptocurrencies listed in coinmarketcap.com have no known market capitalization, this is represented by NaN in the data, and NaNs are not counted by count(). These cryptocurrencies are of little interest to us in this analysis, so they are safe to remove.

## 3. How big is Bitcoin compared with the rest of the cryptocurrencies?
At the time of writing, Bitcoin is under serious competition from other projects, but it is still dominant in market capitalization. Let's plot the market capitalization for the top 10 coins as a barplot to better visualize this.

## 4. Making the plot easier to read and more informative
While the plot above is informative enough, it can be improved. Bitcoin is too big, and the other coins are hard to distinguish because of this. Instead of the percentage, let's use a log10 scale of the ""raw"" capitalization. Plus, let's use color to group similar coins and make the plot more informative1. 
For the colors rationale: bitcoin-cash and bitcoin-gold are forks of the bitcoin blockchain2. Ethereum and Cardano both offer Turing Complete smart contracts. Iota and Ripple are not minable. Dash, Litecoin, and Monero get their own color.
1 This coloring is a simplification. There are more differences and similarities that are not being represented here.
2 The bitcoin forks are actually very different, but it is out of scope to talk about them here. Please see the warning above and do your own research.

## 5. What is going on?! Volatility in cryptocurrencies
The cryptocurrencies market has been spectacularly volatile since the first exchange opened. This notebook didn't start with a big, bold warning for nothing. Let's explore this volatility a bit more! We will begin by selecting and plotting the 24 hours and 7 days percentage change, which we already have available.

## 6. Well, we can already see that things are *a bit* crazy
It seems you can lose a lot of money quickly on cryptocurrencies. Let's plot the top 10 biggest gainers and top 10 losers in market capitalization.

## 7. Ok, those are... interesting. Let's check the weekly Series too.
800% daily increase?! Why are we doing this tutorial and not buying random coins?1
After calming down, let's reuse the function defined above to see what is going weekly instead of daily.
1 Please take a moment to understand the implications of the red plots on how much value some cryptocurrencies lose in such short periods of time

## 8. How small is small?
The names of the cryptocurrencies above are quite unknown, and there is a considerable fluctuation between the 1 and 7 days percentage changes. As with stocks, and many other financial products, the smaller the capitalization, the bigger the risk and reward. Smaller cryptocurrencies are less stable projects in general, and therefore even riskier investments than the bigger ones1. Let's classify our dataset based on Investopedia's capitalization definitions for company stocks. 
1 Cryptocurrencies are a new asset class, so they are not directly comparable to stocks. Furthermore, there are no limits set in stone for what a ""small"" or ""large"" stock is. Finally, some investors argue that bitcoin is similar to gold, this would make them more comparable to a commodity instead.

## 9. Most coins are tiny
Note that many coins are not comparable to large companies in market cap, so let's divert from the original Investopedia definition by merging categories.
This is all for now. Thanks for completing this project!

"
https://github.com/mrbarkis/DataCamp_projects/tree/master/Exploring%20the%20Evolution%20of%20Linux,Exploring the Evolution of Linux,python,Markus Harrer,Software Development Analyst,https://www.datacamp.com/projects/111,"Find out about the evolution of the Linux operating system by exploring its version control system.
",https://www.github.com/mrbarkis/DataCamp_projects/blob/master/Exploring%20the%20Evolution%20of%20Linux/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/Exploring%20the%20Evolution%20of%20Linux/notebook.ipynb,['import pandas as pd'],"['.csv', '.csv', '.gz', '.timestamp', '.author', '.author', '.timestamp', '.timestamp', '.timestamp', '.columns', '.index', '.year', '.year']","['.read()', '.read_csv()', '.head()', '.count()', '.dropna()', '.unique()', '.value_counts()', '.to_datetime()', '.describe()', '.min()', '.max()', '.describe()', '.groupby()', '.Grouper()', '.count()', '.head()', '.plot()', '.set_xlabel()', '.set_ylabel()', '.idxmax()']","['open()', 'print()', 'len()', 'print()', 'print()', 'print()']","## 1. Introduction



Version control repositories like CVS, Subversion or Git can be a real gold mine for software developers. They contain every change to the source code including the date (the ""when""), the responsible developer (the ""who""), as well as a little message that describes the intention (the ""what"") of a change.
In this notebook, we will analyze the evolution of a very famous open-source project – the Linux kernel. The Linux kernel is the heart of some Linux distributions like Debian, Ubuntu or CentOS. Our dataset at hand contains the history of kernel development of almost 13 years (early 2005 - late 2017). We get some insights into the work of the development efforts by 

identifying the TOP 10 contributors and
visualizing the commits over the years.


## 2. Reading in the dataset
The dataset was created by using the command git log --encoding=latin-1 --pretty=""%at#%aN"" in late 2017. The latin-1 encoded text output was saved in a header-less CSV file. In this file, each row is a commit entry with the following information:

timestamp: the time of the commit as a UNIX timestamp in seconds since 1970-01-01 00:00:00 (Git log placeholder ""%at"")
author: the name of the author that performed the commit (Git log placeholder ""%aN"")

The columns are separated by the number sign #. The complete dataset is in the datasets/ directory. It is a gz-compressed csv file named git_log.gz.

## 3. Getting an overview
The dataset contains the information about every single code contribution (a ""commit"") to the Linux kernel over the last 13 years. We'll first take a look at the number of authors and their commits to the repository.

## 4. Finding the TOP 10 contributors
There are some very important people that changed the Linux kernel very often. To see if there are any bottlenecks, we take a look at the TOP 10 authors with the most commits.

## 5. Wrangling the data
For our analysis, we want to visualize the contributions over time. For this, we use the information in the timestamp column to create a time series-based column.

## 6. Treating wrong timestamps
As we can see from the results above, some contributors had their operating system's time incorrectly set when they committed to the repository. We'll clean up the timestamp column by dropping the rows with the incorrect timestamps.

## 7. Grouping commits per year
To find out how the development activity has increased over time, we'll group the commits by year and count them up.

## 8. Visualizing the history of Linux
Finally, we'll make a plot out of these counts to better see how the development effort on Linux has increased over the the last few years. 

## 9.  Conclusion
Thanks to the solid foundation and caretaking of Linux Torvalds, many other developers are now able to contribute to the Linux kernel as well. There is no decrease of development activity at sight!

"
https://github.com/mrbarkis/DataCamp_projects/tree/master/Exploring%20the%20History%20of%20Lego,Exploring the History of Lego,python,Ramnath Vaidyanathan,VP of Product Research at DataCamp,https://www.datacamp.com/projects/10,Use a variety of data manipulation techniques to explore different aspects of Lego's history!,https://www.github.com/mrbarkis/DataCamp_projects/blob/master/Exploring%20the%20History%20of%20Lego/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/Exploring%20the%20History%20of%20Lego/notebook.ipynb,['import pandas as pd'],"['.csv', '.rgb', '.rgb', '.csv', '.Series', '.count']","['.read_csv()', '.head()', '.unique()', '.duplicated()', '.sum()', '.groupby()', '.count()', '.read_csv()', '.groupby()', '.mean()', '.plot()', '.set_xlabel()', '.set_ylabel()', '.set_title()', '.groupby()', '.count()', '.groupby()', '.count()', '.groupby()', '.agg()', '.head()']","['print()', 'len()', 'print()', 'len()', 'print()', 'print()']","## 1. Introduction
Everyone loves Lego (unless you ever stepped on one). Did you know by the way that ""Lego"" was derived from the Danish phrase leg godt, which means ""play well""? Unless you speak Danish, probably not. 
In this project, we will analyze a fascinating dataset on every single lego block that has ever been built!


## 2. Reading Data
A comprehensive database of lego blocks is provided by Rebrickable. The data is available as csv files and the schema is shown below.

Let us start by reading in the colors data to get a sense of the diversity of lego sets!

## 3. Exploring Colors
Now that we have read the colors data, we can start exploring it! Let us start by understanding the number of colors available.

## 4. Transparent Colors in Lego Sets
The colors data has a column named is_trans that indicates whether a color is transparent or not. It would be interesting to explore the distribution of transparent vs. non-transparent colors.

## 5. Explore Lego Sets
Another interesting dataset available in this database is the sets data. It contains a comprehensive list of sets over the years and the number of parts that each of these sets contained. 

Let us use this data to explore how the average number of parts in Lego sets has varied over the years.

## 6. Lego Themes Over Years
Lego blocks ship under multiple themes. Let us try to get a sense of how the number of themes shipped has varied over the years.

## 7. Wrapping It All Up!
Lego blocks offer an unlimited amount of fun across ages. We explored some interesting trends around colors, parts, and themes. 

"
https://github.com/mrbarkis/DataCamp_projects/tree/master/Extract%20Stock%20Sentiment%20from%20News%20Headlines,Extract Stock Sentiment from News Headlines,python,Juan González-Vallinas,Director Data Science at multilayer.io,https://www.datacamp.com/projects/611,"Scrape news headlines for FB and TSLA then apply sentiment analysis to generate investment insight.
",https://www.github.com/mrbarkis/DataCamp_projects/blob/master/Extract%20Stock%20Sentiment%20from%20News%20Headlines/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/Extract%20Stock%20Sentiment%20from%20News%20Headlines/notebook.ipynb,"['import os', ""import nltk; #nltk.download('vader_lexicon')"", 'import pandas as pd', 'import matplotlib.pyplot as plt', 'from bs4 import BeautifulSoup', 'from nltk.sentiment.vader import SentimentIntensityAnalyzer']","['.title', '.html', '.a', '.text', '.text', '.text', '.text', '.text', '.a', '.text', '.td', '.text', '.sentiment', '.vader', '.lexicon', '.polarity_scores', '.date', '.dt', '.date', '.pyplot', '.style', '.plot', '.dt', '.time', '.columns', '.plot', '.2', '.5']","['.listdir()', '.prettify()', '.find()', '.find()', '.find()', '.prettify()', '.find()', '.find()', '.find()', '.get_text()', '.find_all()', '.find()', '.find()', '.items()', '.findAll()', '.get_text()', '.split()', '.split()', '.append()', '.download()', '.update()', '.DataFrame()', '.head()', '.apply()', '.to_list()', '.DataFrame()', '.join()', '.to_datetime()', '.head()', '.use()', '.groupby()', '.mean()', '.head()', '.unstack()', '.head()', '.xs()', '.head()', '.bar()', '.count()', '.drop_duplicates()', '.count()', '.set_index()', '.head()', '.xs()', '.head()', '.head()', '.to_datetime()', '.set_index()', '.sort_index()', '.head()', '.drop()', '.bar()', '.legend()', '.ylabel()']","['open()', 'BeautifulSoup()', 'print()', 'print()', 'print()', 'print()', 'print()', 'print()', 'print()', 'enumerate()', 'print()', 'print()', 'print()', 'len()', 'SentimentIntensityAnalyzer()']","## 1. Searching for gold inside HTML files
It used to take days for financial news to spread via radio, newspapers, and word of mouth. Now, in the age of the internet, it takes seconds. Did you know news articles are automatically being generated from figures and earnings call streams? Hedge funds and independent traders are using data science to process this wealth of information in the quest for profit.
In this notebook, we will generate investing insight by applying sentiment analysis on financial news headlines from FINVIZ.com. Using this natural language processing technique, we can understand the emotion behind the headlines and predict whether the market feels good or bad about a stock. It would then be possible to make educated guesses on how certain stocks will perform and trade accordingly. (And hopefully, make money!)

Why headlines? And why from FINVIZ?

Headlines, which have similar length, are easier to parse and group than full articles, which vary in length.
FINVIZ has a list of trusted websites, and headlines from these sites tend to be more consistent in their jargon than those from independent bloggers. Consistent textual patterns will improve the sentiment analysis.

As web scraping requires data science ethics (sending a lot of traffic to a FINVIZ's servers isn't very nice), the HTML files for Facebook and Tesla at various points in time have been downloaded. Let's import these files into memory.
Disclaimer: Investing in the stock market involves risk and can lead to monetary loss. The content in this notebook is not to be taken as financial advice. 

## 2. What is inside those files anyway?
We've grabbed the table that contains the headlines from each stock's HTML file, but before we start parsing those tables further, we need to understand how the data in that table is structured. We have a few options for this:

Open the HTML file with a text editor (preferably one with syntax highlighting, like Sublime Text) and explore it there
Use your browser's webdev toolkit to explore the HTML
Explore the headlines table here in this notebook!

Let's do the third option.

## 3. Extra, extra! Extract the news headlines
As we saw above, the interesting data inside each table row (<tr>) is in the text inside the <td> and <a> tags. Let's now actually parse the data for all tables in a comfortable data structure.

## 4. Make NLTK think like a financial journalist
Sentiment analysis is very sensitive to context. As an example, saying ""This is so addictive!"" often means something positive if the context is a video game you are enjoying with your friends, but it very often means something negative when we are talking about opioids. Remember that the reason we chose headlines is so we can try to extract sentiment from financial journalists, who like most professionals, have their own lingo. Let's now make NLTK think like a financial journalist by adding some new words and sentiment values to our lexicon.

## 5. BREAKING NEWS: NLTK Crushes Sentiment Estimates
Now that we have the data and the algorithm loaded, we will get to the core of the matter: programmatically predicting sentiment out of news headlines! Luckily for us, VADER is very high level so, in this case, we will not adjust the model further* other than the lexicon additions from before.
*VADER ""out-of-the-box"" with some extra lexicon would likely translate into heavy losses with real money. A real sentiment analysis tool with chances of being profitable will require a very extensive and dedicated to finance news lexicon. Furthermore, it might also not be enough using a pre-packaged model like VADER.

[The difference between using Vader and more low level NLTK](http://www.nltk.org/howto/sentiment.html)

## 6. Plot all the sentiment in subplots
Now that we have the scores, let's start plotting the results. We will start by plotting the time series for the stocks we have.

## 7. Weekends and duplicates
What happened to Tesla on November 22nd? Since we happen to have the headlines inside our DataFrame, a quick peek reveals that there are a few problems with that particular day: 

There are only 5 headlines for that day.
Two headlines are verbatim the same as another but from another news outlet.

Let's clean up the dataset a bit, but not too much! While some headlines are the same news piece from different sources, the fact that they are written differently could provide different perspectives on the same story. Plus, when one piece of news is more important, it tends to get more headlines from multiple sources. What we want to get rid of is verbatim copied headlines, as these are very likely coming from the same journalist and are just being ""forwarded"" around, so to speak.

## 8. Sentiment on one single trading day and stock
Just to understand the possibilities of this dataset and get a better feel of the data, let's focus on one trading day and one single stock. We will make an informative plot where we will see the smallest grain possible: headline and subscores.

## 9. Visualize the single day
We will make a plot to visualize the positive, negative and neutral scores for a single day of trading and a single stock. This is just one of the many ways to visualize this dataset.

"
https://github.com/mrbarkis/DataCamp_projects/tree/master/Find%20Movie%20Similarity%20from%20Plot%20Summaries,Find Movie Similarity from Plot Summaries,python,Anubhav Singh,Founder at The Code Foundation,https://www.datacamp.com/projects/648,"Use NLP and clustering on movie plot summaries from IMDb and Wikipedia to quantify movie similarity.
",https://www.github.com/mrbarkis/DataCamp_projects/blob/master/Find%20Movie%20Similarity%20from%20Plot%20Summaries/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/Find%20Movie%20Similarity%20from%20Plot%20Summaries/notebook.ipynb,"['import numpy as np', 'import pandas as pd', ""import nltk; nltk.download('punkt')"", 'import re', 'import matplotlib.pyplot as plt', 'from nltk.stem.snowball import SnowballStemmer', 'from sklearn.feature_extraction.text import TfidfVectorizer', 'from sklearn.cluster import KMeans', 'from sklearn.metrics.pairwise import cosine_similarity', 'from scipy.cluster.hierarchy import linkage, dendrogram']","['.random', '.csv', '.stem', '.snowball', '.feature_extraction', '.text', '.8', '.2', '.shape', '.cluster', '.labels_', '.plot', '.metrics', '.pairwise', '.pyplot', '.pyplot', '.cluster', '.hierarchy', '.shape']","['.download()', '.seed()', '.read_csv()', '.astype()', '.astype()', '.head()', '.sent_tokenize()', '.word_tokenize()', '.search()', '.stem()', '.sent_tokenize()', '.word_tokenize()', '.search()', '.stem()', '.fit_transform()', '.fit()', '.tolist()', '.value_counts()', '.hist()', '.gcf()', '.set_color()', '.gca()', '.get_xmajorticklabels()', '.set_size_inches()', '.show()']","['print()', 'len()', 'Not()', 'SnowballStemmer()', 'print()', 'print()', 'tokenize_and_stem()', 'SnowballStemmer()', 'tokenize_and_stem()', 'print()', 'TfidfVectorizer()', 'print()', 'KMeans()', 'cosine_similarity()', 'linkage()', 'print()', 'dendrogram()', 'print()']","## 1. Import and observe dataset
We all love watching movies! There are some movies we like, some we don't. Most people have a preference for movies of a similar genre. Some of us love watching action movies, while some of us like watching horror. Some of us like watching movies that have ninjas in them, while some of us like watching superheroes.
Movies within a genre often share common base parameters. Consider the following two movies:


Both movies, 2001: A Space Odyssey and Close Encounters of the Third Kind, are movies based on aliens coming to Earth. I've seen both, and they indeed share many similarities. We could conclude that both of these fall into the same genre of movies based on intuition, but that's no fun in a data science context. In this notebook, we will quantify the similarity of movies based on their plot summaries available on IMDb and Wikipedia, then separate them into groups, also known as clusters. We'll create a dendrogram to represent how closely the movies are related to each other.
Let's start by importing the dataset and observing the data provided.

## 2. Combine Wikipedia and IMDb plot summaries
The dataset we imported currently contains two columns titled wiki_plot and imdb_plot. They are the plot found for the movies on Wikipedia and IMDb, respectively. The text in the two columns is similar, however, they are often written in different tones and thus provide context on a movie in a different manner of linguistic expression. Further, sometimes the text in one column may mention a feature of the plot that is not present in the other column. For example, consider the following plot extracts from The Godfather:

Wikipedia: ""On the day of his only daughter's wedding, Vito Corleone""
IMDb: ""In late summer 1945, guests are gathered for the wedding reception of Don Vito Corleone's daughter Connie""

While the Wikipedia plot only mentions it is the day of the daughter's wedding, the IMDb plot also mentions the year of the scene and the name of the daughter. 
Let's combine both the columns to avoid the overheads in computation associated with extra columns to process.

## 3. Tokenization
Tokenization is the process  by which we break down articles into individual sentences or words, as needed. Besides the tokenization method provided by NLTK, we might have to perform additional filtration to remove tokens which are entirely numeric values or punctuation.
While a program may fail to build context from ""While waiting at a bus stop in 1981"" (Forrest Gump), because this string would not match in any dictionary, it is possible to build context from the words ""while"", ""waiting"" or ""bus"" because they are present in the English dictionary. 
Let us perform tokenization on a small extract from The Godfather.

## 4. Stemming
Stemming is the process by which we bring down a word from its different forms to the root word. This helps us establish meaning to different forms of the same words without having to deal with each form separately. For example, the words 'fishing', 'fished', and 'fisher' all get stemmed to the word 'fish'.
Consider the following sentences:

""Young William Wallace witnesses the treachery of Longshanks"" ~ Gladiator
""escapes to the city walls only to witness Cicero's death"" ~ Braveheart

Instead of building separate dictionary entries for both witnesses and witness, which mean the same thing outside of quantity, stemming them reduces them to 'wit'.
There are different algorithms available for stemming such as the Porter Stemmer, Snowball Stemmer, etc. We shall use the Snowball Stemmer.

## 5. Club together Tokenize & Stem
We are now able to tokenize and stem sentences. But we may have to use the two functions repeatedly one after the other to handle a large amount of data, hence we can think of wrapping them in a function and passing the text to be tokenized and stemmed as the function argument. Then we can pass the new wrapping function, which shall perform both tokenizing and stemming instead of just tokenizing, as the tokenizer argument while creating the TF-IDF vector of the text.  
What difference does it make though? Consider the sentence from the plot of The Godfather: ""Today (May 19, 2016) is his only daughter's wedding."" If we do a 'tokenize-only' for this sentence, we have the following result:

'today', 'may', 'is', 'his', 'only', 'daughter', ""'s"", 'wedding'

But when we do a 'tokenize-and-stem' operation we get:

'today', 'may', 'is', 'his', 'onli', 'daughter', ""'s"", 'wed'

All the words are in their root form, which will lead to a better establishment of meaning as some of the non-root forms may not be present in the NLTK training corpus.

## 6. Create TfidfVectorizer
Computers do not understand text. These are machines only capable of understanding numbers and performing numerical computation. Hence, we must convert our textual plot summaries to numbers for the computer to be able to extract meaning from them. One simple method of doing this would be to count all the occurrences of each word in the entire vocabulary and return the counts in a vector. Enter CountVectorizer.
Consider the word 'the'. It appears quite frequently in almost all movie plots and will have a high count in each case. But obviously, it isn't the theme of all the movies! Term Frequency-Inverse Document Frequency (TF-IDF) is one method which overcomes the shortcomings of CountVectorizer. The Term Frequency of a word is the measure of how often it appears in a document, while the Inverse Document Frequency is the parameter which reduces the importance of a word if it frequently appears in several documents.
For example, when we apply the TF-IDF on the first 3 sentences from the plot of The Wizard of Oz, we are told that the most important word there is 'Toto', the pet dog of the lead character. This is because the movie begins with 'Toto' biting someone due to which the journey of Oz begins!
In simplest terms, TF-IDF recognizes words which are unique and important to any given document. Let's create one for our purposes.

## 7. Fit transform TfidfVectorizer
Once we create a TF-IDF Vectorizer, we must fit the text to it and then transform the text to produce the corresponding numeric form of the data which the computer will be able to understand and derive meaning from. To do this, we use the fit_transform() method of the TfidfVectorizer object. 
If we observe the TfidfVectorizer object we created, we come across a parameter stopwords. 'stopwords' are those words in a given text which do not contribute considerably towards the meaning of the sentence and are generally grammatical filler words. For example, in the sentence 'Dorothy Gale lives with her dog Toto on the farm of her Aunt Em and Uncle Henry', we could drop the words 'her' and 'the', and still have a similar overall meaning to the sentence. Thus, 'her' and 'the' are stopwords and can be conveniently dropped from the sentence. 
On setting the stopwords to 'english', we direct the vectorizer to drop all stopwords from a pre-defined list of English language stopwords present in the nltk module. Another parameter, ngram_range, defines the length of the ngrams to be formed while vectorizing the text.

## 8. Import KMeans and create clusters
To determine how closely one movie is related to the other by the help of unsupervised learning, we can use clustering techniques. Clustering is the method of grouping together a number of items such that they exhibit similar properties. According to the measure of similarity desired, a given sample of items can have one or more clusters. 
A good basis of clustering in our dataset could be the genre of the movies. Say we could have a cluster '0' which holds movies of the 'Drama' genre. We would expect movies like Chinatown or Psycho to belong to this cluster. Similarly, the cluster '1' in this project holds movies which belong to the 'Adventure' genre (Lawrence of Arabia and the Raiders of the Lost Ark, for example).
K-means is an algorithm which helps us to implement clustering in Python. The name derives from its method of implementation: the given sample is divided into K clusters where each cluster is denoted by the mean of all the items lying in that cluster. 
We get the following distribution for the clusters:


## 9. Calculate similarity distance
Consider the following two sentences from the movie The Wizard of Oz: 

""they find in the Emerald City""
""they finally reach the Emerald City""

If we put the above sentences in a CountVectorizer, the vocabulary produced would be ""they, find, in, the, Emerald, City, finally, reach"" and the vectors for each sentence would be as follows: 

1, 1, 1, 1, 1, 1, 0, 0
1, 0, 0, 1, 1, 1, 1, 1

When we calculate the cosine angle formed between the vectors represented by the above, we get a score of 0.667. This means the above sentences are very closely related. Similarity distance is 1 - cosine similarity angle. This follows from that if the vectors are similar, the cosine of their angle would be 1 and hence, the distance between then would be 1 - 1 = 0.
Let's calculate the similarity distance for all of our movies.

## 10. Import Matplotlib, Linkage, and Dendrograms
We shall now create a tree-like diagram (called a dendrogram) of the movie titles to help us understand the level of similarity between them visually. Dendrograms help visualize the results of hierarchical clustering, which is an alternative to k-means clustering. Two pairs of movies at the same level of hierarchical clustering are expected to have similar strength of similarity between the corresponding pairs of movies. For example, the movie Fargo would be as similar to North By Northwest as the movie Platoon is to Saving Private Ryan, given both the pairs exhibit the same level of the hierarchy.
Let's import the modules we'll need to create our dendrogram.

## 11. Create merging and plot dendrogram
We shall plot a dendrogram of the movies whose similarity measure will be given by the similarity distance we previously calculated. The lower the similarity distance between any two movies, the lower their linkage will make an intercept on the y-axis. For instance, the lowest dendrogram linkage we shall discover will be between the movies, It's a Wonderful Life and A Place in the Sun. This indicates that the movies are very similar to each other in their plots.

## 12. Which movies are most similar?
We can now determine the similarity between movies based on their plots! To wrap up, let's answer one final question: which movie is most similar to the movie Braveheart?

"
https://github.com/mrbarkis/DataCamp_projects/tree/master/Generating%20Keywords%20for%20Google%20Ads,Generating Keywords for Google Ads,python,Elias Dabbas,Owner at The Media Supermarket,https://www.datacamp.com/projects/400,"Automatically generate keywords for a search engine marketing campaign using Python. 
",https://www.github.com/mrbarkis/DataCamp_projects/blob/master/Generating%20Keywords%20for%20Google%20Ads/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/Generating%20Keywords%20for%20Google%20Ads/notebook.ipynb,"['import pandas as pd', 'from pprint import pprint']",['.csv'],"['.append()', '.append()', '.DataFrame()', '.from_records()', '.head()', '.rename()', '.copy()', '.append()', '.to_csv()', '.groupby()', '.count()']","['pprint()', 'print()']","## 1. The brief
Imagine working for a digital marketing agency, and the agency is approached by a massive online retailer of furniture. They want to test our skills at creating large campaigns for all of their website. We are tasked with creating a prototype set of keywords for search campaigns for their sofas section. The client says that they want us to generate keywords for the following products: 

sofas
convertible sofas
love seats
recliners
sofa beds

The brief: The client is generally a low-cost retailer, offering many promotions and discounts. We will need to focus on such keywords. We will also need to move away from luxury keywords and topics, as we are targeting price-sensitive customers. Because we are going to be tight on budget, it would be good to focus on a tightly targeted set of keywords and make sure they are all set to exact and phrase match.
Based on the brief above we will first need to generate a list of words, that together with the products given above would make for good keywords. Here are some examples:

Products: sofas, recliners
Words: buy, prices

The resulting keywords: 'buy sofas', 'sofas buy', 'buy recliners', 'recliners buy',
          'prices sofas', 'sofas prices', 'prices recliners', 'recliners prices'.
As a final result, we want to have a DataFrame that looks like this: 



Campaign
Ad Group
Keyword
Criterion Type




Campaign1
AdGroup_1
keyword 1a
Exact


Campaign1
AdGroup_1
keyword 1a
Phrase


Campaign1
AdGroup_1
keyword 1b
Exact


Campaign1
AdGroup_1
keyword 1b
Phrase


Campaign1
AdGroup_2
keyword 2a
Exact


Campaign1
AdGroup_2
keyword 2a
Phrase



The first step is to come up with a list of words that users might use to express their desire in buying low-cost sofas.

## 2. Combine the words with the product names
Imagining all the possible combinations of keywords can be stressful! But not for us, because we are keyword ninjas! We know how to translate campaign briefs into Python data structures and can imagine the resulting DataFrames that we need to create.
Now that we have brainstormed the words that work well with the brief that we received, it is now time to combine them with the product names to generate meaningful search keywords. We want to combine every word with every product once before, and once after, as seen in the example above.
As a quick reminder, for the product 'recliners' and the words 'buy' and 'price' for example, we would want to generate the following combinations: 
buy recliners
recliners buy
price recliners
recliners price
...  
and so on for all the words and products that we have.

## 3. Convert the list of lists into a DataFrame
Now we want to convert this list of lists into a DataFrame so we can easily manipulate it and manage the final output.

## 4. Rename the columns of the DataFrame
Before we can upload this table of keywords, we will need to give the columns meaningful names. If we inspect the DataFrame we just created above, we can see that the columns are currently named 0 and 1. Ad Group (example: ""sofas"") and Keyword (example: ""sofas buy"") are much more appropriate names.

## 5. Add a campaign column
Now we need to add some additional information to our DataFrame. 
We need a new column called Campaign for the campaign name. We want campaign names to be descriptive of our group of keywords and products, so let's call this campaign 'SEM_Sofas'.

## 6. Create the match type column
There are different keyword match types. One is exact match, which is for matching the exact term or are close variations of that exact term. Another match type is broad match, which means ads may show on searches that include misspellings, synonyms, related searches, and other relevant variations.
Straight from Google's AdWords documentation:

In general, the broader the match type, the more traffic potential that keyword will have, since your ads may be triggered more often. Conversely, a narrower match type means that your ads may show less often—but when they do, they’re likely to be more related to someone’s search.

Since the client is tight on budget, we want to make sure all the keywords are in exact match at the beginning.

## 7. Duplicate all the keywords into 'phrase' match
The great thing about exact match is that it is very specific, and we can control the process very well. The tradeoff, however, is that:  

The search volume for exact match is lower than other match types
We can't possibly think of all the ways in which people search, and so, we are probably missing out on some high-quality keywords.

So it's good to use another match called phrase match as a discovery mechanism to allow our ads to be triggered by keywords that include our exact match keywords, together with anything before (or after) them.
Later on, when we launch the campaign, we can explore with modified broad match, broad match, and negative match types, for better visibility and control of our campaigns.

## 8. Save and summarize!
To upload our campaign, we need to save it as a CSV file. Then we will be able to import it to AdWords editor or BingAds editor. There is also the option of pasting the data into the editor if we want, but having easy access to the saved data is great so let's save to a CSV file!
Looking at a summary of our campaign structure is good now that we've wrapped up our keyword work. We can do that by grouping by ad group and criterion type and counting by keyword. This summary shows us that we assigned specific keywords to specific ad groups, which are each part of a campaign. In essence, we are telling Google (or Bing, etc.) that we want any of the words in each ad group to trigger one of the ads in the same ad group. Separately, we will have to create another table for ads, which is a task for another day and would look something like this:



Campaign
Ad Group
Headline 1
Headline 2
Description
Final URL




SEM_Sofas
Sofas
Looking for Quality Sofas?
Explore Our Massive Collection
30-day Returns With Free Delivery Within the US. Start Shopping Now
DataCampSofas.com/sofas


SEM_Sofas
Sofas
Looking for Affordable Sofas?
Check Out Our Weekly Offers
30-day Returns With Free Delivery Within the US. Start Shopping Now
DataCampSofas.com/sofas


SEM_Sofas
Recliners
Looking for Quality Recliners?
Explore Our Massive Collection
30-day Returns With Free Delivery Within the US. Start Shopping Now
DataCampSofas.com/recliners


SEM_Sofas
Recliners
Need Affordable Recliners?
Check Out Our Weekly Offers
30-day Returns With Free Delivery Within the US. Start Shopping Now
DataCampSofas.com/recliners



Together, these tables get us the sample keywords -> ads -> landing pages mapping shown in the diagram below.


"
https://github.com/mrbarkis/DataCamp_projects/tree/master/Introduction%20to%20DataCamp%20Projects,Introduction to DataCamp Projects,python,Rasmus Bååth,Senior Data Scientist at King (Activision Blizzard),https://www.datacamp.com/projects/33,"If you've never done a DataCamp project, this is the place to start! 
",https://www.github.com/mrbarkis/DataCamp_projects/blob/master/Introduction%20to%20DataCamp%20Projects/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/Introduction%20to%20DataCamp%20Projects/notebook.ipynb,"['import pandas as pd', 'import matplotlib.pyplot as plt', 'import folium']","['.csv', '.pyplot', '.4970', '.0266', '.5', '.3318', '.0311', '.4', '.5431', '.0579', '.9']","['.read_csv()', '.plot()', '.xlabel()', '.ylabel()', '.title()', '.show()', '.Map()', '.Marker()', '.add_to()']","['greet()', 'greet()', 'print()']","## 1. This is a Jupyter notebook!
A Jupyter notebook is a document that contains text cells (what you're reading right now) and code cells. What is special with a notebook is that it's interactive: You can change or add code cells, and then run a cell by first selecting it and then clicking the run cell button above ( ▶| Run ) or hitting ctrl + enter. 

The result will be displayed directly in the notebook. You could use a notebook as a simple calculator. For example, it's estimated that on average 256 children were born every minute in 2016. The code cell below calculates how many children were born on average on a day. 

## 2. Put any code in code cells
But a code cell can contain much more than a simple one-liner! This is a notebook running python and you can put any python code in a code cell (but notebooks can run other languages too, like R). Below is a code cell where we define a whole new function (greet). To show the output of greet we run it last in the code cell as the last value is always printed out. 

## 3. Jupyter notebooks ♡ data
We've seen that notebooks can display basic objects such as numbers and strings. But notebooks also support the objects used in data science, which makes them great for interactive data analysis!
For example, below we create a pandas DataFrame by reading in a csv-file with the average global temperature for the years 1850 to 2016. If we look at the head of this DataFrame the notebook will render it as a nice-looking table.

## 4. Jupyter notebooks ♡ plots
Tables are nice but — as the saying goes — ""a plot can show a thousand data points"". Notebooks handle plots as well, but it requires a bit of magic. Here magic does not refer to any arcane rituals but to so-called ""magic commands"" that affect how the Jupyter notebook works. Magic commands start with either % or %% and the command we need to nicely display plots inline is %matplotlib inline. With this magic in place, all plots created in code cells will automatically be displayed inline. 
Let's take a look at the global temperature for the last 150 years.

## 5. Jupyter notebooks ♡ a lot more
Tables and plots are the most common outputs when doing data analysis, but Jupyter notebooks can render many more types of outputs such as sound, animation, video, etc. Yes, almost anything that can be shown in a modern web browser. This also makes it possible to include interactive widgets directly in the notebook!
For example, this (slightly complicated) code will create an interactive map showing the locations of the three largest smartphone companies in 2016. You can move and zoom the map, and you can click the markers for more info! 

## 6. Goodbye for now!
This was just a short introduction to Jupyter notebooks, an open source technology that is increasingly used for data science and analysis. I hope you enjoyed it! :)

"
https://github.com/mrbarkis/DataCamp_projects/tree/master/Introduction%20to%20DataCamp%20Projects,Introduction to DataCamp Projects,python,David Venturi,Curriculum Manager at DataCamp,https://www.datacamp.com/projects/571,"If you've never done a DataCamp project, this is the place to start!
",https://www.github.com/mrbarkis/DataCamp_projects/blob/master/Introduction%20to%20DataCamp%20Projects/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/Introduction%20to%20DataCamp%20Projects/notebook.ipynb,"['import pandas as pd', 'import matplotlib.pyplot as plt', 'import folium']","['.csv', '.pyplot', '.4970', '.0266', '.5', '.3318', '.0311', '.4', '.5431', '.0579', '.9']","['.read_csv()', '.plot()', '.xlabel()', '.ylabel()', '.title()', '.show()', '.Map()', '.Marker()', '.add_to()']","['greet()', 'greet()', 'print()']","## 1. This is a Jupyter notebook!
A Jupyter notebook is a document that contains text cells (what you're reading right now) and code cells. What is special with a notebook is that it's interactive: You can change or add code cells, and then run a cell by first selecting it and then clicking the run cell button above ( ▶| Run ) or hitting ctrl + enter. 

The result will be displayed directly in the notebook. You could use a notebook as a simple calculator. For example, it's estimated that on average 256 children were born every minute in 2016. The code cell below calculates how many children were born on average on a day. 

## 2. Put any code in code cells
But a code cell can contain much more than a simple one-liner! This is a notebook running python and you can put any python code in a code cell (but notebooks can run other languages too, like R). Below is a code cell where we define a whole new function (greet). To show the output of greet we run it last in the code cell as the last value is always printed out. 

## 3. Jupyter notebooks ♡ data
We've seen that notebooks can display basic objects such as numbers and strings. But notebooks also support the objects used in data science, which makes them great for interactive data analysis!
For example, below we create a pandas DataFrame by reading in a csv-file with the average global temperature for the years 1850 to 2016. If we look at the head of this DataFrame the notebook will render it as a nice-looking table.

## 4. Jupyter notebooks ♡ plots
Tables are nice but — as the saying goes — ""a plot can show a thousand data points"". Notebooks handle plots as well, but it requires a bit of magic. Here magic does not refer to any arcane rituals but to so-called ""magic commands"" that affect how the Jupyter notebook works. Magic commands start with either % or %% and the command we need to nicely display plots inline is %matplotlib inline. With this magic in place, all plots created in code cells will automatically be displayed inline. 
Let's take a look at the global temperature for the last 150 years.

## 5. Jupyter notebooks ♡ a lot more
Tables and plots are the most common outputs when doing data analysis, but Jupyter notebooks can render many more types of outputs such as sound, animation, video, etc. Yes, almost anything that can be shown in a modern web browser. This also makes it possible to include interactive widgets directly in the notebook!
For example, this (slightly complicated) code will create an interactive map showing the locations of the three largest smartphone companies in 2016. You can move and zoom the map, and you can click the markers for more info! 

## 6. Goodbye for now!
This was just a short introduction to Jupyter notebooks, an open source technology that is increasingly used for data science and analysis. I hope you enjoyed it! :)

"
https://github.com/mrbarkis/DataCamp_projects/tree/master/Na%C3%AFve%20Bees:%20Image%20Loading%20and%20Processing,Naïve Bees: Image Loading and Processing,python,Peter Bull,Co-founder of DrivenData,https://www.datacamp.com/projects/374,"Load, transform, and understand images of honey bees and bumble bees in Python.
",https://www.github.com/mrbarkis/DataCamp_projects/blob/master/Na%C3%AFve%20Bees:%20Image%20Loading%20and%20Processing/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/Na%C3%AFve%20Bees:%20Image%20Loading%20and%20Processing/notebook.ipynb,"['import matplotlib.pyplot as plt', 'import pandas as pd', 'import numpy as np', 'from pathlib import Path', 'from IPython.display import display', 'from PIL import Image']","['.pyplot', '.display', '.random', '.jpg', '.size', '.FLIP_LEFT_RIGHT', '.shape', '.cm', '.Reds_r', '.cm', '.Greens_r', '.cm', '.Blues_r', '.g', '.plot', '.jpg', '.jpg', '.jpg', '.jpg', '.shape', '.cm', '.gray', '.FLIP_LEFT_RIGHT', '.jpg', '.cm', '.gray', '.jpg', '.jpg', '.jpg', '.jpg', '.jpg', '.jpg', '.stem', '.jpg', '.stem']","['.beta()', '.imshow()', '.open()', '.format()', '.crop()', '.rotate()', '.transpose()', '.array()', '.format()', '.imshow()', '.show()', '.imshow()', '.show()', '.imshow()', '.show()', '.imshow()', '.show()', '.flatten()', '.Series()', '.density()', '.show()', '.open()', '.array()', '.open()', '.array()', '.convert()', '.array()', '.format()', '.imshow()', '.show()', '.transpose()', '.save()', '.maximum()', '.imshow()', '.fromarray()', '.save()', '.open()', '.format()', '.format()', '.format()', '.convert()', '.save()', '.format()', '.rotate()', '.crop()', '.resize()', '.save()']","['print()', 'display()', 'display()', 'display()', 'print()', 'plot_kde()', 'plot_rgb()', 'enumerate()', 'plot_kde()', 'plot_rgb()', 'display()', 'display()', 'plot_rgb()', 'display()', 'plot_rgb()', 'display()', 'print()', 'plot_kde()', 'display()', 'process_image()', 'print()', 'print()', 'process_image()', 'Path()']","## 1. Import Python libraries

A honey bee.
The question at hand is: can a machine identify a bee as a honey bee or a bumble bee? These bees have different behaviors and appearances, but given the variety of backgrounds, positions, and image resolutions it can be a challenge for machines to tell them apart.
Being able to identify bee species from images is a task that ultimately would allow researchers to more quickly and effectively collect field data. Pollinating bees have critical roles in both ecology and agriculture, and diseases like colony collapse disorder threaten these species. Identifying different species of bees in the wild means that we can better understand the prevalence and growth of these important insects.

A bumble bee.
This notebook walks through loading and processing images. After loading and processing these images, they will be ready for building models that can automatically detect honeybees and bumblebees.

## 2. Opening images with PIL
Now that we have all of our imports ready, it is time to work with some real images.
Pillow is a very flexible image loading and manipulation library. It works with many different image formats, for example, .png, .jpg, .gif and more. For most image data, one can work with images using the Pillow library (which is imported as PIL).
Now we want to load an image, display it in the notebook, and print out the dimensions of the image. By dimensions, we mean the width of the image and the height of the image. These are measured in pixels. The documentation for Image in Pillow gives a comprehensive view of what this object can do.

## 3. Image manipulation with PIL
Pillow has a number of common image manipulation tasks built into the library. For example, one may want to resize an image so that the file size is smaller. Or, perhaps, convert an image to black-and-white instead of color. Operations that Pillow provides include:

resizing
cropping
rotating
flipping
converting to greyscale (or other color modes)

Often, these kinds of manipulations are part of the pipeline for turning a small number of images into more images to create training data for machine learning algorithms. This technique is called data augmentation, and it is a common technique for image classification.
We'll try a couple of these operations and look at the results.

## 4. Images as arrays of data
What is an image? So far, PIL has handled loading images and displaying them. However, if we're going to use images as data, we need to understand what that data looks like.
Most image formats have three color ""channels"": red, green, and blue (some images also have a fourth channel called ""alpha"" that controls transparency). For each pixel in an image, there is a value for every channel.

The way this is represented as data is as a three-dimensional matrix. The width of the matrix is the width of the image, the height of the matrix is the height of the image, and the depth of the matrix is the number of channels. So, as we saw, the height and width of our image are both 100 pixels. This means that the underlying data is a matrix with the dimensions 100x100x3.

## 5. Explore the color channels
Color channels can help provide more information about an image. A picture of the ocean will be more blue, whereas a picture of a field will be more green. This kind of information can be useful when building models or examining the differences between images.
We'll look at the kernel density estimate for each of the color channels on the same plot so that we can understand how they differ.
When we make this plot, we'll see that a shape that appears further to the right means more of that color, whereas further to the left means less of that color.

## 6. Honey bees and bumble bees (i)
Now we'll look at two different images and some of the differences between them. The first image is of a honey bee, and the second image is of a bumble bee.
First, let's look at the honey bee.

## 7. Honey bees and bumble bees (ii)
Now let's look at the bumble bee.
When one compares these images, it is clear how different the colors are. The honey bee image above, with a blue flower, has a strong peak on the right-hand side of the blue channel. The bumble bee image, which has a lot of yellow for the bee and the background, has almost perfect overlap between the red and green channels (which together make yellow).

## 8. Simplify, simplify, simplify
While sometimes color information is useful, other times it can be distracting. In this examples where we are looking at bees, the bees themselves are very similar colors. On the other hand, the bees are often on top of different color flowers. We know that the colors of the flowers may be distracting from separating honey bees from bumble bees, so let's convert these images to black-and-white, or ""grayscale.""
Grayscale is just one of the modes that Pillow supports. Switching between modes is done with the .convert() method, which is passed a string for the new mode.
Because we change the number of color ""channels,"" the shape of our array changes with this change. It also will be interesting to look at how the KDE of the grayscale version compares to the RGB version above.

## 9. Save your work!
We've been talking this whole time about making changes to images and the manipulations that might be useful as part of a machine learning pipeline. To use these images in the future, we'll have to save our work after we've made changes.
Now, we'll make a couple changes to the Image object from Pillow and save that. We'll flip the image left-to-right, just as we did with the color version. Then, we'll change the NumPy version of the data by clipping it. Using the np.maximum function, we can take any number in the array smaller than 100 and replace it with 100. Because this reduces the range of values, it will increase the contrast of the image. We'll then convert that back to an Image and save the result.

## 10. Make a pipeline
Now it's time to create an image processing pipeline. We have all the tools in our toolbox to load images, transform them, and save the results.
In this pipeline we will do the following:

Load the image with Image.open and create paths to save our images to
Convert the image to grayscale
Save the grayscale image
Rotate, crop, and zoom in on the image and save the new image


"
https://github.com/mrbarkis/DataCamp_projects/tree/master/Na%C3%AFve%20Bees:%20Predict%20Species%20from%20Images,Naïve Bees: Predict Species from Images,python,Peter Bull,Co-founder of DrivenData,https://www.datacamp.com/projects/412,"Build a model that can automatically detect honey bees and bumble bees in images.
",https://www.github.com/mrbarkis/DataCamp_projects/blob/master/Na%C3%AFve%20Bees:%20Predict%20Species%20from%20Images/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/Na%C3%AFve%20Bees:%20Predict%20Species%20from%20Images/notebook.ipynb,"['import os', 'import matplotlib as mpl', 'import matplotlib.pyplot as plt', 'import pandas as pd', 'import numpy as np', 'from IPython.display import display', 'from PIL import Image', 'from skimage.feature import hog', 'from skimage.color import rgb2gray', 'from sklearn.preprocessing import StandardScaler', 'from sklearn.decomposition import PCA', 'from sklearn.model_selection import train_test_split', 'from sklearn.svm import SVC', 'from sklearn.metrics import roc_curve, auc, accuracy_score']","['.pyplot', '.display', '.feature', '.color', '.preprocessing', '.decomposition', '.model_selection', '.svm', '.metrics', '.csv', '.jpg', '.path', '.0', '.genus', '.0', '.index', '.0', '.genus', '.index', '.shape', '.cm', '.gray', '.shape', '.cm', '.gray', '.shape', '.index', '.shape', '.shape', '.genus', '.values', '.3', '.0', '.2f']","['.read_csv()', '.head()', '.format()', '.join()', '.open()', '.array()', '.imshow()', '.show()', '.imshow()', '.show()', '.imshow()', '.imshow()', '.flatten()', '.hstack()', '.append()', '.array()', '.fit_transform()', '.fit_transform()', '.Series()', '.value_counts()', '.fit()', '.predict()', '.predict_proba()', '.title()', '.plot()', '.format()', '.legend()', '.plot()', '.ylabel()', '.xlabel()']","['display()', 'get_image()', 'get_image()', 'get_image()', 'get_image()', 'print()', 'rgb2gray()', 'print()', 'hog()', 'create_features()', 'rgb2gray()', 'hog()', 'create_features()', 'create_feature_matrix()', 'get_image()', 'create_features()', 'create_feature_matrix()', 'print()', 'StandardScaler()', 'PCA()', 'print()', 'train_test_split()', 'SVC()', 'accuracy_score()', 'print()', 'roc_curve()', 'auc()']","## 1. Import Python libraries

A honey bee (Apis).
Can a machine identify a bee as a honey bee or a bumble bee? These bees have different behaviors and appearances, but given the variety of backgrounds, positions, and image resolutions, it can be a challenge for machines to tell them apart.
Being able to identify bee species from images is a task that ultimately would allow researchers to more quickly and effectively collect field data. Pollinating bees have critical roles in both ecology and agriculture, and diseases like colony collapse disorder threaten these species. Identifying different species of bees in the wild means that we can better understand the prevalence and growth of these important insects.

A bumble bee (Bombus).
After loading and pre-processing images, this notebook walks through building a model that can automatically detect honey bees and bumble bees.

## 2. Display image of each bee type
Now that we have all of our imports ready, it is time to look at some images. We will load our labels.csv file into a dataframe called labels, where the index is the image name (e.g. an index of 1036 refers to an image named 1036.jpg) and the genus column tells us the bee type. genus takes the value of either 0.0 (Apis or honey bee) or 1.0 (Bombus or bumble bee).
The function get_image converts an index value from the dataframe into a file path where the image is located, opens the image using the Image object in Pillow, and then returns the image as a numpy array.
We'll use this function to load the sixth Apis image and then the sixth Bombus image in the dataframe.

## 3. Image manipulation with rgb2grey
scikit-image has a number of image processing functions built into the library, for example, converting an image to greyscale. The rgb2grey function computes the luminance of an RGB image using the following formula Y = 0.2125 R + 0.7154 G + 0.0721 B. 
Image data is represented as a matrix, where the depth is the number of channels. An RGB image has three channels (red, green, and blue) whereas the returned greyscale image has only one channel. Accordingly, the original color image has the dimensions 100x100x3 but after calling rgb2grey, the resulting greyscale image has only one channel, making the dimensions 100x100x1.

## 4. Histogram of oriented gradients
Now we need to turn these images into something that a machine learning algorithm can understand. Traditional computer vision techniques have relied on mathematical transforms to turn images into useful features. For example, you may want to detect edges of objects in an image, increase the contrast, or filter out particular colors.
We've got a matrix of pixel values, but those don't contain enough interesting information on their own for most algorithms. We need to help the algorithms along by picking out some of the salient features for them using the histogram of oriented gradients (HOG) descriptor. The idea behind HOG is that an object's shape within an image can be inferred by its edges, and a way to identify edges is by looking at the direction of intensity gradients (i.e. changes in luminescence). 

An image is divided in a grid fashion into cells, and for the pixels within each cell, a histogram of gradient directions is compiled. To improve invariance to highlights and shadows in an image, cells are block normalized, meaning an intensity value is calculated for a larger region of an image called a block and used to contrast normalize all cell-level histograms within each block. The HOG feature vector for the image is the concatenation of these cell-level histograms.

## 5. Create image features and flatten into a single row
Algorithms require data to be in a format where rows correspond to images and columns correspond to features. This means that all the information for a given image needs to be contained in a single row.
We want to provide our model with the raw pixel values from our images as well as the HOG features we just calculated. To do this, we will write a function called create_features that combines these two sets of features by flattening the three-dimensional array into a one-dimensional (flat) array.

## 6. Loop over images to preprocess
Above we generated a flattened features array for the bombus image. Now it's time to loop over all of our images. We will create features for each image and then stack the flattened features arrays into a big matrix we can pass into our model.
In the create_feature_matrix function, we'll do the following:

Load an image
Generate a row of features using the create_features function above
Stack the rows into a features matrix

In the resulting features matrix, rows correspond to images and columns to features.

## 7. Scale feature matrix + PCA
Our features aren't quite done yet. Many machine learning methods are built to work best with data that has a mean of 0 and unit variance. Luckily, scikit-learn provides a simple way to rescale your data to work well using StandardScaler. They've got a more thorough explanation of why that is in the linked docs.
Remember also that we have over 31,000 features for each image and only 500 images total. To use an SVM, our model of choice, we also need to reduce the number of features we have using principal component analysis (PCA). 
PCA is a way of linearly transforming the data such that most of the information in the data is contained within a smaller number of features called components. Below is a visual example from an image dataset containing handwritten numbers. The image on the left is the original image with 784 components. We can see that the image on the right (post PCA) captures the shape of the number quite effectively even with only 59 components.

In our case, we will keep 500 components. This means our feature matrix will only have 500 columns rather than the original 31,296.

## 8. Split into train and test sets
Now we need to convert our data into train and test sets. We'll use 70% of images as our training data and test our model on the remaining 30%. Scikit-learn's train_test_split function makes this easy.

## 9. Train model
It's finally time to build our model! We'll use a support vector machine (SVM), a type of supervised machine learning model used for regression, classification, and outlier detection."" An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.""
Here's a visualization of the maximum margin separating two classes using an SVM classifier with a linear kernel.

Since we have a classification task -- honey or bumble bee -- we will use the support vector classifier (SVC), a type of SVM. We imported this class at the top of the notebook.

For an excellent visualization of how SVMs work, check out [this video](https://brohrer.github.io/open_the_black_box.html)

## 10. Score model
Now we'll use our trained model to generate predictions for our test data. To see how well our model did, we'll calculate the accuracy by comparing our predicted labels for the test set with the true labels in the test set. Accuracy is the number of correct predictions divided by the total number of predictions. Scikit-learn's accuracy_score function will do math for us. Sometimes accuracy can be misleading, but since we have an equal number of honey and bumble bees, it is a useful metric for this problem.

## 11. ROC curve + AUC
Above, we used svm.predict to predict either 0.0 or 1.0 for each image in X_test. Now, we'll use svm.predict_proba to get the probability that each class is the true label. For example, predict_proba returns [0.46195176, 0.53804824] for the first image, meaning there is a 46% chance the bee in the image is an Apis (0.0) and a 53% chance the bee in the image is a Bombus (1.0). Note that the two probabilities for each image always sum to 1. 
Using the default settings, probabilities of 0.5 or above are assigned a class label of 1.0 and those below are assigned a 0.0. However, this threshold can be adjusted. The receiver operating characteristic curve (ROC curve) plots the false positive rate and true positive rate at different thresholds. ROC curves are judged visually by how close they are to the upper lefthand corner. 
The area under the curve (AUC) is also calculated, where 1 means every predicted label was correct. Generally, the worst score for AUC is 0.5, which is the performance of a model that randomly guesses. See the scikit-learn documentation for more resources and examples on ROC curves and AUC.

"
https://github.com/mrbarkis/DataCamp_projects/tree/master/Predicting%20Credit%20Card%20Approvals,Predicting Credit Card Approvals,python,Sayak  Paul,Deep Learning Associate at PyImageSearch,https://www.datacamp.com/projects/558,"Build a machine learning model to predict if a credit card application will get approved. 
",https://www.github.com/mrbarkis/DataCamp_projects/blob/master/Predicting%20Credit%20Card%20Approvals/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/Predicting%20Credit%20Card%20Approvals/notebook.ipynb,"['import pandas as pd', 'import numpy as np', 'from sklearn.preprocessing import LabelEncoder', 'from sklearn.model_selection import train_test_split', 'from sklearn.preprocessing import MinMaxScaler', 'from sklearn.linear_model import LogisticRegression', 'from sklearn.metrics import confusion_matrix', 'from sklearn.model_selection import GridSearchCV']","['.data', '.nan', '.dtypes', '.index', '.preprocessing', '.dtypes', '.values', '.model_selection', '.values', '.33', '.preprocessing', '.linear_model', '.metrics', '.model_selection', '.best_score_', '.best_params_']","['.read_csv()', '.head()', '.describe()', '.info()', '.tail()', '.tail()', '.replace()', '.tail()', '.isnull()', '.sum()', '.sum()', '.mean()', '.fillna()', '.mean()', '.isnull()', '.sum()', '.sum()', '.value_counts()', '.fillna()', '.isnull()', '.sum()', '.sum()', '.fit_transform()', '.drop()', '.fit_transform()', '.fit_transform()', '.fit()', '.predict()', '.score()', '.fit_transform()', '.fit()']","['print()', 'print()', 'print()', 'print()', 'print()', 'print()', 'print()', 'LabelEncoder()', 'train_test_split()', 'MinMaxScaler()', 'LogisticRegression()', 'print()', 'confusion_matrix()', 'dict()', 'zip()', 'GridSearchCV()', 'print()']","## 1. Credit card applications
Commercial banks receive a lot of applications for credit cards. Many of them get rejected for many reasons, like high loan balances, low income levels, or too many inquiries on an individual's credit report, for example. Manually analyzing these applications is mundane, error-prone, and time-consuming (and time is money!). Luckily, this task can be automated with the power of machine learning and pretty much every commercial bank does so nowadays. In this notebook, we will build an automatic credit card approval predictor using machine learning techniques, just like the real banks do.

We'll use the Credit Card Approval dataset from the UCI Machine Learning Repository. The structure of this notebook is as follows:

First, we will start off by loading and viewing the dataset.
We will see that the dataset has a mixture of both numerical and non-numerical features, that it contains values from different ranges, plus that it contains a number of missing entries.
We will have to preprocess the dataset to ensure the machine learning model we choose can make good predictions.
After our data is in good shape, we will do some exploratory data analysis to build our intuitions.
Finally, we will build a machine learning model that can predict if an individual's application for a credit card will be accepted.

First, loading and viewing the dataset. We find that since this data is confidential, the contributor of the dataset has anonymized the feature names.

## 2. Inspecting the applications
The output may appear a bit confusing at its first sight, but let's try to figure out the most important features of a credit card application. The features of this dataset have been anonymized to protect the privacy, but this blog gives us a pretty good overview of the probable features. The probable features in a typical credit card application are Gender, Age, Debt, Married, BankCustomer, EducationLevel, Ethnicity, YearsEmployed, PriorDefault, Employed, CreditScore, DriversLicense, Citizen, ZipCode, Income and finally the ApprovalStatus. This gives us a pretty good starting point, and we can map these features with respect to the columns in the output.   
As we can see from our first glance at the data, the dataset has a mixture of numerical and non-numerical features. This can be fixed with some preprocessing, but before we do that, let's learn about the dataset a bit more to see if there are other dataset issues that need to be fixed.

## 3. Handling the missing values (part i)
We've uncovered some issues that will affect the performance of our machine learning model(s) if they go unchanged:

Our dataset contains both numeric and non-numeric data (specifically data that are of float64, int64 and object types). Specifically, the features 2, 7, 10 and 14 contain numeric values (of types float64, float64, int64 and int64 respectively) and all the other features contain non-numeric values.
The dataset also contains values from several ranges. Some features have a value range of 0 - 28, some have a range of 2 - 67, and some have a range of 1017 - 100000. Apart from these, we can get useful statistical information (like mean, max, and min) about the features that have numerical values. 
Finally, the dataset has missing values, which we'll take care of in this task. The missing values in the dataset are labeled with '?', which can be seen in the last cell's output.

Now, let's temporarily replace these missing value question marks with NaN.

## 4. Handling the missing values (part ii)
We replaced all the question marks with NaNs. This is going to help us in the next missing value treatment that we are going to perform.
An important question that gets raised here is why are we giving so much importance to missing values? Can't they be just ignored? Ignoring missing values can affect the performance of a machine learning model heavily. While ignoring the missing values our machine learning model may miss out on information about the dataset that may be useful for its training. Then, there are many models which cannot handle missing values implicitly such as LDA. 
So, to avoid this problem, we are going to impute the missing values with a strategy called mean imputation.

## 5. Handling the missing values (part iii)
We have successfully taken care of the missing values present in the numeric columns. There are still some missing values to be imputed for columns 0, 1, 3, 4, 5, 6 and 13. All of these columns contain non-numeric data and this why the mean imputation strategy would not work here. This needs a different treatment. 
We are going to impute these missing values with the most frequent values as present in the respective columns. This is good practice when it comes to imputing missing values for categorical data in general.

## 6. Preprocessing the data (part i)
The missing values are now successfully handled.
There is still some minor but essential data preprocessing needed before we proceed towards building our machine learning model. We are going to divide these remaining preprocessing steps into three main tasks:

Convert the non-numeric data into numeric.
Split the data into train and test sets. 
Scale the feature values to a uniform range.

First, we will be converting all the non-numeric values into numeric ones. We do this because not only it results in a faster computation but also many machine learning models (like XGBoost) (and especially the ones developed using scikit-learn) require the data to be in a strictly numeric format. We will do this by using a technique called label encoding.

## 7. Splitting the dataset into train and test sets
We have successfully converted all the non-numeric values to numeric ones.
Now, we will split our data into train set and test set to prepare our data for two different phases of machine learning modeling: training and testing. Ideally, no information from the test data should be used to scale the training data or should be used to direct the training process of a machine learning model. Hence, we first split the data and then apply the scaling.
Also, features like DriversLicense and ZipCode are not as important as the other features in the dataset for predicting credit card approvals. We should drop them to design our machine learning model with the best set of features. In Data Science literature, this is often referred to as feature selection. 

## 8. Preprocessing the data (part ii)
The data is now split into two separate sets - train and test sets respectively. We are only left with one final preprocessing step of scaling before we can fit a machine learning model to the data. 
Now, let's try to understand what these scaled values mean in the real world. Let's use CreditScore as an example. The credit score of a person is their creditworthiness based on their credit history. The higher this number, the more financially trustworthy a person is considered to be. So, a CreditScore of 1 is the highest since we're rescaling all the values to the range of 0-1.

## 9. Fitting a logistic regression model to the train set
Essentially, predicting if a credit card application will be approved or not is a classification task. According to UCI, our dataset contains more instances that correspond to ""Denied"" status than instances corresponding to ""Approved"" status. Specifically, out of 690 instances, there are 383 (55.5%) applications that got denied and 307 (44.5%) applications that got approved. 
This gives us a benchmark. A good machine learning model should be able to accurately predict the status of the applications with respect to these statistics.
Which model should we pick? A question to ask is: are the features that affect the credit card approval decision process correlated with each other? Although we can measure correlation, that is outside the scope of this notebook, so we'll rely on our intuition that they indeed are correlated for now. Because of this correlation, we'll take advantage of the fact that generalized linear models perform well in these cases. Let's start our machine learning modeling with a Logistic Regression model (a generalized linear model).

## 10. Making predictions and evaluating performance
But how well does our model perform? 
We will now evaluate our model on the test set with respect to classification accuracy. But we will also take a look the model's confusion matrix. In the case of predicting credit card applications, it is equally important to see if our machine learning model is able to predict the approval status of the applications as denied that originally got denied. If our model is not performing well in this aspect, then it might end up approving the application that should have been approved. The confusion matrix helps us to view our model's performance from these aspects.  

## 11. Grid searching and making the model perform better
Our model was pretty good! It was able to yield an accuracy score of almost 84%.
For the confusion matrix, the first element of the of the first row of the confusion matrix denotes the true negatives meaning the number of negative instances (denied applications) predicted by the model correctly. And the last element of the second row of the confusion matrix denotes the true positives meaning the number of positive instances (approved applications) predicted by the model correctly.
Let's see if we can do better. We can perform a grid search of the model parameters to improve the model's ability to predict credit card approvals.
scikit-learn's implementation of logistic regression consists of different hyperparameters but we will grid search over the following two:

tol
max_iter


## 12. Finding the best performing model
We have defined the grid of hyperparameter values and converted them into a single dictionary format which GridSearchCV() expects as one of its parameters. Now, we will begin the grid search to see which values perform best.
We will instantiate GridSearchCV() with our earlier logreg model with all the data we have. Instead of passing train and test sets separately, we will supply X (scaled version) and y. We will also instruct GridSearchCV() to perform a cross-validation of five folds.
We'll end the notebook by storing the best-achieved score and the respective best parameters.
While building this credit card predictor, we tackled some of the most widely-known preprocessing steps such as scaling, label encoding, and missing value imputation. We finished with some machine learning to predict if a person's application for a credit card would get approved or not given some information about that person.

"
https://github.com/mrbarkis/DataCamp_projects/tree/master/Real-time%20Insights%20from%20Social%20Media%20Data,Real-time Insights from Social Media Data,python,Samia Khalid,Senior Software Engineer at Microsoft,https://www.datacamp.com/projects/760,"Learn to analyze Twitter data and do a deep dive into a hot trend.
",https://www.github.com/mrbarkis/DataCamp_projects/blob/master/Real-time%20Insights%20from%20Social%20Media%20Data/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/Real-time%20Insights%20from%20Social%20Media%20Data/notebook.ipynb,"['import json', 'import matplotlib.pyplot as plt', 'import pandas as pd', 'from collections import Counter']","['.json', '.json', '.json', '.pyplot', '.style']","['.loads()', '.read()', '.loads()', '.read()', '.dumps()', '.dumps()', '.keys()', '.keys()', '.intersection()', '.loads()', '.read()', '.keys()', '.keys()', '.dumps()', '.dumps()', '.dumps()', '.most_common()', '.DataFrame()', '.groupby()', '.sum()', '.sort_values()', '.background_gradient()', '.append()', '.keys()', '.hist()']","['open()', 'open()', 'print()', 'print()', 'print()', 'print()', 'print()', 'print()', 'type()', 'print()', 'len()', 'print()', 'type()', 'print()', 'print()', 'type()', 'print()', 'type()', 'print()', 'set()', 'set()', 'print()', 'print()', 'len()', 'open()', 'print()', 'type()', 'print()', 'len()', 'print()', 'type()', 'print()', 'print()', 'Counter()', 'Counter()', 'len()', 'print()']","## 1. Local and global thought patterns
While we might not be Twitter fans, we have to admit that it has a huge influence on the world (who doesn't know about Trump's tweets). Twitter data is not only gold in terms of insights, but Twitter-storms are available for analysis in near real-time. This means we can learn about the big waves of thoughts and moods around the world as they arise. 
As any place filled with riches, Twitter has security guards blocking us from laying our hands on the data right away ⛔️ Some  authentication steps (really straightforward) are needed to call their APIs for data collection. Since our goal today is learning to extract insights from data, we have already gotten a green-pass from security ✅ Our data is ready for usage in the datasets folder — we can concentrate on the fun part! 🕵️‍♀️🌎




Twitter provides both global and local trends. Let's load and inspect data for topics that were hot worldwide (WW) and in the United States (US) at the moment of query  — snapshot of JSON response from the call to Twitter's GET trends/place API.
Note: Here is the documentation for this call, and here a full overview on Twitter's APIs.

## 2. Prettifying the output
Our data was hard to read! Luckily, we can resort to the json.dumps() method to have it formatted as a pretty JSON string.

## 3.  Finding common trends
🕵️‍♀️ From the pretty-printed results (output of the previous task), we can observe that:

We have an array of trend objects having: the name of the trending topic, the query parameter that can be used to search for the topic on Twitter-Search, the search URL and the volume of tweets for the last 24 hours, if available. (The trends get updated every 5 mins.)
At query time #BeratKandili, #GoodFriday and #WeLoveTheEarth were trending WW.
""tweet_volume"" tell us that #WeLoveTheEarth was the most popular among the three.
Results are not sorted by ""tweet_volume"". 
There are some trends which are unique to the US.


It’s easy to skim through the two sets of trends and spot common trends, but let's not do ""manual"" work. We can use Python’s set data structure to find common trends — we can iterate through the two trends objects, cast the lists of names to sets, and call the intersection method to get the common names between the two sets.

## 4. Exploring the hot trend
🕵️‍♀️ From the intersection (last output) we can see that, out of the two sets of trends (each of size 50), we have 11 overlapping topics. In particular, there is one common trend that sounds very interesting: #WeLoveTheEarth — so good to see that Twitteratis are unanimously talking about loving Mother Earth! 💚 
Note: We could have had no overlap or a much higher overlap; when we did the query for getting the trends, people in the US could have been on fire obout topics only relevant to them.


Image Source:Official Music Video Cover: https://welovetheearth.org/video/

We have found a hot-trend, #WeLoveTheEarth. Now let's see what story it is screaming to tell us! 
If we query Twitter's search API with this hashtag as query parameter, we get back actual tweets related to it. We have the response from the search API stored in the datasets folder as 'WeLoveTheEarth.json'. So let's load this dataset and do a deep dive in this trend.

## 5. Digging deeper
🕵️‍♀️ Printing the first two tweet items makes us realize that there’s a lot more to a tweet than what we normally think of as a tweet — there is a lot more than just a short text!

But hey, let's not get overwhemled by all the information in a tweet object! Let's focus on a few interesting fields and see if we can find any hidden insights there. 

## 6. Frequency analysis
🕵️‍♀️ Just from the first few results of the last extraction, we can deduce that:

We are talking about a song about loving the Earth.
A lot of big artists are the forces behind this Twitter wave, especially Lil Dicky.
Ed Sheeran was some cute koala in the song — ""EdSheeranTheKoala"" hashtag! 🐨


Observing the first 10 items of the interesting fields gave us a sense of the data. We can now take a closer look by doing a simple, but very useful, exercise — computing frequency distributions. Starting simple with frequencies is generally a good approach; it helps in getting ideas about how to proceed further.

## 7. Activity around the trend
🕵️‍♀️ Based on the last frequency distributions we can further build-up on our deductions:

We can more safely say that this was a music video about Earth (hashtag 'EarthMusicVideo') by Lil Dicky. 
DiCaprio is not a music artist, but he was involved as well (Leo is an environmentalist so not a surprise to see his name pop up here). 
We can also say that the video was released on a Friday; very likely on April 19th. 

We have been able to extract so many insights. Quite powerful, isn't it?!

Let's further analyze the data to find patterns in the activity around the tweets — did all retweets occur around a particular tweet? 
If a tweet has been retweeted, the 'retweeted_status'  field gives many interesting details about the original tweet itself and its author. 
We can measure a tweet's popularity by analyzing the retweetcount and favoritecount fields. But let's also extract the number of followers of the tweeter  —  we have a lot of celebs in the picture, so can we tell if their advocating for #WeLoveTheEarth influenced a significant proportion of their followers?

Note: The retweet_count gives us the total number of times the original tweet was retweeted. It should be the same in both the original tweet and all the next retweets. Tinkering around with some sample tweets and the official documentaiton are the way to get your head around the mnay fields.

## 8. A table that speaks a 1000 words
Let's manipulate the data further and visualize it in a better and richer way — ""looks matter!""

## 9. Analyzing used languages
🕵️‍♀️ Our table tells us that:

Lil Dicky's followers reacted the most — 42.4% of his followers liked his first tweet. 
Even if celebrities like Katy Perry and Ellen have a huuge Twitter following, their followers hardly reacted, e.g., only 0.0098% of Katy's followers liked her tweet. 
While Leo got the most likes and retweets in terms of counts, his first tweet was only liked by 2.19% of his followers. 

The large differences in reactions could be explained by the fact that this was Lil Dicky's music video. Leo still got more traction than Katy or Ellen because he played some major role in this initiative.

Can we find some more interesting patterns in the data? From the text of the tweets, we could spot different languages, so let's create a frequency distribution for the languages.

## 10. Final thoughts
🕵️‍♀️ The last histogram tells us that:

Most of the tweets were in English.
Polish, Italian and Spanish were the next runner-ups. 
There were a lot of tweets with a language alien to Twitter (lang = 'und'). 

Why is this sort of information useful? Because it can allow us to get an understanding of the ""category"" of people interested in this topic (clustering). We could also analyze the device type used by the Twitteratis, tweet['source'], to answer questions like, ""Does owning an Apple compared to Andorid influences people's propensity towards this trend?"". I will leave that as a further exercise for you!



What an exciting journey it has been! We started almost clueless, and here we are.. rich in insights. 

From location based comparisons to analyzing the activity around a tweet to finding patterns from languages and devices, we have covered a lot today — let's give ourselves a well-deserved pat on the back! ✋


Magic Formula = Data + Python + Creativity + Curiosity


"
https://github.com/mrbarkis/DataCamp_projects/tree/master/Recreating%20John%20Snow's%20Ghost%20Map,Recreating John Snow's Ghost Map,python,Radovan Kavicky,President and Principal Data Scientist at GapData Institute,https://www.datacamp.com/projects/132,"Recreate John Snow's famous map of the 1854 cholera outbreak in London.
",https://www.github.com/mrbarkis/DataCamp_projects/blob/master/Recreating%20John%20Snow's%20Ghost%20Map/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/Recreating%20John%20Snow's%20Ghost%20Map/notebook.ipynb,"['import pandas as pd', 'import folium', 'import bokeh', 'from bokeh.plotting import output_notebook, figure, show']","['.csv', '.shape', '.5132119', '.13666', '.4', '.csv', '.csv', '.dt', '.weekday_name', '.plotting', '.resources', '.INLINE', '.2']","['.read_csv()', '.head()', '.info()', '.rename()', '.describe()', '.iterrows()', '.Map()', '.CircleMarker()', '.add_to()', '.read_csv()', '.Marker()', '.add_to()', '.read_csv()', '.to_datetime()', '.info()', '.groupby()', '.sum()', '.line()', '.circle()', '.line()']","['print()', 'zip()', 'list()', 'len()', 'range()', 'len()', 'zip()', 'range()', 'len()', 'output_notebook()', 'figure()', 'show()']","## 1. Dr. John Snow

Dr. John Snow (1813-1858) was a famous British physician and is widely recognized as a legendary figure in the history of public health and a leading pioneer in the development of anesthesia. Some even say one of the greatest physicians of all time.
As a leading advocate of both anesthesia and hygienic practices in medicine, he not only experimented with ether and chloroform but also designed a mask and method how to administer it. He personally administered chloroform to Queen Victoria during the births of her eighth and ninth children, in 1853 and 1857, which assured a growing public acceptance of the use of anesthetics during childbirth.
But, as we will show later, not all his life was just a success. John Snow is now also recognized as one of the founders of modern epidemiology (some also consider him as the founder of data visualization, spatial analysis, data science in general, and many other related fields) for his scientific and pretty modern data approach in identifying the source of a cholera outbreak in Soho, London in 1854, but it wasn't always like this. In fact, for a long time, he was simply ignored by the scientific community and currently is very often mythified. 
In this notebook, we're not only going to rediscover his ""data story"", but reanalyze the data that he collected in 1854 and recreate his famous map (also called The Ghost Map).

## 2. Cholera attacks!

Prior to John Snow's discovery cholera was a regular visitor to London’s overcrowded and unsanitary streets. During the time of the third cholera outbreak, it was one of the most studied subjects (between years 1839-1856 over 700 studies and essays were published in London alone) and nearly all of the authors believed the outbreaks were due to miasma or ""bad air"". 
It was John Snow's pioneering work with anesthesia and gases that made him doubt the miasma model of the disease. Originally he formulated and published his theory that cholera is spread by water or food  in an essay On the Mode of Communication of Cholera (before the outbreak in 1849). The essay received negative reviews in the Lancet and the London Medical Gazette. 
We know now that he was right, but Dr. Snow's dilemma was how to prove it? His first step to getting there was checking the data. Our dataset has 489 rows of data in 3 columns but to work with dataset more easily we will first make few changes. 

## 3. You know nothing, John Snow!

It was somehow unthinkable that one man could debunk the miasma theory and prove that all the others got it wrong, so his work was mostly ignored. His medical colleagues simply said: ""You know nothing, John Snow!""
As already mentioned John Snow's first attempt to debunk the ""miasma"" theory ended with negative reviews. However, a reviewer made a helpful suggestion in terms of what evidence would be compelling: the crucial natural experiment would be to find people living side by side with lifestyles similar in all respects except for the water source. The cholera outbreak in Soho, London in 1854 gave Snow the opportunity not only to save lives this time but also to further test and improve his theory. But what about the final proof that he was right?  
We now know how John Snow did it, so let's get the data right first.

## 4. The Ghost Map
 
His original map, unfortunately, is not available (it might never even existed). We can see the famous one that he drew about a year later in 1855, though, and it is displayed in this cell. Because the map depicts and visualizes the deaths sometimes it is called also The Ghost Map. 
We now know how John Snow did it and have the data too, so let's recreate his map using modern techniques. 

## 5. It's the pump!

After marking the deaths on the map, what John Snow saw was not a random pattern (we saw this on our recreation of The Ghost Map too). The majority of the deaths were concentrated at the corner of Broad Street (now Broadwick Street) and Cambridge Street (now Lexington Street). A cluster of deaths around the junction of these streets was the epicenter of the outbreak, but what was there? Yes, a water pump.
John Snow at the time already had a developed theory that cholera spreads through water, so to test this he marked on the map also the locations of the water pumps nearby. And here it was, the whole picture.
By combining the location of deaths related to cholera with locations of the water pumps, Snow was able to show that the majority were clustered around one particular public water pump in Broad Street, Soho. Finally, he had the proof that he needed.
We will now do the same and add the locations of the pumps to our recreation of The Ghost Map.

## 6.  You know nothing, John Snow! (again)

So, John Snow finally had his proof that there was a connection between deaths as a consequence of the cholera outbreak and the public water pump that was probably contaminated. But he didn't just stop there and investigated further.
He was looking for anomalies now (we would now say ""outliers in data"") and found two in fact where there were no deaths. First was brewery right on the Broad Street, so he went there and learned that they drank mostly beer (in other words not the water from the local pump, which confirms his theory that the pump is the source of the outbreak). The second building without any deaths was workhouse near Poland street where he learned that their source of water was not the pump on the Broad Street (confirmation again). The locations of both buildings are visualized also on the map on the left.
He was now sure, and although officials did not trust him nor his theory they removed the handle to the pump next day, 8th of September 1854. John Snow later collected and published in his famous book also all the data about deaths in chronological order, before and after the peak of the outbreak and we will now analyze and compare the effect when the handle was removed.

## 7. The picture worth a thousand words
 
Removing the handle from the pump prevented any more of the infected water from being collected. The spring below the pump was later found to have been contaminated with sewage. This act was later recognized as an early example of epidemiology, public health medicine and the application of science (the germ theory of disease) in a real-life crisis. 
A replica of the pump, together with an explanatory and memorial plaque and without a handle was erected in 1992  near the location of the original close to the back wall of what today is the John Snow pub. The site is subtly marked with a pink granite kerbstone in front of a small wall plaque.
We can learn a lot from John Snow's data. We can take a look at absolute counts, but this observation could lead us to a wrong conclusion so let's take a different look on the data using Bokeh. 
Thanks to John Snow we have the data in chronological order (i.e. as time series data), so the best way to see the whole picture is to visualize it and look at it the way he saw it while writing On the Mode of Communication of Cholera (1855).

## 8. John Snow's myth & Did we learn something?
 
From the previous interactive visualization, we can clearly see that the peak of the cholera outbreak happened before removing the handle and it was already in decline (downside trajectory) before the 8th of September 1854.
This different view on the data is very important because in case that we compare just absolute numbers this could lead us to wrong conclusion that removing the handle on Broad Street pump for sure stopped the outbreak, which is simply not true (it surely did help but did not stop the outbreak) and John Snow was aware of this (he just did what needed to be done and never aspired to become a hero).
But people love stories about heroes and other myths (definitely more than science or data science). According to John Snow's myth, he was the superhero who in two days defied their equals by hypothesizing that cholera was a waterborne disease. Despite no one listening to him, he bravely continued drawing his map, convinced local authorities to remove the handle of the infected water pump with his findings, and stopped the outbreak. John Snow saved the lives of many Londoners.
If we take a better look behind this story, we can find also the true John Snow, who was fighting the disease with limited tools and wanted to get proof that he was right and ""knew something"" about cholera. He just did what he could with limited time and always boiled his water before drinking.

"
https://github.com/mrbarkis/DataCamp_projects/tree/master/Risk%20and%20Returns:%20The%20Sharpe%20Ratio,Risk and Returns: The Sharpe Ratio,python,Stefan Jansen,Founder & Lead Data Scientist at Applied Artificial Intelligence,https://www.datacamp.com/projects/66,"Use pandas to calculate and compare profitability and risk of different investments using the Sharpe Ratio.
",https://www.github.com/mrbarkis/DataCamp_projects/blob/master/Risk%20and%20Returns:%20The%20Sharpe%20Ratio/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/Risk%20and%20Returns:%20The%20Sharpe%20Ratio/notebook.ipynb,"['import pandas as pd', 'import numpy as np', 'import matplotlib.pyplot as plt']","['.pyplot', '.style', '.csv', '.csv', '.plot', '.plot', '.plot']","['.use()', '.read_csv()', '.dropna()', '.read_csv()', '.dropna()', '.info()', '.info()', '.plot()', '.show()', '.describe()', '.plot()', '.show()', '.describe()', '.pct_change()', '.plot()', '.show()', '.describe()', '.pct_change()', '.plot()', '.show()', '.describe()', '.sub()', '.plot()', '.show()', '.describe()', '.mean()', '.bar()', '.show()', '.std()', '.bar()', '.show()', '.div()', '.sqrt()', '.bar()', '.show()']","['print()', 'print()', 'print()', 'print()']","## 1. Meet Professor William Sharpe
An investment may make sense if we expect it to return more money than it costs. But returns are only part of the story because they are risky - there may be a range of possible outcomes. How does one compare different investments that may deliver similar results on average, but exhibit different levels of risks?

Enter William Sharpe. He introduced the reward-to-variability ratio in 1966 that soon came to be called the Sharpe Ratio. It compares the expected returns for two investment opportunities and calculates the additional return per unit of risk an investor could obtain by choosing one over the other. In particular, it looks at the difference in returns for two investments and compares the average difference to the standard deviation (as a measure of risk) of this difference. A higher Sharpe ratio means that the reward will be higher for a given amount of risk. It is common to compare a specific opportunity against a benchmark that represents an entire category of investments.
The Sharpe ratio has been one of the most popular risk/return measures in finance, not least because it's so simple to use. It also helped that Professor Sharpe won a Nobel Memorial Prize in Economics in 1990 for his work on the capital asset pricing model (CAPM).
The Sharpe ratio is usually calculated for a portfolio and uses the risk-free interest rate as benchmark. We will simplify our example and use stocks instead of a portfolio. We will also use a stock index as benchmark rather than the risk-free interest rate because both are readily available at daily frequencies and we do not have to get into converting interest rates from annual to daily frequency. Just keep in mind that you would run the same calculation with portfolio returns and your risk-free rate of choice, e.g, the 3-month Treasury Bill Rate. 
So let's learn about the Sharpe ratio by calculating it for the stocks of the two tech giants Facebook and Amazon. As benchmark we'll use the S&P 500 that measures the performance of the 500 largest stocks in the US. When we use a stock index instead of the risk-free rate, the result is called the Information Ratio and is used to benchmark the return on active portfolio management because it tells you how much more return for a given unit of risk your portfolio manager earned relative to just putting your money into a low-cost index fund.

## 2. A first glance at the data
Let's take a look the data to find out how many observations and variables we have at our disposal.

## 3. Plot & summarize daily prices for Amazon and Facebook
Before we compare an investment in either Facebook or Amazon with the index of the 500 largest companies in the US, let's visualize the data, so we better understand what we're dealing with.

## 4. Visualize & summarize daily values for the S&P 500
Let's also take a closer look at the value of the S&P 500, our benchmark.

## 5. The inputs for the Sharpe Ratio: Starting with Daily Stock Returns
The Sharpe Ratio uses the difference in returns between the two investment opportunities under consideration.
However, our data show the historical value of each investment, not the return. To calculate the return, we need to calculate the percentage change in value from one day to the next. We'll also take a look at the summary statistics because these will become our inputs as we calculate the Sharpe Ratio. Can you already guess the result?

## 6. Daily S&P 500 returns
For the S&P 500, calculating daily returns works just the same way, we just need to make sure we select it as a Series using single brackets [] and not as a DataFrame to facilitate the calculations in the next step.

## 7. Calculating Excess Returns for Amazon and Facebook vs. S&P 500
Next, we need to calculate the relative performance of stocks vs. the S&P 500 benchmark. This is calculated as the difference in returns between stock_returns and sp_returns for each day.

## 8. The Sharpe Ratio, Step 1: The Average Difference in Daily Returns Stocks vs S&P 500
Now we can finally start computing the Sharpe Ratio. First we need to calculate the average of the excess_returns. This tells us how much more or less the investment yields per day compared to the benchmark.

## 9. The Sharpe Ratio, Step 2: Standard Deviation of the Return Difference
It looks like there was quite a bit of a difference between average daily returns for Amazon and Facebook.
Next, we calculate the standard deviation of the excess_returns. This shows us the amount of risk an investment in the stocks implies as compared to an investment in the S&P 500.

## 10. Putting it all together
Now we just need to compute the ratio of avg_excess_returns and sd_excess_returns. The result is now finally the Sharpe ratio and indicates how much more (or less) return the investment opportunity under consideration yields per unit of risk.
The Sharpe Ratio is often annualized by multiplying it by the square root of the number of periods. We have used daily data as input, so we'll use the square root of the number of trading days (5 days, 52 weeks, minus a few holidays): √252

## 11. Conclusion
Given the two Sharpe ratios, which investment should we go for? In 2016, Amazon had a Sharpe ratio twice as high as Facebook. This means that an investment in Amazon returned twice as much compared to the S&P 500 for each unit of risk an investor would have assumed. In other words, in risk-adjusted terms, the investment in Amazon would have been more attractive.
This difference was mostly driven by differences in return rather than risk between Amazon and Facebook. The risk of choosing Amazon over FB (as measured by the standard deviation) was only slightly higher so that the higher Sharpe ratio for Amazon ends up higher mainly due to the higher average daily returns for Amazon. 
When faced with investment alternatives that offer both different returns and risks, the Sharpe Ratio helps to make a decision by adjusting the returns by the differences in risk and allows an investor to compare investment opportunities on equal terms, that is, on an 'apples-to-apples' basis.

"
https://github.com/mrbarkis/DataCamp_projects/tree/master/TV%2C%20Halftime%20Shows%2C%20and%20the%20Big%20Game,"TV, Halftime Shows, and the Big Game",python,David Venturi,Curriculum Manager at DataCamp,https://www.datacamp.com/projects/684,"Load, clean, and explore Super Bowl data in the age of soaring ad costs and flashy halftime shows.
",https://www.github.com/mrbarkis/DataCamp_projects/blob/master/TV%2C%20Halftime%20Shows%2C%20and%20the%20Big%20Game/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/TV%2C%20Halftime%20Shows%2C%20and%20the%20Big%20Game/notebook.ipynb,"['import pandas as pd', 'import seaborn as sns', 'from matplotlib import pyplot as plt']","['.csv', '.csv', '.csv', '.style', '.combined_pts', '.difference_pts', '.super_bowl', '.avg_us_viewers', '.super_bowl', '.rating_household', '.super_bowl', '.ad_cost', '.super_bowl', '.super_bowl', '.musician', '.str', '.musician', '.str', '.values', '.num_songs', '.and']","['.read_csv()', '.read_csv()', '.read_csv()', '.head()', '.head()', '.head()', '.info()', '.info()', '.use()', '.hist()', '.xlabel()', '.ylabel()', '.show()', '.hist()', '.xlabel()', '.ylabel()', '.show()', '.merge()', '.regplot()', '.subplot()', '.plot()', '.title()', '.subplot()', '.plot()', '.title()', '.subplot()', '.plot()', '.title()', '.xlabel()', '.tight_layout()', '.sort_values()', '.groupby()', '.count()', '.reset_index()', '.sort_values()', '.contains()', '.contains()', '.hist()', '.dropna()', '.xlabel()', '.ylabel()', '.show()', '.sort_values()', '.head()']","['display()', 'display()', 'display()', 'print()', 'display()', 'display()', 'game()', 'display()', 'display()', 'int()', 'max()', 'display()', 'print()']","## 1. TV, halftime shows, and the Big Game
Whether or not you like football, the Super Bowl is a spectacle. There's a little something for everyone at your Super Bowl party. Drama in the form of blowouts, comebacks, and controversy for the sports fan. There are the ridiculously expensive ads, some hilarious, others gut-wrenching, thought-provoking, and weird. The half-time shows with the biggest musicians in the world, sometimes riding giant mechanical tigers or leaping from the roof of the stadium. It's a show, baby. And in this notebook, we're going to find out how some of the elements of this show interact with each other. After exploring and cleaning our data a little, we're going to answer questions like:

What are the most extreme game outcomes?
How does the game affect television viewership?
How have viewership, TV ratings, and ad cost evolved over time?
Who are the most prolific musicians in terms of halftime show performances?


Left Shark Steals The Show. Katy Perry performing at halftime of Super Bowl XLIX. Photo by Huntley Paton. Attribution-ShareAlike 2.0 Generic (CC BY-SA 2.0).
The dataset we'll use was scraped and polished from Wikipedia. It is made up of three CSV files, one with game data, one with TV data, and one with halftime musician data for all 52 Super Bowls through 2018. Let's take a look, using display() instead of print() since its output is much prettier in Jupyter Notebooks.

## 2. Taking note of dataset issues
For the Super Bowl game data, we can see the dataset appears whole except for missing values in the backup quarterback columns (qb_winner_2 and qb_loser_2), which make sense given most starting QBs in the Super Bowl (qb_winner_1 and qb_loser_1) play the entire game.
From the visual inspection of TV and halftime musicians data, there is only one missing value displayed, but I've got a hunch there are more. The Super Bowl goes all the way back to 1967, and the more granular columns (e.g. the number of songs for halftime musicians) probably weren't tracked reliably over time. Wikipedia is great but not perfect.
An inspection of the .info() output for tv and halftime_musicians shows us that there are multiple columns with null values.

## 3. Combined points distribution
For the TV data, the following columns have missing values and a lot of them:

total_us_viewers (amount of U.S. viewers who watched at least some part of the broadcast)
rating_18_49 (average % of U.S. adults 18-49 who live in a household with a TV that were watching for the entire broadcast)
share_18_49 (average % of U.S. adults 18-49 who live in a household with a TV in use that were watching for the entire broadcast)

For the halftime musician data, there are missing numbers of songs performed (num_songs) for about a third of the performances.
There are a lot of potential reasons for these missing values. Was the data ever tracked? Was it lost in history? Is the research effort to make this data whole worth it? Maybe. Watching every Super Bowl halftime show to get song counts would be pretty fun. But we don't have the time to do that kind of stuff now! Let's take note of where the dataset isn't perfect and start uncovering some insights.
Let's start by looking at combined points for each Super Bowl by visualizing the distribution. Let's also pinpoint the Super Bowls with the highest and lowest scores.

## 4. Point difference distribution
Most combined scores are around 40-50 points, with the extremes being roughly equal distance away in opposite directions. Going up to the highest combined scores at 74 and 75, we find two games featuring dominant quarterback performances. One even happened recently in 2018's Super Bowl LII where Tom Brady's Patriots lost to Nick Foles' underdog Eagles 41-33 for a combined score of 74.
Going down to the lowest combined scores, we have Super Bowl III and VII, which featured tough defenses that dominated. We also have Super Bowl IX in New Orleans in 1975, whose 16-6 score can be attributed to inclement weather. The field was slick from overnight rain, and it was cold at 46 °F (8 °C), making it hard for the Steelers and Vikings to do much offensively. This was the second-coldest Super Bowl ever and the last to be played in inclement weather for over 30 years. The NFL realized people like points, I guess.
UPDATE: In Super Bowl LIII in 2019, the Patriots and Rams broke the record for the lowest-scoring Super Bowl with a combined score of 16 points (13-3 for the Patriots).
Let's take a look at point difference now.

## 5. Do blowouts translate to lost viewers?
The vast majority of Super Bowls are close games. Makes sense. Both teams are likely to be deserving if they've made it this far. The closest game ever was when the Buffalo Bills lost to the New York Giants by 1 point in 1991, which was  best remembered for Scott Norwood's last-second missed field goal attempt that went wide right, kicking off four Bills Super Bowl losses in a row. Poor Scott. The biggest point discrepancy ever was 45 points (!) where Hall of Famer Joe Montana's led the San Francisco 49ers to victory in 1990, one year before the closest game ever.
I remember watching the Seahawks crush the Broncos by 35 points (43-8) in 2014, which was a boring experience in my opinion. The game was never really close. I'm pretty sure we changed the channel at the end of the third quarter. Let's combine our game data and TV to see if this is a universal phenomenon. Do large point differences translate to lost viewers? We can plot household share (average percentage of U.S. households with a TV in use that were watching for the entire broadcast) vs. point difference to find out.

## 6. Viewership and the ad industry over time
The downward sloping regression line and the 95% confidence interval for that regression suggest that bailing on the game if it is a blowout is common. Though it matches our intuition, we must take it with a grain of salt because the linear relationship in the data is weak due to our small sample size of 52 games.
Regardless of the score though, I bet most people stick it out for the halftime show, which is good news for the TV networks and advertisers. A 30-second spot costs a pretty \$5 million now, but has it always been that way? And how have number of viewers and household ratings trended alongside ad cost? We can find out using line plots that share a ""Super Bowl"" x-axis.

## 7. Halftime shows weren't always this great
We can see viewers increased before ad costs did. Maybe the networks weren't very data savvy and were slow to react? Makes sense since DataCamp didn't exist back then.
Another hypothesis: maybe halftime shows weren't that good in the earlier years? The modern spectacle of the Super Bowl has a lot to do with the cultural prestige of big halftime acts. I went down a YouTube rabbit hole and it turns out the old ones weren't up to today's standards. Some offenders:

Super Bowl XXVI in 1992: A Frosty The Snowman rap performed by children.
Super Bowl XXIII in 1989: An Elvis impersonator that did magic tricks and didn't even sing one Elvis song.
Super Bowl XXI in 1987: Tap dancing ponies. (Okay, that's pretty awesome actually.)

It turns out Michael Jackson's Super Bowl XXVII performance, one of the most watched events in American TV history, was when the NFL realized the value of Super Bowl airtime and decided they needed to sign big name acts from then on out. The halftime shows before MJ indeed weren't that impressive, which we can see by filtering our halftime_musician data.

## 8. Who has the most halftime show appearances?
Lots of marching bands. American jazz clarinetist Pete Fountain. Miss Texas 1973 playing a violin. Nothing against those performers, they're just simply not Beyoncé. To be fair, no one is.
Let's see all of the musicians that have done more than one halftime show, including their performance counts.

## 9. Who performed the most songs in a halftime show?
The world famous Grambling State University Tiger Marching Band takes the crown with six appearances. Beyoncé, Justin Timberlake, Nelly, and Bruno Mars are the only post-Y2K musicians with multiple appearances (two each).
From our previous inspections, the num_songs column has lots of missing values:

A lot of the marching bands don't have num_songs entries.
For non-marching bands, missing data starts occurring at Super Bowl XX.

Let's filter out marching bands by filtering out musicians with the word ""Marching"" in them and the word ""Spirit"" (a common naming convention for marching bands is ""Spirit of [something]""). Then we'll filter for Super Bowls after Super Bowl XX to address the missing data issue, then let's see who has the most number of songs.

## 10. Conclusion
So most non-band musicians do 1-3 songs per halftime show. It's important to note that the duration of the halftime show is fixed (roughly 12 minutes) so songs per performance is more a measure of how many hit songs you have. JT went off in 2018, wow. 11 songs! Diana Ross comes in second with 10 in her medley in 1996.
In this notebook, we loaded, cleaned, then explored Super Bowl game, television, and halftime show data. We visualized the distributions of combined points, point differences, and halftime show performances using histograms. We used line plots to see how ad cost increases lagged behind viewership increases. And we discovered that blowouts do appear to lead to a drop in viewers.
This year's Big Game will be here before you know it. Who do you think will win Super Bowl LIII?
UPDATE: Spoiler alert.

"
https://github.com/mrbarkis/DataCamp_projects/tree/master/The%20Android%20App%20Market%20on%20Google%20Play,The Android App Market on Google Play,python,Lavanya Gupta,Machine Learning Engineer at PropTiger.com,https://www.datacamp.com/projects/619,"Load, clean, and visualize scraped Google Play Store data to understand the Android app market.
",https://www.github.com/mrbarkis/DataCamp_projects/blob/master/The%20Android%20App%20Market%20on%20Google%20Play/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/The%20Android%20App%20Market%20on%20Google%20Play/notebook.ipynb,"['import pandas as pd', 'import plotly', 'import plotly.graph_objs as go', 'import seaborn as sns', 'import warnings', 'import matplotlib.pyplot as plt']","['.csv', '.shape', '.str', '.offline', '.graph_objs', '.index', '.values', '.offline', '.Rating', '.offline', '.Category', '.Category', '.Category', '.pyplot', '.Category', '.offline', '.csv', '.csv']","['.read_csv()', '.head()', '.info()', '.duplicated()', '.sum()', '.drop_duplicates()', '.sample()', '.replace()', '.to_numeric()', '.init_notebook_mode()', '.unique()', '.value_counts()', '.sort_values()', '.Bar()', '.iplot()', '.mean()', '.Histogram()', '.iplot()', '.set_style()', '.filterwarnings()', '.groupby()', '.filter()', '.reset_index()', '.nunique()', '.nunique()', '.nunique()', '.jointplot()', '.jointplot()', '.subplots()', '.set_size_inches()', '.isin()', '.stripplot()', '.set_title()', '.query()', '.subplots()', '.set_size_inches()', '.stripplot()', '.set_title()', '.Box()', '.Box()', '.Layout()', '.iplot()', '.read_csv()', '.sample()', '.merge()', '.dropna()', '.set_style()', '.subplots()', '.set_size_inches()', '.boxplot()', '.set_title()']","['print()', 'print()', 'len()', 'len()', 'print()', 'print()', 'round()', 'len()', 'print()', 'dict()']","## 1. Google Play Store apps and reviews
Mobile apps are everywhere. They are easy to create and can be lucrative. Because of these two factors, more and more apps are being developed. In this notebook, we will do a comprehensive analysis of the Android app market by comparing over ten thousand apps in Google Play across different categories. We'll look for insights in the data to devise strategies to drive growth and retention.

Let's take a look at the data, which consists of two files:

apps.csv: contains all the details of the applications on Google Play. There are 13 features that describe a given app.
user_reviews.csv: contains 100 reviews for each app, most helpful first. The text in each review has been pre-processed and attributed with three new features: Sentiment (Positive, Negative or Neutral), Sentiment Polarity and Sentiment Subjectivity.


## 2. Data cleaning
The three features that we will be working with most frequently henceforth are Installs, Size, and Price. A careful glance of the dataset reveals that some of these columns mandate data cleaning in order to be consumed by code we'll write later. Specifically, the presence of special characters (, $ +) and letters (M k) in the Installs, Size, and Price columns make their conversion to a numerical data type difficult. Let's clean by removing these and converting each column to a numeric type.

## 3. Exploring app categories
With more than 1 billion active users in 190 countries around the world, Google Play continues to be an important distribution platform to build a global audience. For businesses to get their apps in front of users, it's important to make them more quickly and easily discoverable on Google Play. To improve the overall search experience, Google has introduced the concept of grouping apps into categories.
This brings us to the following questions:

Which category has the highest share of (active) apps in the market? 
Is any specific category dominating the market?
Which categories have the fewest number of apps?

We will see that there are 33 unique app categories present in our dataset. Family and Game apps have the highest market prevalence. Interestingly, Tools, Business and Medical apps are also at the top.

## 4. Distribution of app ratings
After having witnessed the market share for each category of apps, let's see how all these apps perform on an average. App ratings (on a scale of 1 to 5) impact the discoverability, conversion of apps as well as the company's overall brand image. Ratings are a key performance indicator of an app.
From our research, we found that the average volume of ratings across all app categories is 4.17. The histogram plot is skewed to the right indicating that the majority of the apps are highly rated with only a few exceptions in the low-rated apps.

## 5. Size and price of an app
Let's now examine app size and app price. For size, if the mobile app is too large, it may be difficult and/or expensive for users to download. Lengthy download times could turn users off before they even experience your mobile app. Plus, each user's device has a finite amount of disk space. For price, some users expect their apps to be free or inexpensive. These problems compound if the developing world is part of your target market; especially due to internet speeds, earning power and exchange rates.
How can we effectively come up with strategies to size and price our app?

Does the size of an app affect its rating? 
Do users really care about system-heavy apps or do they prefer light-weighted apps? 
Does the price of an app affect its rating? 
Do users always prefer free apps over paid apps?

We find that the majority of top rated apps (rating over 4) range from 2 MB to 20 MB. We also find that the vast majority of apps price themselves under \$10.

## 6. Relation between app category and app price
So now comes the hard part. How are companies and developers supposed to make ends meet? What monetization strategies can companies use to maximize profit? The costs of apps are largely based on features, complexity, and platform.
There are many factors to consider when selecting the right pricing strategy for your mobile app. It is important to consider the willingness of your customer to pay for your app. A wrong price could break the deal before the download even happens. Potential customers could be turned off by what they perceive to be a shocking cost, or they might delete an app they’ve downloaded after receiving too many ads or simply not getting their money's worth.
Different categories demand different price ranges. Some apps that are simple and used daily, like the calculator app, should probably be kept free. However, it would make sense to charge for a highly-specialized medical app that diagnoses diabetic patients. Below, we see that Medical and Family apps are the most expensive. Some medical apps extend even up to \$80! All game apps are reasonably priced below \$20.

## 7. Filter out ""junk"" apps
It looks like a bunch of the really expensive apps are ""junk"" apps. That is, apps that don't really have a purpose. Some app developer may create an app called I Am Rich Premium or most expensive app (H) just for a joke or to test their app development skills. Some developers even do this with malicious intent and try to make money by hoping people accidentally click purchase on their app in the store.
Let's filter out these junk apps and re-do our visualization. The distribution of apps under \$20 becomes clearer.

## 8. Popularity of paid apps vs free apps
For apps in the Play Store today, there are five types of pricing strategies: free, freemium, paid, paymium, and subscription. Let's focus on free and paid apps only. Some characteristics of free apps are:

Free to download.
Main source of income often comes from advertisements.
Often created by companies that have other products and the app serves as an extension of those products.
Can serve as a tool for customer retention, communication, and customer service.

Some characteristics of paid apps are:

Users are asked to pay once for the app to download and use it.
The user can't really get a feel for the app before buying it.

Are paid apps installed as much as free apps? It turns out that paid apps have a relatively lower number of installs than free apps, though the difference is not as stark as I would have expected!

## 9. Sentiment analysis of user reviews
Mining user review data to determine how people feel about your product, brand, or service can be done using a technique called sentiment analysis. User reviews for apps can be analyzed to identify if the mood is positive, negative or neutral about that app. For example, positive words in an app review might include words such as 'amazing', 'friendly', 'good', 'great', and 'love'. Negative words might be words like 'malware', 'hate', 'problem', 'refund', and 'incompetent'.
By plotting sentiment polarity scores of user reviews for paid and free apps, we observe that free apps receive a lot of harsh comments, as indicated by the outliers on the negative y-axis. Reviews for paid apps appear never to be extremely negative. This may indicate something about app quality, i.e., paid apps being of higher quality than free apps on average. The median polarity score for paid apps is a little higher than free apps, thereby syncing with our previous observation.
In this notebook, we analyzed over ten thousand apps from the Google Play Store. We can use our findings to inform our decisions should we ever wish to create an app ourselves.

"
https://github.com/mrbarkis/DataCamp_projects/tree/master/The%20GitHub%20History%20of%20the%20Scala%20Language,The GitHub History of the Scala Language,python,Anita Sarma,Associate Professor at Oregon State University,https://www.datacamp.com/projects/163,"Find the true Scala experts by exploring its development history in Git and GitHub.
",https://www.github.com/mrbarkis/DataCamp_projects/blob/master/The%20GitHub%20History%20of%20the%20Scala%20Language/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/The%20GitHub%20History%20of%20the%20Scala%20Language/notebook.ipynb,['import pandas as pd'],"['.csv', '.csv', '.csv', '.dtypes', '.dtypes', '.dt', '.index', '.index', '.index', '.plot', '.plot', '.scala', '.scala', '.date', '.dt', '.year', '.plot', '.scala', '.dt', '.year', '.scala']","['.read_csv()', '.read_csv()', '.read_csv()', '.append()', '.to_datetime()', '.head()', '.merge()', '.head()', '.strftime()', '.groupby()', '.count()', '.head()', '.startswith()', '.split()', '.apply()', '.head()', '.to_datetime()', '.sort_index()', '.head()', '.bar()', '.set_xlabel()', '.set_ylabel()', '.groupby()', '.count()', '.hist()', '.set_xlabel()', '.sort_values()', '.merge()', '.head()', '.groupby()', '.count()', '.nlargest()', '.merge()', '.sort_values()', '.isin()', '.groupby()', '.agg()', '.reset_index()', '.head()', '.pivot_table()', '.head()', '.bar()', '.set_xlabel()', '.set_ylabel()', '.isin()', '.groupby()', '.count()', '.reset_index()', '.head()', '.pivot_table()', '.plot()', '.set_xlabel()', '.set_ylabel()']","['print()', 'len()', 'print()', 'len()', 'print()', 'len()', 'print()', 'print()', 'january_only()', 'set()', 'set()']","## 1. Scala's real-world project repository data
With almost 30k commits and a history spanning over ten years, Scala is a mature programming language. It is a general-purpose programming language that has recently become another prominent language for data scientists.
Scala is also an open source project. Open source projects have the advantage that their entire development histories -- who made changes, what was changed, code reviews, etc. -- publicly available. 
We're going to read in, clean up, and visualize the real world project repository of Scala that spans data from a version control system (Git) as well as a project hosting site (GitHub). We will find out who has had the most influence on its development and who are the experts.
The dataset we will use, which has been previously mined and extracted from GitHub, is comprised of three files:

pulls_2011-2013.csv contains the basic information about the pull requests, and spans from the end of 2011 up to (but not including) 2014.
pulls_2014-2018.csv contains identical information, and spans from 2014 up to 2018.
pull_files.csv contains the files that were modified by each pull request.


## 2. Preparing and cleaning the data
First, we will need to combine the data from the two separate pull DataFrames. 
Next, the raw data extracted from GitHub contains dates in the ISO8601 format. However, pandas imports them as regular strings. To make our analysis easier, we need to convert the strings into Python's DateTime objects. DateTime objects have the important property that they can be compared and sorted.
The pull request times are all in UTC (also known as Coordinated Universal Time). The commit times, however, are in the local time of the author with time zone information (number of hours difference from UTC). To make comparisons easy, we should convert all times to UTC.

## 3. Merging the DataFrames
The data extracted comes in two separate files. Merging the two DataFrames will make it easier for us to analyze the data in the future tasks.

## 4. Is the project still actively maintained?
The activity in an open source project is not very consistent. Some projects might be active for many years after the initial release, while others can slowly taper out into oblivion. Before committing to contributing to a project, it is important to understand the state of the project. Is development going steadily, or is there a drop? Has the project been abandoned altogether?
The data used in this project was collected in January of 2018. We are interested in the evolution of the number of contributions up to that date.
For Scala, we will do this by plotting a chart of the project's activity. We will calculate the number of pull requests submitted each (calendar) month during the project's lifetime. We will then plot these numbers to see the trend of contributions.

## 5. Is there camaraderie in the project?
The organizational structure varies from one project to another, and it can influence your success as a contributor. A project that has a very small community might not be the best one to start working on. The small community might indicate a high barrier of entry. This can be caused by several factors, including a community that is reluctant to accept pull requests from ""outsiders,"" that the code base is hard to work with, etc. However, a large community can serve as an indicator that the project is regularly accepting pull requests from new contributors. Such a project would be a good place to start.
In order to evaluate the dynamics of the community, we will plot a histogram of the number of pull requests submitted by each user. A distribution that shows that there are few people that only contribute a small number of pull requests can be used as in indicator that the project is not welcoming of new contributors. 

## 6. What files were changed in the last ten pull requests?
Choosing the right place to make a contribution is as important as choosing the project to contribute to. Some parts of the code might be stable, some might be dead. Contributing there might not have the most impact. Therefore it is important to understand the parts of the system that have been recently changed. This allows us to pinpoint the ""hot"" areas of the code where most of the activity is happening. Focusing on those parts might not the most effective use of our times.

## 7. Who made the most pull requests to a given file?
When contributing to a project, we might need some guidance. We might find ourselves needing some information regarding the codebase. It is important direct any questions to the right person. Contributors to open source projects generally have other day jobs, so their time is limited. It is important to address our questions to the right people. One way to identify the right target for our inquiries is by using their contribution history.
We identified src/compiler/scala/reflect/reify/phases/Calculate.scala as being recently changed. We are interested in the top 3 developers who changed that file. Those developers are the ones most likely to have the best understanding of the code.

## 8. Who made the last ten pull requests on a given file?
Open source projects suffer from fluctuating membership. This makes the problem of finding the right person more challenging: the person has to be knowledgeable and still be involved in the project. A person that contributed a lot in the past might no longer be available (or willing) to help. To get a better understanding, we need to investigate the more recent history of that particular part of the system. 
Like in the previous task, we will look at the history of  src/compiler/scala/reflect/reify/phases/Calculate.scala.

## 9. The pull requests of two special developers
Now that we have identified two potential contacts in the projects, we need to find the person who was most involved in the project in recent times. That person is most likely to answer our questions. For each calendar year, we are interested in understanding the number of pull requests the authors submitted. This will give us a high-level image of their contribution trend to the project.

## 10. Visualizing the contributions of each developer
As mentioned before, it is important to make a distinction between the global expertise and contribution levels and the contribution levels at a more granular level (file, submodule, etc.) In our case, we want to see which of our two developers of interest have the most experience with the code in a given file. We will measure experience by the number of pull requests submitted that affect that file and how recent those pull requests were submitted.

"
https://github.com/mrbarkis/DataCamp_projects/tree/master/The%20Hottest%20Topics%20in%20Machine%20Learning,The Hottest Topics in Machine Learning,python,Lars Hulstaert,Data Scientist at Microsoft,https://www.datacamp.com/projects/158,"Use Natural Language Processing on NIPS papers to uncover the trendiest topics in machine learning research.
",https://www.github.com/mrbarkis/DataCamp_projects/blob/master/The%20Hottest%20Topics%20in%20Machine%20Learning/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/The%20Hottest%20Topics%20in%20Machine%20Learning/notebook.ipynb,"['import pandas as pd', 'import matplotlib.pyplot', 'import re', 'import wordcloud', 'import numpy as np', 'import warnings', 'from sklearn.feature_extraction.text import CountVectorizer', 'from sklearn.decomposition import LatentDirichletAllocation as LDA']","['.csv', '.csv', '.gz', '.pyplot', '.plot', '.str', '.feature_extraction', '.text', '.pyplot', '.decomposition', '.components_']","['.read_csv()', '.head()', '.drop()', '.head()', '.groupby()', '.count()', '.size()', '.bar()', '.head()', '.map()', '.sub()', '.lower()', '.head()', '.join()', '.WordCloud()', '.generate()', '.to_image()', '.get_feature_names()', '.zeros()', '.toarray()', '.arange()', '.bar()', '.xticks()', '.xlabel()', '.ylabel()', '.title()', '.show()', '.fit_transform()', '.simplefilter()', '.get_feature_names()', '.join()', '.argsort()', '.fit()']","['print()', 'print()', 'plot_10_most_common_words()', 'len()', 'zip()', 'sorted()', 'len()', 'CountVectorizer()', 'plot_10_most_common_words()', 'print_topics()', 'enumerate()', 'print()', 'print()', 'LDA()', 'print()', 'print_topics()']","## 1. Loading the NIPS papers
The NIPS conference (Neural Information Processing Systems) is one of the most prestigious yearly events in the machine learning community. At each NIPS conference, a large number of research papers are published. Over 50,000 PDF files were automatically downloaded and processed to obtain a dataset on various machine learning techniques. These NIPS papers are stored in datasets/papers.csv. The CSV file contains information on the different NIPS papers that were published from 1987 until 2017 (30 years!). These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods and many more.

First, we will explore the CSV file to determine what type of data we can use for the analysis and how it is structured. A research paper typically consists of a title, an abstract and the main text. Other data such as figures and tables were not extracted from the PDF files. Each paper discusses a novel technique or improvement. In this analysis, we will focus on analyzing these papers with natural language processing methods.

## 2. Preparing the data for analysis
For the analysis of the papers, we are only interested in the text data associated with the paper as well as the year the paper was published in.
We will analyze this text data using natural language processing.  Since the file contains some metadata such as id's and filenames, it is necessary to remove all the columns that do not contain useful text information.

## 3. Plotting how machine learning has evolved over time
In order to understand how the machine learning field has recently exploded in popularity, we will begin by visualizing the number of publications per year. 
By looking at the number of published papers per year,  we can understand the extent of the machine learning 'revolution'! Typically, this significant increase in popularity is attributed to the large amounts of compute power, data and improvements in algorithms.

## 4. Preprocessing the text data
Let's now analyze the titles of the different papers to identify machine learning trends. First, we will perform some simple preprocessing on the titles in order to make them more amenable for analysis. We will use a regular expression to remove any punctuation in the title. Then we will perform lowercasing. We'll then print the titles of the first rows before and after applying the modification.

## 5.  A word cloud to visualize the preprocessed text data
In order to verify whether the preprocessing happened correctly, we can make a word cloud of the titles of the research papers. This will give us a visual representation of the most common words. Visualisation is key to understanding whether we are still on the right track! In addition, it allows us to verify whether we need additional preprocessing before further analyzing the text data.
Python has a massive number of open libraries! Instead of trying to develop a method to create word clouds ourselves, we'll use Andreas Mueller's wordcloud library.

## 6.  Prepare the text for LDA analysis
The main text analysis method that we will use is latent Dirichlet allocation (LDA). LDA is able to perform topic detection on large document sets, determining what the main 'topics' are in a large unlabeled set of texts. A 'topic' is a collection of words that tend to co-occur often. The hypothesis is that LDA might be able to clarify what the different topics in the research titles are. These topics can then be used as a starting point for further analysis.
LDA does not work directly on text data. First, it is necessary to convert the documents into a simple vector representation. This representation will then be used by LDA to determine the topics. Each entry of a 'document vector' will correspond with the number of times a word occurred in the document. In conclusion, we will convert a list of titles into a list of vectors, all with length equal to the vocabulary. For example, 'Analyzing machine learning trends with neural networks.' would be transformed into [1, 0, 1, ..., 1, 0].
We'll then plot the 10 most common words based on the outcome of this operation (the list of document vectors). As a check, these words should also occur in the word cloud.

## 7. Analysing trends with LDA
Finally, the research titles will be analyzed using LDA. Note that in order to process a new set of documents (e.g. news articles), a similar set of steps will be required to preprocess the data. The flow that was constructed here can thus easily be exported for a new text dataset.
The only parameter we will tweak is the number of topics in the LDA algorithm. Typically, one would calculate the 'perplexity' metric to determine which number of topics is best and iterate over different amounts of topics until the lowest 'perplexity' is found. For now, let's play around with a different number of topics. From there, we can distinguish what each topic is about ('neural networks', 'reinforcement learning', 'kernel methods', 'gaussian processes', etc.).

## 8. The future of machine learning
Machine learning has become increasingly popular over the past years. The number of NIPS conference papers has risen exponentially, and people are continuously looking for ways on how they can incorporate machine learning into their products and services.
Although this analysis focused on analyzing machine learning trends in research, a lot of these techniques are rapidly being adopted in industry. Following the latest machine learning trends is a critical skill for a data scientist, and it is recommended to continuously keep learning by going through blogs, tutorials, and courses.

"
https://github.com/mrbarkis/DataCamp_projects/tree/master/Up%20and%20Down%20With%20the%20Kardashians,Up and Down With the Kardashians,python,David Venturi,Curriculum Manager at DataCamp,https://www.datacamp.com/projects/538,"Plot Google Trends data to find the most famous Kardashian/Jenner sister. Is it Kim? Kendall? Kylie?
",https://www.github.com/mrbarkis/DataCamp_projects/blob/master/Up%20and%20Down%20With%20the%20Kardashians/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/Up%20and%20Down%20With%20the%20Kardashians/notebook.ipynb,['import pandas as pd'],"['.csv', '.columns', '.columns', '.str']","['.read_csv()', '.head()', '.head()', '.info()', '.to_string()', '.replace()', '.to_numeric()', '.info()', '.head()', '.to_datetime()', '.info()', '.head()', '.set_index()', '.head()', '.plot()', '.plot()', '.rolling()', '.mean()', '.plot()', '.mean()', '.mean()', '.plot()']",[],"## 1. The sisters and Google Trends
While I'm not a fan nor a hater of the Kardashians and Jenners, the polarizing family intrigues me. Why? Their marketing prowess. Say what you will about them and what they stand for, they are great at the hype game. Everything they touch turns to content.
The sisters in particular over the past decade have been especially productive in this regard. Let's get some facts straight. I consider the ""sisters"" to be the following daughters of Kris Jenner. Three from her first marriage to lawyer Robert Kardashian:

Kourtney Kardashian (daughter of Robert Kardashian, born in 1979)
Kim Kardashian (daughter of Robert Kardashian, born in 1980)
Khloé Kardashian (daughter of Robert Kardashian, born in 1984)

And two from her second marriage to Olympic gold medal-winning decathlete, Caitlyn Jenner (formerly Bruce):

Kendall Jenner (daughter of Caitlyn Jenner, born in 1995)
Kylie Jenner (daughter of Caitlyn Jenner, born in 1997)


This family tree can be confusing, but we aren't here to explain it. We're here to explore the data underneath the hype, and we'll do it using search interest data from Google Trends. We'll recreate the Google Trends plot to visualize their ups and downs over time, then make a few custom plots of our own. And we'll answer the big question: is Kim even the most famous sister anymore?
First, let's load and inspect our Google Trends data, which was downloaded in CSV form. The query parameters: each of the sisters, worldwide search data, 2007 to present day. (2007 was the year Kim became ""active"" according to Wikipedia.)

## 2. Better ""kolumn"" names
So we have a column for each month since January 2007 and a column for the worldwide search interest for each of the sisters each month. By the way, Google defines the values of search interest as:

Numbers represent search interest relative to the highest point on the chart for the given region and time. A value of 100 is the peak popularity for the term. A value of 50 means that the term is half as popular. A score of 0 means there was not enough data for this term.

Okay, that's great Google, but you are not making this data easily analyzable for us. I see a few things. Let's do the column names first. A column named ""Kim Kardashian: (Worldwide)"" is not the most usable for coding purposes. Let's shorten those so we can access their values better. Might as well standardize all column formats, too. I like lowercase, short column names.

## 3. Pesky data types
That's better. We don't need to scroll our eyes across the table to read the values anymore since it is much less wide. And seeing five columns that all start with the letter ""k"" ... the aesthetics ... we should call them ""kolumns"" now! (Bad joke.)
The next thing I see that is going to be an issue is that ""<"" sign. If ""a score of 0 means there was not enough data for this term,"" ""<1"" must mean it is between 0 and 1 and Google does not want to give us the fraction from google.trends.com for whatever reason. That's fine, but this ""<"" sign means we won't be able to analyze or visualize our data right away because those column values aren't going to be represented as numbers in our data structure. Let's confirm that by inspecting our data types.

## 4. From object to integer
Yes, okay, the khloe, kourtney, and kendall columns aren't integers like the kim and kylie columns are. Again, because of the ""<"" sign that indicates a search interest value between zero and one. Is this an early hint at the hierarchy of sister popularity? We'll see shortly. Before that, we'll need to remove that pesky ""<"" sign. Then we can change the type of those columns to integer.

## 5. From object to datetime
Okay, great, no more ""<"" signs. All the sister columns are of integer type.
Now let's convert our month column from type object to datetime to make our date data more accessible.

## 6. Set month as index
And finally, let's set the month column as our index to wrap our data cleaning. Having month as index rather than the zero-based row numbers will allow us to write shorter lines of code to create plots, where month will represent our x-axis.

## 7. The early Kim hype
Okay! So our data is ready to plot. Because we cleaned our data, we only need one line of code (and just thirteen characters!) to remake the Google Trends chart, plus another line to make the plot show up in our notebook.

## 8. Kylie's rise
Oh my! There is so much to make sense of here. Kim's sharp rise in 2007, with the beginning of Keeping Up with the Kardashians, among other things. There was no significant search interest for the other four sisters until mid-2009 when Kourtney and Khloé launched the reality television series, Kourtney and Khloé Take Miami. Then there was Kim's rise from famous to literally more famous than God in 2011. This Cosmopolitan article covers the timeline that includes the launch of music videos, fragrances,  iPhone and Android games, another television series, joining Instagram, and more. Then there was Kim's ridiculous spike in December 2014: posing naked on the cover of Paper Magazine in a bid to break the internet will do that for you.
A curious thing starts to happen after that bid as well. Let's zoom in...

## 9. Smooth out the fluctuations with rolling means
It looks like my suspicion may be true: Kim is not always the most searched Kardashian or Jenner sister. Since late-2016, at various months, Kylie overtakes Kim. Two big spikes where she smashed Kim's search interest: in September 2017 when it was reported that Kylie was expecting her first child with rapper Travis Scott and in February 2018 when she gave birth to her daughter, Stormi Webster. The continued success of Kylie Cosmetics has kept her in the news, not to mention making her the ""The Youngest Self-Made Billionaire Ever"" according to Forbes.
These fluctuations are descriptive but do not really help us answer our question: is Kim even the most famous sister anymore? We can use rolling means to smooth out short-term fluctuations in time series data and highlight long-term trends. Let's make the window twelve months a.k.a. one year.

## 10. Who's more famous? The Kardashians or the Jenners?
Whoa, okay! So by this metric, Kim is still the most famous sister despite Kylie being close and nearly taking her crown. Honestly, the biggest takeaway from this whole exercise might be Kendall not showing up that much. It makes sense, though, despite her wildly successful modeling career. Some have called her ""the only normal one in her family"" as she tends to shy away from the more dramatic and controversial parts of the media limelight that generate oh so many clicks.
Let's end this analysis with one last plot. In it, we will plot (pun!) the Kardashian sisters against the Jenner sisters to see which family line is more popular now. We will use average search interest to make things fair, i.e., total search interest divided by the number of sisters in the family line.
The answer? Since 2015, it has been a toss-up. And in the future? With this family and their penchant for big events, who knows?

"
https://github.com/mrbarkis/DataCamp_projects/tree/master/Who%20Is%20Drunk%20and%20When%20in%20Ames%2C%20Iowa%3F,"Who Is Drunk and When in Ames, Iowa?",python,Samantha Tyner,Postdoctoral Research Associate at Iowa State University,https://www.datacamp.com/projects/475,"Flex your data manipulation muscles on breath alcohol test data from Ames, Iowa, USA.
",https://www.github.com/mrbarkis/DataCamp_projects/blob/master/Who%20Is%20Drunk%20and%20When%20in%20Ames%2C%20Iowa%3F/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/Who%20Is%20Drunk%20and%20When%20in%20Ames%2C%20Iowa%3F/notebook.ipynb,['import pandas as pd'],"['.csv', '.plot', '.bar', '.plot', '.08', '.08', '.shape', '.shape', '.dt', '.week']","['.read_csv()', '.head()', '.value_counts()', '.value_counts()', '.groupby()', '.size()', '.bar()', '.groupby()', '.size()', '.sort_values()', '.bar()', '.value_counts()', '.dropna()', '.assign()', '.boxplot()', '.to_datetime()', '.head()', '.groupby()', '.count()', '.unstack()', '.plot()']",['print()'],"## 1. Breath alcohol tests in Ames, Iowa, USA
Ames, Iowa, USA is the home of Iowa State University, a land grant university with over 36,000 students. By comparison, the city of Ames, Iowa, itself only has about 65,000 residents. As with any other college town, Ames has had its fair share of alcohol-related incidents. (For example, Google 'VEISHEA riots 2014'.) We will take a look at some breath alcohol test data from Ames that is published by the State of Iowa.
 
The data file 'breath_alcohol_ames.csv' contains 1,556 readings from breath alcohol tests administered by the Ames and Iowa State University Police Departments from January 2013 to December 2017. The columns in this data set are year, month, day, hour, location, gender, Res1, Res2.

## 2. What is the busiest police department in Ames?
There are two police departments in the data set: the Iowa State University Police Department and the Ames Police Department. Which one administers more breathalyzer tests? 

## 3. Nothing Good Happens after 2am

We all know that ""nothing good happens after 2am."" Thus, there are inevitably some times of the day when breath alcohol tests, especially in a college town like Ames, are most and least common. Which hours of the day have the most and least breathalyzer tests?  

## 4. Breathalyzer tests by month
Now that we have discovered which time of day is most common for breath alcohol tests, we will determine which time of the year has the most breathalyzer tests. Which month will have the most recorded tests?

## 5. COLLEGE
 
When we think of (binge) drinking in college towns in America, we usually think of something like this image at the left. And so, one might suspect that breath alcohol tests are given to men more often than women and that men drink more than women. 

## 6. Above the legal limit
In the USA, it is illegal to drive with a blood alcohol concentration (BAC) above 0.08%. This is the case for all 50 states. Assuming everyone tested in our data was driving (though we have no way of knowing this from the data), if either of the results (Res1, Res2) are above 0.08, the person would be charged with DUI (driving under the influence). 

## 7. Breathalyzer tests: is there a pattern over time?
We previously saw that 2am is the most common time of day for breathalyzer tests to be administered, and August is the most common month of the year for breathalyzer tests. Now, we look at the weeks in the year over time. 

## 8. Looking at timelines
How do the weeks differ over time? One of the most common data visualizations is the time series, a line tracking the changes in a variable over time. We will use the new week variable to look at test frequency over time. We end with a time series plot showing the frequency of breathalyzer tests by week in year, with one line for each year. 

## 9. The end of VEISHEA
From Wikipedia: 
""VEISHEA was an annual week-long celebration held each spring on the campus of Iowa State University in Ames, Iowa. The celebration featured an annual parade and many open-house demonstrations of the university facilities and departments. Campus organizations exhibited products, technologies, and held fundraisers for various charity groups. In addition, VEISHEA brought speakers, lecturers, and entertainers to Iowa State. [...] VEISHEA was the largest student-run festival in the nation, bringing in tens of thousands of visitors to the campus each year.""
This over 90-year tradition in Ames was terminated permanently after riots in 2014, where drunk celebrators flipped over multiple vehicles and tore light poles down. This was not the first incidence of violence and severe property damage in VEISHEA's history. Did former President Leath make the right decision by canceling VEISHEA?

"
https://github.com/mrbarkis/DataCamp_projects/tree/master/Who's%20Tweeting%3F%20Trump%20or%20Trudeau%3F,Who's Tweeting? Trump or Trudeau?,python,Katharine Jarmul,"Founder, kjamistan",https://www.datacamp.com/projects/467,"Build a machine learning classifier that knows whether President Trump or Prime Minister Trudeau is tweeting!
",https://www.github.com/mrbarkis/DataCamp_projects/blob/master/Who's%20Tweeting%3F%20Trump%20or%20Trudeau%3F/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/Who's%20Tweeting%3F%20Trump%20or%20Trudeau%3F/notebook.ipynb,"['import random; random.seed(53)', 'import pandas as pd', 'from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer', 'from sklearn.model_selection import train_test_split', 'from sklearn.naive_bayes import MultinomialNB', 'from sklearn.svm import LinearSVC', 'from sklearn import metrics', 'from datasets.helper_functions import plot_confusion_matrix', 'from datasets.helper_functions import plot_and_return_top_features', 'from pprint import pprint']","['.feature_extraction', '.text', '.model_selection', '.naive_bayes', '.svm', '.csv', '.33', '.05', '.9', '.05', '.9', '.helper_functions', '.3f', '.helper_functions', '.e']","['.seed()', '.read_csv()', '.sample()', '.fit_transform()', '.transform()', '.fit_transform()', '.transform()', '.fit()', '.predict()', '.accuracy_score()', '.fit()', '.predict()', '.accuracy_score()', '.confusion_matrix()', '.confusion_matrix()', '.fit()', '.predict()', '.accuracy_score()', '.confusion_matrix()', '.transform()', '.transform()', '.predict()', '.predict()']","['print()', 'train_test_split()', 'CountVectorizer()', 'TfidfVectorizer()', 'MultinomialNB()', 'MultinomialNB()', 'print()', 'print()', 'plot_confusion_matrix()', 'plot_confusion_matrix()', 'LinearSVC()', 'print()', 'plot_confusion_matrix()', 'plot_and_return_top_features()', 'pprint()', 'print()', 'print()']","## 1. Tweet classification: Trump vs. Trudeau
So you think you can classify text? How about tweets? In this notebook, we'll take a dive into the world of social media text classification by investigating how to properly classify tweets from two prominent North American politicians: Donald Trump and Justin Trudeau.

Photo Credit: Executive Office of the President of the United States
Tweets pose specific problems to NLP, including the fact they are shorter texts. There are also plenty of platform-specific conventions to give you hassles: mentions, #hashtags, emoji, links and short-hand phrases (ikr?). Can we overcome those challenges and build a useful classifier for these two tweeters? Yes! Let's get started.
To begin, we will import all the tools we need from scikit-learn. We will need to properly vectorize our data (CountVectorizer and TfidfVectorizer). And we will also want to import some models, including MultinomialNB from the naive_bayes module, LinearSVC from the svm module and PassiveAggressiveClassifier from the linear_model module. Finally, we'll need sklearn.metrics and train_test_split and GridSearchCV from the model_selection module to evaluate and optimize our model.

[ scikit-learn API documentation ](https://scikit-learn.org/stable/modules/classes.html)

## 2. Transforming our collected data
To begin, let's start with a corpus of tweets which were collected in November 2017. They are available in CSV format. We'll use a Pandas DataFrame to help import the data and pass it to scikit-learn for further processing.
Since the data has been collected via the Twitter API and not split into test and training sets, we'll need to do this. Let's use train_test_split() with random_state=53 and a test size of 0.33, just as we did in the DataCamp course. This will ensure we have enough test data and we'll get the same results no matter where or when we run this code.

## 3. Vectorize the tweets
We have the training and testing data all set up, but we need to create vectorized representations of the tweets in order to apply machine learning.
To do so, we will utilize the CountVectorizer and TfidfVectorizer classes which we will first need to fit to the data.
Once this is complete, we can start modeling with the new vectorized tweets!

## 4. Training a multinomial naive Bayes model
Now that we have the data in vectorized form, we can train the first model. Investigate using the Multinomial Naive Bayes model with both the CountVectorizer and TfidfVectorizer data. Which do will perform better? How come?
To assess the accuracies, we will print the test sets accuracy scores for both models.

## 5. Evaluating our model using a confusion matrix
We see that the TF-IDF model performs better than the count-based approach. Based on what we know from the NLP fundamentals course, why might that be? We know that TF-IDF allows unique tokens to have a greater weight - perhaps tweeters are using specific important words that identify them! Let's continue the investigation.
For classification tasks, an accuracy score doesn't tell the whole picture. A better evaluation can be made if we look at the confusion matrix, which shows the number correct and incorrect classifications based on each class. We can use the metrics, True Positives, False Positives, False Negatives, and True Negatives, to determine how well the model performed on a given class. How many times was Trump misclassified as Trudeau?

## 6. Trying out another classifier: Linear SVC
So the Bayesian model only has one prediction difference between the TF-IDF and count vectorizers -- fairly impressive! Interestingly, there is some confusion when the predicted label is Trump but the actual tweeter is Trudeau. If we were going to use this model, we would want to investigate what tokens are causing the confusion in order to improve the model. 
Now that we've seen what the Bayesian model can do, how about trying a different approach? LinearSVC is another popular choice for text classification. Let's see if using it with the TF-IDF vectors improves the accuracy of the classifier!

## 7. Introspecting our top model
Wow, the LinearSVC model is even better than the Multinomial Bayesian one. Nice work! Via the confusion matrix we can see that, although there is still some confusion where Trudeau's tweets are classified as Trump's, the False Positive rate is better than the previous model. So, we have a performant model, right? 
We might be able to continue tweaking and improving all of the previous models by learning more about parameter optimization or applying some better preprocessing of the tweets. 
Now let's see what the model has learned. Using the LinearSVC Classifier with two classes (Trump and Trudeau) we can sort the features (tokens), by their weight and see the most important tokens for both Trump and Trudeau. What are the most Trump-like or Trudeau-like words? Did the model learn something useful to distinguish between these two men? 

## 8. Bonus: can you write a Trump or Trudeau tweet?
So, what did our model learn? It seems like it learned that Trudeau tweets in French!
I challenge you to write your own tweet using the knowledge gained to trick the model! Use the printed list or plot above to make some inferences about what words will classify your text as Trump or Trudeau. Can you fool the model into thinking you are Trump or Trudeau?
If you can write French, feel free to make your Trudeau-impersonation tweet in French! As you may have noticed, these French words are common words, or, ""stop words"". You could remove both English and French stop words from the tweets as a preprocessing step, but that might decrease the accuracy of the model because Trudeau is the only French-speaker in the group. If you had a dataset with more than one French speaker, this would be a useful preprocessing step.
Future work on this dataset could involve:

Add extra preprocessing (such as removing URLs or French stop words) and see the effects
Use GridSearchCV to improve both your Bayesian and LinearSVC models by finding the optimal parameters
Introspect your Bayesian model to determine what words are more Trump- or Trudeau- like
Add more recent tweets to your dataset using tweepy and retrain

Good luck writing your impersonation tweets -- feel free to share them on Twitter!

"
https://github.com/mrbarkis/DataCamp_projects/tree/master/Word%20Frequency%20in%20Classic%20Novels,Word Frequency in Classic Novels,python,Hugo Bowne-Anderson,Data Scientist at DataCamp,https://www.datacamp.com/projects/38,"Use web scraping and NLP to find the most frequent words in one of two pieces of classic literature: Herman Melville's novel, Moby Dick, or Peter Pan by J. M. Barrie.
",https://www.github.com/mrbarkis/DataCamp_projects/blob/master/Word%20Frequency%20in%20Classic%20Novels/notebook.ipynb,https://raw.githubusercontent.com/mrbarkis/DataCamp_projects/master/Word%20Frequency%20in%20Classic%20Novels/notebook.ipynb,"['import requests', 'import nltk', 'from bs4 import BeautifulSoup']","['.amazonaws', '.com', '.datacamp', '.com', '.htm', '.encoding', '.text', '.tokenize', '.corpus', '.stopwords']","['.get()', '.get_text()', '.RegexpTokenizer()', '.tokenize()', '.lower()', '.download()', '.words()', '.FreqDist()', '.plot()']",['BeautifulSoup()'],"## 1. Tools for text processing
 
What are the most frequent words in Herman Melville's novel, Moby Dick, and how often do they occur?
In this notebook, we'll scrape the novel Moby Dick from the website Project Gutenberg (which contains a large corpus of books) using the Python package requests. Then we'll extract words from this web data using BeautifulSoup. Finally, we'll dive into analyzing the distribution of words using the Natural Language ToolKit (nltk). 
The Data Science pipeline we'll build in this notebook can be used to visualize the word frequency distributions of any novel that you can find on Project Gutenberg. The natural language processing tools used here apply to much of the data that data scientists encounter as a vast proportion of the world's data is unstructured data and includes a great deal of text.
Let's start by loading in the three main Python packages we are going to use.

## 2. Request Moby Dick
To analyze Moby Dick, we need to get the contents of Moby Dick from somewhere. Luckily, the text is freely available online at Project Gutenberg as an HTML file: https://www.gutenberg.org/files/2701/2701-h/2701-h.htm .
Note that HTML stands for Hypertext Markup Language and is the standard markup language for the web.
To fetch the HTML file with Moby Dick we're going to use the request package to make a GET request for the website, which means we're getting data from it. This is what you're doing through a browser when visiting a webpage, but now we're getting the requested page directly into Python instead. 

## 3. Get the text from the HTML
This HTML is not quite what we want. However, it does contain what we want: the text of Moby Dick. What we need to do now is wrangle this HTML to extract the text of the novel. For this we'll use the package BeautifulSoup.
Firstly, a word on the name of the package: Beautiful Soup? In web development, the term ""tag soup"" refers to structurally or syntactically incorrect HTML code written for a web page. What Beautiful Soup does best is to make tag soup beautiful again and to extract information from it with ease! In fact, the main object created and queried when using this package is called BeautifulSoup. After creating the soup, we can use its .get_text() method to extract the text.

[quick start quide for BeatifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#quick-start)

## 4. Extract the words
We now have the text of the novel! There is some unwanted stuff at the start and some unwanted stuff at the end. We could remove it, but this content is so much smaller in amount than the text of Moby Dick that, to a first approximation, it is okay to leave it in.
Now that we have the text of interest, it's time to count how many times each word appears, and for this we'll use nltk – the Natural Language Toolkit. We'll start by tokenizing the text, that is, remove everything that isn't a word (whitespace, punctuation, etc.) and then split the text into a list of words.

For how to use the nltk.tokenize.RegexpTokenizer function, please see [the example in the nltk documentation.](http://www.nltk.org/api/nltk.tokenize.html?highlight=regexp#module-nltk.tokenize.regexp)

## 5. Make the words lowercase
OK! We're nearly there. Note that in the above 'Or' has a capital 'O' and that in other places it may not, but both 'Or' and 'or' should be counted as the same word. For this reason, we should build a list of all words in Moby Dick in which all capital letters have been made lower case.

## 6. Load in stop words
It is common practice to remove words that appear a lot in the English language such as 'the', 'of' and 'a' because they're not so interesting. Such words are known as stop words. The package nltk includes a good list of stop words in English that we can use.

See the nltk documentation for [how to load in the stop words.](http://www.nltk.org/book/ch02.html#wordlist-corpora)

## 7. Remove stop words in Moby Dick
We now want to create a new list with all words in Moby Dick, except those that are stop words (that is, those words listed in sw). One way to get this list is to loop over all elements of words and add each word to a new list if they are not in sw.

## 8. We have the answer
Our original question was:

What are the most frequent words in Herman Melville's novel Moby Dick and how often do they occur?

We are now ready to answer that! Let's create a word frequency distribution plot using nltk. 

See the nltk documentation for [how to use nltk.FreqDist()](http://www.nltk.org/book/ch01.html#frequency-distributions)

## 9. The most common word
Nice! The frequency distribution plot above is the answer to our question. 
The natural language processing skills we used in this notebook are also applicable to much of the data that Data Scientists encounter as the vast proportion of the world's data is unstructured data and includes a great deal of text. 
So, what word turned out to (not surprisingly) be the most common word in Moby Dick?

"
